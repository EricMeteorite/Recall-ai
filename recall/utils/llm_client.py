"""LLMå®¢æˆ·ç«¯ - ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£"""

import os
import time
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass


# Windows GBK ç¼–ç å…¼å®¹çš„å®‰å…¨æ‰“å°å‡½æ•°
def _safe_print(msg: str) -> None:
    """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œæ›¿æ¢ emoji ä¸º ASCII ç­‰ä»·ç‰©ä»¥é¿å… Windows GBK ç¼–ç é”™è¯¯"""
    emoji_map = {
        'ğŸ“¥': '[IN]', 'ğŸ“¤': '[OUT]', 'ğŸ”': '[SEARCH]', 'âœ…': '[OK]', 'âŒ': '[FAIL]',
        'âš ï¸': '[WARN]', 'ğŸ’¾': '[SAVE]', 'ğŸ—ƒï¸': '[DB]', 'ğŸ§¹': '[CLEAN]', 'ğŸ“Š': '[STATS]',
        'ğŸ”„': '[SYNC]', 'ğŸ“¦': '[PKG]', 'ğŸš€': '[START]', 'ğŸ¯': '[TARGET]', 'ğŸ’¡': '[HINT]',
        'ğŸ”§': '[FIX]', 'ğŸ“': '[NOTE]', 'ğŸ‰': '[DONE]', 'â±ï¸': '[TIME]', 'ğŸŒ': '[NET]',
        'ğŸ§ ': '[BRAIN]', 'ğŸ’¬': '[CHAT]', 'ğŸ·ï¸': '[TAG]', 'ğŸ“': '[DIR]', 'ğŸ”’': '[LOCK]',
        'ğŸŒ±': '[PLANT]', 'ğŸ—‘ï¸': '[DEL]', 'ğŸ’«': '[MAGIC]', 'ğŸ­': '[MASK]', 'ğŸ“–': '[BOOK]',
        'âš¡': '[FAST]', 'ğŸ”¥': '[HOT]', 'ğŸ’': '[GEM]', 'ğŸŒŸ': '[STAR]', 'ğŸ¨': '[ART]'
    }
    for emoji, ascii_equiv in emoji_map.items():
        msg = msg.replace(emoji, ascii_equiv)
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode('ascii', errors='replace').decode('ascii'))


@dataclass
class LLMResponse:
    """LLMå“åº”"""
    content: str
    model: str
    usage: Dict[str, int]
    latency_ms: float
    raw_response: Optional[Any] = None


class LLMClient:
    """ç»Ÿä¸€LLMå®¢æˆ·ç«¯
    
    æ”¯æŒ OpenAI å…¼å®¹æ¥å£ï¼š
    - OpenAI (gpt-4, gpt-3.5-turbo)
    - ç¡…åŸºæµåŠ¨ (DeepSeek, Qwen ç­‰)
    - Ollama æœ¬åœ°
    - å…¶ä»– OpenAI å…¼å®¹ API
    """
    
    def __init__(
        self,
        model: str = "gpt-3.5-turbo",
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
        timeout: float = 30.0,
        max_retries: int = 3
    ):
        self.model = model
        # ä¼˜å…ˆä½¿ç”¨ LLM_API_KEYï¼Œå…¼å®¹æ—§çš„ OPENAI_API_KEY
        self.api_key = api_key or os.environ.get('LLM_API_KEY') or os.environ.get('OPENAI_API_KEY')
        self.api_base = api_base or os.environ.get('LLM_API_BASE')
        self.timeout = timeout
        self.max_retries = max_retries
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self._client = None
    
    @property
    def client(self):
        """è·å– OpenAI å®¢æˆ·ç«¯"""
        if self._client is None:
            try:
                from openai import OpenAI
                
                # åˆ›å»ºå®¢æˆ·ç«¯
                client_kwargs = {
                    "api_key": self.api_key,
                    "timeout": self.timeout,
                }
                
                # è®¾ç½®è‡ªå®šä¹‰ API åœ°å€
                if self.api_base:
                    client_kwargs["base_url"] = self.api_base
                
                self._client = OpenAI(**client_kwargs)
                    
            except ImportError:
                raise ImportError(
                    "openai æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install openai"
                )
        return self._client
    
    def complete(
        self,
        prompt: str,
        max_tokens: int = 500,
        temperature: float = 0.7,
        stop: Optional[List[str]] = None
    ) -> str:
        """ç®€å•æ–‡æœ¬è¡¥å…¨"""
        response = self.chat(
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=temperature,
            stop=stop
        )
        return response.content
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 500,
        temperature: float = 0.7,
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> LLMResponse:
        """èŠå¤©è¡¥å…¨ - ä½¿ç”¨ OpenAI SDKï¼Œå¸¦é€Ÿç‡é™åˆ¶å¤„ç†"""
        start_time = time.time()
        
        for attempt in range(self.max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stop=stop,
                    **kwargs
                )
                
                latency = (time.time() - start_time) * 1000
                
                return LLMResponse(
                    content=response.choices[0].message.content,
                    model=response.model,
                    usage={
                        'prompt_tokens': response.usage.prompt_tokens,
                        'completion_tokens': response.usage.completion_tokens,
                        'total_tokens': response.usage.total_tokens
                    },
                    latency_ms=latency,
                    raw_response=response
                )
                
            except Exception as e:
                error_str = str(e).lower()
                
                # ç‰¹æ®Šå¤„ç† 429 é”™è¯¯ï¼ˆAPI é™æµï¼‰
                if '429' in error_str or 'rate limit' in error_str or 'too many requests' in error_str:
                    if attempt < self.max_retries - 1:
                        # æŒ‡æ•°é€€é¿ï¼š15, 30, 45 ç§’
                        wait_time = (attempt + 1) * 15
                        _safe_print(f"[LLM] API é™æµ (429)ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯• ({attempt + 1}/{self.max_retries})")
                        time.sleep(wait_time)
                        continue
                    else:
                        raise RuntimeError(f"LLM API é™æµï¼Œå·²é‡è¯• {self.max_retries} æ¬¡: {e}")
                
                # å…¶ä»–é”™è¯¯
                if attempt == self.max_retries - 1:
                    raise
                time.sleep(1 * (attempt + 1))  # æ™®é€šé‡è¯•é—´éš”
        
        raise RuntimeError("LLMè°ƒç”¨å¤±è´¥")
    
    async def achat(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 500,
        temperature: float = 0.7,
        **kwargs
    ) -> LLMResponse:
        """å¼‚æ­¥èŠå¤©è¡¥å…¨"""
        from openai import AsyncOpenAI
        
        start_time = time.time()
        
        # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
        client_kwargs = {
            "api_key": self.api_key,
            "timeout": self.timeout,
        }
        if self.api_base:
            client_kwargs["base_url"] = self.api_base
        
        async_client = AsyncOpenAI(**client_kwargs)
        
        response = await async_client.chat.completions.create(
            model=self.model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            **kwargs
        )
        
        latency = (time.time() - start_time) * 1000
        
        return LLMResponse(
            content=response.choices[0].message.content,
            model=response.model,
            usage={
                'prompt_tokens': response.usage.prompt_tokens,
                'completion_tokens': response.usage.completion_tokens,
                'total_tokens': response.usage.total_tokens
            },
            latency_ms=latency,
            raw_response=response
        )
    
    def embed(
        self,
        texts: Union[str, List[str]],
        model: Optional[str] = None
    ) -> List[List[float]]:
        """è·å–åµŒå…¥å‘é‡"""
        if isinstance(texts, str):
            texts = [texts]
        
        embed_model = model or "text-embedding-ada-002"
        
        response = self.client.embeddings.create(
            model=embed_model,
            input=texts
        )
        
        return [item.embedding for item in response.data]
    
    def extract_entities(self, text: str) -> List[Dict[str, str]]:
        """ä½¿ç”¨LLMæå–å®ä½“"""
        prompt = f"""ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“ï¼ˆäººåã€åœ°åã€ç»„ç»‡ã€ç‰©å“ç­‰ï¼‰ã€‚
ä»¥JSONæ ¼å¼è¿”å›ï¼š[{{"name": "å®ä½“å", "type": "ç±»å‹"}}]

æ–‡æœ¬ï¼š{text}

å®ä½“ï¼š"""
        
        response = self.complete(prompt, max_tokens=200, temperature=0)
        
        try:
            import json
            return json.loads(response)
        except:
            return []
    
    def summarize(self, text: str, max_length: int = 100) -> str:
        """ä½¿ç”¨LLMæ‘˜è¦"""
        prompt = f"""è¯·ç”¨{max_length}å­—ä»¥å†…æ‘˜è¦ä»¥ä¸‹å†…å®¹ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼š

{text}

æ‘˜è¦ï¼š"""
        
        return self.complete(prompt, max_tokens=max_length, temperature=0.3)
    
    def check_relevance(
        self,
        query: str,
        documents: List[str]
    ) -> List[float]:
        """æ£€æŸ¥æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§"""
        prompt = f"""å¯¹ä»¥ä¸‹æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§è¯„åˆ†ï¼ˆ0-1ï¼‰ã€‚

æŸ¥è¯¢ï¼š{query}

æ–‡æ¡£ï¼š
{chr(10).join([f"{i+1}. {d[:100]}" for i, d in enumerate(documents)])}

è¯·åªè¿”å›åˆ†æ•°ï¼Œç”¨é€—å·åˆ†éš”ï¼š"""
        
        response = self.complete(prompt, max_tokens=50, temperature=0)
        
        try:
            scores = [float(s.strip()) for s in response.split(',')]
            return scores[:len(documents)]
        except:
            return [0.5] * len(documents)

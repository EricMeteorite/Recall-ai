"""LLMå®¢æˆ·ç«¯ - ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£"""

import os
import time
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass


# Windows GBK ç¼–ç å…¼å®¹çš„å®‰å…¨æ‰“å°å‡½æ•°
def _safe_print(msg: str) -> None:
    """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œæ›¿æ¢ emoji ä¸º ASCII ç­‰ä»·ç‰©ä»¥é¿å… Windows GBK ç¼–ç é”™è¯¯"""
    emoji_map = {
        'ğŸ“¥': '[IN]', 'ğŸ“¤': '[OUT]', 'ğŸ”': '[SEARCH]', 'âœ…': '[OK]', 'âŒ': '[FAIL]',
        'âš ï¸': '[WARN]', 'ğŸ’¾': '[SAVE]', 'ğŸ—ƒï¸': '[DB]', 'ğŸ§¹': '[CLEAN]', 'ğŸ“Š': '[STATS]',
        'ğŸ”„': '[SYNC]', 'ğŸ“¦': '[PKG]', 'ğŸš€': '[START]', 'ğŸ¯': '[TARGET]', 'ğŸ’¡': '[HINT]',
        'ğŸ”§': '[FIX]', 'ğŸ“': '[NOTE]', 'ğŸ‰': '[DONE]', 'â±ï¸': '[TIME]', 'ğŸŒ': '[NET]',
        'ğŸ§ ': '[BRAIN]', 'ğŸ’¬': '[CHAT]', 'ğŸ·ï¸': '[TAG]', 'ğŸ“': '[DIR]', 'ğŸ”’': '[LOCK]',
        'ğŸŒ±': '[PLANT]', 'ğŸ—‘ï¸': '[DEL]', 'ğŸ’«': '[MAGIC]', 'ğŸ­': '[MASK]', 'ğŸ“–': '[BOOK]',
        'âš¡': '[FAST]', 'ğŸ”¥': '[HOT]', 'ğŸ’': '[GEM]', 'ğŸŒŸ': '[STAR]', 'ğŸ¨': '[ART]'
    }
    for emoji, ascii_equiv in emoji_map.items():
        msg = msg.replace(emoji, ascii_equiv)
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode('ascii', errors='replace').decode('ascii'))


@dataclass
class LLMResponse:
    """LLMå“åº”"""
    content: str
    model: str
    usage: Dict[str, int]
    latency_ms: float
    raw_response: Optional[Any] = None


class LLMClient:
    """ç»Ÿä¸€LLMå®¢æˆ·ç«¯
    
    æ”¯æŒ OpenAI å…¼å®¹æ¥å£ï¼š
    - OpenAI (gpt-4, gpt-3.5-turbo)
    - ç¡…åŸºæµåŠ¨ (DeepSeek, Qwen ç­‰)
    - Ollama æœ¬åœ°
    - å…¶ä»– OpenAI å…¼å®¹ API
    """
    
    def __init__(
        self,
        model: str = "gpt-3.5-turbo",
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
        timeout: float = 30.0,
        max_retries: int = 3
    ):
        self.model = model
        # ä¼˜å…ˆä½¿ç”¨ LLM_API_KEYï¼Œå…¼å®¹æ—§çš„ OPENAI_API_KEY
        self.api_key = api_key or os.environ.get('LLM_API_KEY') or os.environ.get('OPENAI_API_KEY')
        self.api_base = api_base or os.environ.get('LLM_API_BASE')
        self.timeout = timeout
        self.max_retries = max_retries
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._client = None
        # v5.0: è‡ªåŠ¨æ£€æµ‹æä¾›å•†ï¼ˆé›¶æ–°å¢é…ç½®ï¼‰
        self._provider = self._detect_provider()
    
    def _detect_provider(self) -> str:
        """æ ¹æ® api_base åŸŸåå’Œ model åç§°è‡ªåŠ¨æ£€æµ‹æä¾›å•†"""
        if self.api_base:
            from urllib.parse import urlparse
            hostname = urlparse(self.api_base.lower()).hostname or ''
            if hostname.endswith('.anthropic.com') or hostname == 'api.anthropic.com':
                return 'anthropic'
            if hostname.endswith('.googleapis.com') or 'generativelanguage' in hostname:
                return 'google'
            return 'openai'
        if self.model:
            model_lower = self.model.lower()
            if model_lower.startswith('claude'):
                return 'anthropic'
            if model_lower.startswith('gemini'):
                return 'google'
        return 'openai'
    
    @property
    def client(self):
        """è·å– LLM å®¢æˆ·ç«¯ï¼ˆv5.0: è‡ªåŠ¨é€‰æ‹©åç«¯ï¼‰"""
        if self._client is None:
            if self._provider == 'anthropic':
                self._client = self._create_anthropic_client()
            elif self._provider == 'google':
                self._client = self._create_google_client()
            else:
                self._client = self._create_openai_client()
        return self._client
    
    def _create_openai_client(self):
        """åˆ›å»º OpenAI å®¢æˆ·ç«¯"""
        try:
            from openai import OpenAI
            client_kwargs = {
                "api_key": self.api_key,
                "timeout": self.timeout,
            }
            if self.api_base:
                client_kwargs["base_url"] = self.api_base
            return OpenAI(**client_kwargs)
        except ImportError:
            raise ImportError("openai æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install openai")
    
    def _create_anthropic_client(self):
        """åˆ›å»º Anthropic å®¢æˆ·ç«¯"""
        try:
            from anthropic import Anthropic
            return Anthropic(api_key=self.api_key, timeout=self.timeout)
        except ImportError:
            raise ImportError("ä½¿ç”¨ Claude æ¨¡å‹éœ€è¦å®‰è£… anthropic: pip install anthropic")
    
    def _create_google_client(self):
        """åˆ›å»º Google Generative AI å®¢æˆ·ç«¯"""
        try:
            import google.generativeai as genai
            genai.configure(api_key=self.api_key)
            return genai
        except ImportError:
            raise ImportError("ä½¿ç”¨ Gemini æ¨¡å‹éœ€è¦å®‰è£… google-generativeai: pip install google-generativeai")
    
    def complete(
        self,
        prompt: str,
        max_tokens: Optional[int] = None,
        temperature: float = 0.7,
        stop: Optional[List[str]] = None
    ) -> str:
        """ç®€å•æ–‡æœ¬è¡¥å…¨
        
        Args:
            prompt: æç¤ºè¯
            max_tokens: æœ€å¤§è¾“å‡º token æ•°ï¼Œé»˜è®¤ä» LLM_DEFAULT_MAX_TOKENS ç¯å¢ƒå˜é‡è¯»å–ï¼ˆé»˜è®¤ 2000ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            stop: åœæ­¢è¯
        """
        if max_tokens is None:
            max_tokens = int(os.environ.get('LLM_DEFAULT_MAX_TOKENS', '2000'))
        response = self.chat(
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=temperature,
            stop=stop
        )
        return response.content
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        max_tokens: Optional[int] = None,
        temperature: float = 0.7,
        stop: Optional[List[str]] = None,
        **kwargs
    ) -> LLMResponse:
        """èŠå¤©è¡¥å…¨ - v5.0: è‡ªåŠ¨è·¯ç”±åˆ°å¯¹åº”æä¾›å•†"""
        if max_tokens is None:
            max_tokens = int(os.environ.get('LLM_DEFAULT_MAX_TOKENS', '2000'))
        if self._provider == 'anthropic':
            return self._chat_anthropic(messages, max_tokens, temperature, stop)
        elif self._provider == 'google':
            return self._chat_google(messages, max_tokens, temperature, stop)
        else:
            return self._chat_openai(messages, max_tokens, temperature, stop, **kwargs)
    
    def _chat_openai(self, messages, max_tokens, temperature, stop, **kwargs):
        """OpenAI èŠå¤©è¡¥å…¨ - å¸¦é€Ÿç‡é™åˆ¶å¤„ç†"""
        start_time = time.time()
        
        for attempt in range(self.max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stop=stop,
                    **kwargs
                )
                
                latency = (time.time() - start_time) * 1000
                
                return LLMResponse(
                    content=response.choices[0].message.content,
                    model=response.model,
                    usage={
                        'prompt_tokens': response.usage.prompt_tokens,
                        'completion_tokens': response.usage.completion_tokens,
                        'total_tokens': response.usage.total_tokens
                    },
                    latency_ms=latency,
                    raw_response=response
                )
                
            except Exception as e:
                error_str = str(e).lower()
                
                # ç‰¹æ®Šå¤„ç† 429 é”™è¯¯ï¼ˆAPI é™æµï¼‰
                if '429' in error_str or 'rate limit' in error_str or 'too many requests' in error_str:
                    if attempt < self.max_retries - 1:
                        # æŒ‡æ•°é€€é¿ï¼š15, 30, 45 ç§’
                        wait_time = (attempt + 1) * 15
                        _safe_print(f"[LLM] API é™æµ (429)ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯• ({attempt + 1}/{self.max_retries})")
                        time.sleep(wait_time)
                        continue
                    else:
                        raise RuntimeError(f"LLM API é™æµï¼Œå·²é‡è¯• {self.max_retries} æ¬¡: {e}")
                
                # å…¶ä»–é”™è¯¯
                if attempt == self.max_retries - 1:
                    raise
                time.sleep(1 * (attempt + 1))  # æ™®é€šé‡è¯•é—´éš”
        
        raise RuntimeError("LLMè°ƒç”¨å¤±è´¥")
    
    def _chat_anthropic(self, messages, max_tokens, temperature, stop):
        """Anthropic èŠå¤©è¡¥å…¨"""
        start_time = time.time()
        system_msg = ""
        chat_messages = []
        for msg in messages:
            if msg['role'] == 'system':
                system_msg += msg['content'] + "\n"
            else:
                chat_messages.append(msg)
        kwargs = {}
        if system_msg.strip():
            kwargs['system'] = system_msg.strip()
        if stop:
            kwargs['stop_sequences'] = stop if isinstance(stop, list) else [stop]
        response = self.client.messages.create(
            model=self.model,
            messages=chat_messages,
            max_tokens=max_tokens,
            temperature=temperature,
            **kwargs,
        )
        latency = (time.time() - start_time) * 1000
        return LLMResponse(
            content=response.content[0].text,
            model=response.model,
            usage={
                'prompt_tokens': response.usage.input_tokens,
                'completion_tokens': response.usage.output_tokens,
                'total_tokens': response.usage.input_tokens + response.usage.output_tokens
            },
            latency_ms=latency,
            raw_response=response
        )
    
    def _chat_google(self, messages, max_tokens, temperature, stop):
        """Google èŠå¤©è¡¥å…¨"""
        start_time = time.time()
        system_msg = ""
        chat_messages = []
        for msg in messages:
            if msg['role'] == 'system':
                system_msg += msg['content'] + "\n"
            else:
                chat_messages.append(msg)
        model_kwargs = {}
        if system_msg.strip():
            model_kwargs['system_instruction'] = system_msg.strip()
        model = self.client.GenerativeModel(self.model, **model_kwargs)
        contents = [{'role': 'user' if m['role'] == 'user' else 'model',
                     'parts': [m['content']]} for m in chat_messages]
        response = model.generate_content(
            contents,
            generation_config={
                'max_output_tokens': max_tokens,
                'temperature': temperature,
                'stop_sequences': stop or [],
            }
        )
        latency = (time.time() - start_time) * 1000
        return LLMResponse(
            content=response.text,
            model=self.model,
            usage={
                'prompt_tokens': getattr(response.usage_metadata, 'prompt_token_count', 0),
                'completion_tokens': getattr(response.usage_metadata, 'candidates_token_count', 0),
                'total_tokens': getattr(response.usage_metadata, 'total_token_count', 0),
            },
            latency_ms=latency,
            raw_response=response
        )
    
    async def achat(
        self,
        messages: List[Dict[str, str]],
        max_tokens: Optional[int] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> LLMResponse:
        """å¼‚æ­¥èŠå¤©è¡¥å…¨ - v5.0: è‡ªåŠ¨è·¯ç”±åˆ°å¯¹åº”æä¾›å•†
        
        Args:
            max_tokens: æœ€å¤§è¾“å‡º token æ•°ï¼Œé»˜è®¤ä» LLM_DEFAULT_MAX_TOKENS ç¯å¢ƒå˜é‡è¯»å–
        """
        if max_tokens is None:
            max_tokens = int(os.environ.get('LLM_DEFAULT_MAX_TOKENS', '2000'))
        if self._provider == 'anthropic':
            return await self._achat_anthropic(messages, max_tokens, temperature, **kwargs)
        elif self._provider == 'google':
            return await self._achat_google(messages, max_tokens, temperature, **kwargs)
        else:
            return await self._achat_openai(messages, max_tokens, temperature, **kwargs)
    
    async def _achat_openai(self, messages, max_tokens, temperature, **kwargs):
        """å¼‚æ­¥ OpenAI èŠå¤©è¡¥å…¨"""
        from openai import AsyncOpenAI
        
        start_time = time.time()
        
        client_kwargs = {
            "api_key": self.api_key,
            "timeout": self.timeout,
        }
        if self.api_base:
            client_kwargs["base_url"] = self.api_base
        
        async_client = AsyncOpenAI(**client_kwargs)
        
        response = await async_client.chat.completions.create(
            model=self.model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            **kwargs
        )
        
        latency = (time.time() - start_time) * 1000
        
        return LLMResponse(
            content=response.choices[0].message.content,
            model=response.model,
            usage={
                'prompt_tokens': response.usage.prompt_tokens,
                'completion_tokens': response.usage.completion_tokens,
                'total_tokens': response.usage.total_tokens
            },
            latency_ms=latency,
            raw_response=response
        )
    
    async def _achat_anthropic(self, messages, max_tokens, temperature, **kwargs):
        """å¼‚æ­¥ Anthropic èŠå¤©è¡¥å…¨"""
        try:
            from anthropic import AsyncAnthropic
        except ImportError:
            raise ImportError("ä½¿ç”¨ Claude æ¨¡å‹éœ€è¦å®‰è£… anthropic: pip install anthropic")
        
        start_time = time.time()
        
        async_client = AsyncAnthropic(api_key=self.api_key, timeout=self.timeout)
        
        system_msg = ""
        chat_messages = []
        for msg in messages:
            if msg['role'] == 'system':
                system_msg += msg['content'] + "\n"
            else:
                chat_messages.append(msg)
        
        ak = {}
        if system_msg.strip():
            ak['system'] = system_msg.strip()
        stop = kwargs.get('stop')
        if stop:
            ak['stop_sequences'] = stop if isinstance(stop, list) else [stop]
        
        response = await async_client.messages.create(
            model=self.model,
            messages=chat_messages,
            max_tokens=max_tokens,
            temperature=temperature,
            **ak,
        )
        
        latency = (time.time() - start_time) * 1000
        
        return LLMResponse(
            content=response.content[0].text,
            model=response.model,
            usage={
                'prompt_tokens': response.usage.input_tokens,
                'completion_tokens': response.usage.output_tokens,
                'total_tokens': response.usage.input_tokens + response.usage.output_tokens
            },
            latency_ms=latency,
            raw_response=response
        )
    
    async def _achat_google(self, messages, max_tokens, temperature, **kwargs):
        """å¼‚æ­¥ Google èŠå¤©è¡¥å…¨"""
        try:
            import google.generativeai as genai
        except ImportError:
            raise ImportError("ä½¿ç”¨ Gemini æ¨¡å‹éœ€è¦å®‰è£… google-generativeai: pip install google-generativeai")
        
        start_time = time.time()
        
        genai.configure(api_key=self.api_key)
        
        system_msg = ""
        chat_messages = []
        for msg in messages:
            if msg['role'] == 'system':
                system_msg += msg['content'] + "\n"
            else:
                chat_messages.append(msg)
        
        model_kwargs = {}
        if system_msg.strip():
            model_kwargs['system_instruction'] = system_msg.strip()
        
        model = genai.GenerativeModel(self.model, **model_kwargs)
        
        contents = [{'role': 'user' if m['role'] == 'user' else 'model',
                     'parts': [m['content']]} for m in chat_messages]
        
        stop = kwargs.get('stop')
        response = await model.generate_content_async(
            contents,
            generation_config={
                'max_output_tokens': max_tokens,
                'temperature': temperature,
                'stop_sequences': stop or [],
            }
        )
        
        latency = (time.time() - start_time) * 1000
        
        return LLMResponse(
            content=response.text,
            model=self.model,
            usage={
                'prompt_tokens': getattr(response.usage_metadata, 'prompt_token_count', 0),
                'completion_tokens': getattr(response.usage_metadata, 'candidates_token_count', 0),
                'total_tokens': getattr(response.usage_metadata, 'total_token_count', 0),
            },
            latency_ms=latency,
            raw_response=response
        )
    
    def embed(
        self,
        texts: Union[str, List[str]],
        model: Optional[str] = None
    ) -> List[List[float]]:
        """è·å–åµŒå…¥å‘é‡"""
        if self._provider != 'openai':
            raise NotImplementedError("è¯·ä½¿ç”¨ APIEmbeddingBackend è¿›è¡Œ Embedding")
        if isinstance(texts, str):
            texts = [texts]
        
        embed_model = model or "text-embedding-ada-002"
        
        response = self.client.embeddings.create(
            model=embed_model,
            input=texts
        )
        
        return [item.embedding for item in response.data]
    
    def extract_entities(self, text: str) -> List[Dict[str, str]]:
        """ä½¿ç”¨LLMæå–å®ä½“"""
        prompt = f"""ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“ï¼ˆäººåã€åœ°åã€ç»„ç»‡ã€ç‰©å“ç­‰ï¼‰ã€‚
ä»¥JSONæ ¼å¼è¿”å›ï¼š[{{"name": "å®ä½“å", "type": "ç±»å‹"}}]

æ–‡æœ¬ï¼š{text}

å®ä½“ï¼š"""
        
        # å®ä½“æå–è¾“å‡ºç›¸å¯¹å›ºå®šï¼Œä½¿ç”¨è¾ƒå°çš„é»˜è®¤å€¼
        response = self.complete(prompt, temperature=0)
        
        try:
            import json
            return json.loads(response)
        except Exception:
            return []
    
    def summarize(self, text: str, max_length: int = 100) -> str:
        """ä½¿ç”¨LLMæ‘˜è¦"""
        prompt = f"""è¯·ç”¨{max_length}å­—ä»¥å†…æ‘˜è¦ä»¥ä¸‹å†…å®¹ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼š

{text}

æ‘˜è¦ï¼š"""
        
        return self.complete(prompt, max_tokens=max_length * 2, temperature=0.3)
    
    def check_relevance(
        self,
        query: str,
        documents: List[str]
    ) -> List[float]:
        """æ£€æŸ¥æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§"""
        prompt = f"""å¯¹ä»¥ä¸‹æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§è¯„åˆ†ï¼ˆ0-1ï¼‰ã€‚

æŸ¥è¯¢ï¼š{query}

æ–‡æ¡£ï¼š
{chr(10).join([f"{i+1}. {d[:100]}" for i, d in enumerate(documents)])}

è¯·åªè¿”å›åˆ†æ•°ï¼Œç”¨é€—å·åˆ†éš”ï¼š"""
        
        # ç›¸å…³æ€§è¯„åˆ†è¾“å‡ºå¾ˆçŸ­ï¼Œä½¿ç”¨è¾ƒå°çš„é»˜è®¤å€¼
        response = self.complete(prompt, max_tokens=100, temperature=0)
        
        try:
            scores = [float(s.strip()) for s in response.split(',')]
            return scores[:len(documents)]
        except Exception:
            return [0.5] * len(documents)

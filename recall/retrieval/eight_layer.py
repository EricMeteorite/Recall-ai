"""å…«å±‚æ¼æ–—æ£€ç´¢æ¶æ„ - Phase 3.6 å‡çº§ç‰ˆï¼šå¹¶è¡Œä¸‰è·¯å¬å› + RRF èåˆ"""

import time
from typing import List, Dict, Any, Optional, Set, Callable, Tuple
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict

from .rrf_fusion import reciprocal_rank_fusion


# Windows GBK ç¼–ç å…¼å®¹çš„å®‰å…¨æ‰“å°å‡½æ•°
def _safe_print(msg: str) -> None:
    """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œæ›¿æ¢ emoji ä¸º ASCII ç­‰ä»·ç‰©ä»¥é¿å… Windows GBK ç¼–ç é”™è¯¯"""
    emoji_map = {
        'ğŸ“¥': '[IN]', 'ğŸ“¤': '[OUT]', 'ğŸ”': '[SEARCH]', 'âœ…': '[OK]', 'âŒ': '[FAIL]',
        'âš ï¸': '[WARN]', 'ğŸ’¾': '[SAVE]', 'ğŸ—ƒï¸': '[DB]', 'ğŸ§¹': '[CLEAN]', 'ğŸ“Š': '[STATS]',
        'ğŸ”„': '[SYNC]', 'ğŸ“¦': '[PKG]', 'ğŸš€': '[START]', 'ğŸ¯': '[TARGET]', 'ğŸ’¡': '[HINT]',
        'ğŸ”§': '[FIX]', 'ğŸ“': '[NOTE]', 'ğŸ‰': '[DONE]', 'â±ï¸': '[TIME]', 'ğŸŒ': '[NET]',
        'ğŸ§ ': '[BRAIN]', 'ğŸ’¬': '[CHAT]', 'ğŸ·ï¸': '[TAG]', 'ğŸ“': '[DIR]', 'ğŸ”’': '[LOCK]',
        'ğŸŒ±': '[PLANT]', 'ğŸ—‘ï¸': '[DEL]', 'ğŸ’«': '[MAGIC]', 'ğŸ­': '[MASK]', 'ğŸ“–': '[BOOK]',
        'âš¡': '[FAST]', 'ğŸ”¥': '[HOT]', 'ğŸ’': '[GEM]', 'ğŸŒŸ': '[STAR]', 'ğŸ¨': '[ART]'
    }
    for emoji, ascii_equiv in emoji_map.items():
        msg = msg.replace(emoji, ascii_equiv)
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode('ascii', errors='replace').decode('ascii'))


class RetrievalLayer(Enum):
    """æ£€ç´¢å±‚çº§"""
    L1_BLOOM_FILTER = "bloom_filter"           # å¸ƒéš†è¿‡æ»¤å™¨å¿«é€Ÿå¦å®š
    L2_INVERTED_INDEX = "inverted_index"       # å€’æ’ç´¢å¼•å…³é”®è¯åŒ¹é…
    L3_ENTITY_INDEX = "entity_index"           # å®ä½“ç´¢å¼•
    L4_NGRAM_INDEX = "ngram_index"             # N-gramç´¢å¼•
    L5_VECTOR_COARSE = "vector_coarse"         # å‘é‡ç²—ç­›
    L6_VECTOR_FINE = "vector_fine"             # å‘é‡ç²¾æ’
    L7_RERANK = "rerank"                       # é‡æ’åº
    L8_LLM_FILTER = "llm_filter"               # LLMè¿‡æ»¤


@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    id: str
    content: str
    score: float
    source_layer: RetrievalLayer
    metadata: Dict[str, Any] = field(default_factory=dict)
    entities: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'id': self.id,
            'content': self.content,
            'score': self.score,
            'source_layer': self.source_layer.value,
            'metadata': self.metadata,
            'entities': self.entities
        }


@dataclass
class LayerStats:
    """å±‚çº§ç»Ÿè®¡"""
    layer: RetrievalLayer
    input_count: int
    output_count: int
    time_ms: float
    filtered_count: int = 0


class EightLayerRetriever:
    """å…«å±‚æ¼æ–—æ£€ç´¢å™¨ - Phase 3.6 å‡çº§ä¸ºå¹¶è¡Œä¸‰è·¯å¬å›
    
    æ£€ç´¢æµç¨‹ï¼ˆPhase 3.6ï¼‰ï¼š
    å¹¶è¡Œæ‰§è¡Œä¸‰è·¯å¬å›:
      - è·¯å¾„ 1: IVF-HNSW è¯­ä¹‰å¬å› (95-99% å¬å›ç‡)
      - è·¯å¾„ 2: å€’æ’ç´¢å¼•å…³é”®è¯å¬å› (100% å¬å›ç‡)
      - è·¯å¾„ 3: å®ä½“ç´¢å¼•å¬å› (100% å¬å›ç‡)
    â†’ RRF èåˆå–å¹¶é›†
    â†’ å¦‚æœç»“æœä¸ºç©ºï¼Œå¯ç”¨ N-gram åŸæ–‡å…œåº• (100% ä¿è¯)
    â†’ é‡æ’åº
    â†’ è¿”å›ç»“æœ
    
    å‘åå…¼å®¹ï¼šä¸²è¡Œæ¼æ–—æ¨¡å¼
    L1: å¸ƒéš†è¿‡æ»¤å™¨ - O(1) å¿«é€Ÿå¦å®šä¸å¯èƒ½çš„å€™é€‰
    L2: å€’æ’ç´¢å¼• - O(log n) å…³é”®è¯åŒ¹é…
    L3: å®ä½“ç´¢å¼• - O(1) å®ä½“å…³è”æŸ¥æ‰¾
    L4: N-gramç´¢å¼• - O(k) æ¨¡ç³ŠåŒ¹é…
    L5: å‘é‡ç²—ç­› - O(n) è¿‘ä¼¼æœ€è¿‘é‚»
    L6: å‘é‡ç²¾æ’ - O(k) ç²¾ç¡®è·ç¦»è®¡ç®—
    L7: é‡æ’åº - O(k log k) å¤šå› ç´ ç»¼åˆæ’åº
    L8: LLMè¿‡æ»¤ - O(k) è¯­ä¹‰ç›¸å…³æ€§åˆ¤æ–­
    """
    
    def __init__(
        self,
        bloom_filter: Optional[Any] = None,
        inverted_index: Optional[Any] = None,
        entity_index: Optional[Any] = None,
        ngram_index: Optional[Any] = None,
        vector_index: Optional[Any] = None,
        llm_client: Optional[Any] = None,
        content_store: Optional[Callable[[str], Optional[str]]] = None,
        # Phase 3.6 æ–°å¢ï¼šç”¨äº VectorIndexIVF çš„å‘é‡ç¼–ç 
        embedding_backend: Optional[Any] = None,
    ):
        self.bloom_filter = bloom_filter
        self.inverted_index = inverted_index
        self.entity_index = entity_index
        self.ngram_index = ngram_index
        self.vector_index = vector_index
        self.llm_client = llm_client
        # å†…å®¹å­˜å‚¨å›è°ƒï¼šé€šè¿‡ ID æŸ¥è¯¢å†…å®¹
        self.content_store = content_store
        # Phase 3.6: embedding_backend ç”¨äº VectorIndexIVFï¼ˆæ— å†…ç½® encodeï¼‰
        self.embedding_backend = embedding_backend
        
        # å†…éƒ¨å†…å®¹ç¼“å­˜ï¼ˆç”¨äºç´¢å¼•æ—¶å­˜å‚¨å†…å®¹ï¼‰
        self._content_cache: Dict[str, str] = {}
        
        # ç»Ÿè®¡
        self.stats: List[LayerStats] = []
        
        # é…ç½®
        self.config = {
            'l1_enabled': True,
            'l2_enabled': True,
            'l3_enabled': True,
            'l4_enabled': True,
            'l5_enabled': True,
            'l6_enabled': True,
            'l7_enabled': True,
            'l8_enabled': False,  # LLMè¿‡æ»¤é»˜è®¤å…³é—­ï¼ˆæ¶ˆè€—èµ„æºï¼‰
            'l5_top_k': 100,      # ç²—ç­›è¿”å›æ•°é‡
            'l6_top_k': 20,       # ç²¾æ’è¿”å›æ•°é‡
            'l7_top_k': 10,       # é‡æ’åºè¿”å›æ•°é‡
            'l8_top_k': 5,        # LLMè¿‡æ»¤è¿”å›æ•°é‡
            # Phase 3.6 æ–°å¢é…ç½®
            'parallel_recall_enabled': True,   # å¯ç”¨å¹¶è¡Œå¬å›
            'rrf_k': 60,                       # RRF å¸¸æ•°
            'vector_weight': 1.0,              # è¯­ä¹‰å¬å›æƒé‡
            'keyword_weight': 1.2,             # å…³é”®è¯å¬å›æƒé‡ï¼ˆ100%å¬å›ï¼Œæƒé‡æ›´é«˜ï¼‰
            'entity_weight': 1.0,              # å®ä½“å¬å›æƒé‡
            'ngram_weight': 1.5,               # N-gramåŸæ–‡å¬å›æƒé‡ï¼ˆæœ€é«˜ï¼Œç¡®ä¿ç²¾ç¡®åŒ¹é…ä¼˜å…ˆï¼‰
            'fallback_enabled': True,          # å¯ç”¨åŸæ–‡å…œåº•ï¼ˆå‘åå…¼å®¹ï¼‰
            'fallback_parallel': True,         # å¹¶è¡Œæ‰«æ
            'fallback_workers': 4,             # æ‰«æçº¿ç¨‹æ•°
        }
    
    def cache_content(self, doc_id: str, content: str):
        """ç¼“å­˜æ–‡æ¡£å†…å®¹ï¼ˆåœ¨æ·»åŠ ç´¢å¼•æ—¶è°ƒç”¨ï¼‰"""
        self._content_cache[doc_id] = content
    
    def get_content(self, doc_id: str) -> str:
        """è·å–æ–‡æ¡£å†…å®¹"""
        # ä¼˜å…ˆä»ç¼“å­˜è·å–
        if doc_id in self._content_cache:
            return self._content_cache[doc_id]
        # å¦åˆ™ä»å¤–éƒ¨å­˜å‚¨è·å–
        if self.content_store:
            content = self.content_store(doc_id)
            if content:
                return content
        return ""
    
    def retrieve(
        self,
        query: str,
        entities: Optional[List[str]] = None,
        keywords: Optional[List[str]] = None,
        top_k: int = 10,
        filters: Optional[Dict[str, Any]] = None,
        temporal_context: Optional[Any] = None,  # Phase 3 å…¼å®¹ï¼šæ¥å—ä½†å¿½ç•¥
        config: Optional[Any] = None  # Phase 3 å…¼å®¹ï¼šæ¥å—ä½†å¿½ç•¥
    ) -> List[RetrievalResult]:
        """æ‰§è¡Œæ£€ç´¢ - Phase 3.6 æ”¯æŒå¹¶è¡Œä¸‰è·¯å¬å› + RRF èåˆ
        
        Note: temporal_context å’Œ config å‚æ•°ç”¨äº Phase 3 å…¼å®¹ï¼Œ
        åœ¨ EightLayerRetriever ä¸­è¢«å¿½ç•¥ï¼Œä»… ElevenLayerRetriever ä½¿ç”¨ã€‚
        """
        # Phase 3.6: æ ¹æ®é…ç½®é€‰æ‹©å¹¶è¡Œæˆ–ä¸²è¡Œæ¨¡å¼
        if self.config.get('parallel_recall_enabled', True):
            return self._parallel_recall(query, entities, keywords, top_k)
        else:
            return self._legacy_retrieve(query, entities, keywords, top_k, filters)
    
    def _parallel_recall(
        self,
        query: str,
        entities: Optional[List[str]],
        keywords: Optional[List[str]],
        top_k: int
    ) -> List[RetrievalResult]:
        """Phase 3.6: å¹¶è¡Œå››è·¯å¬å›å®ç°ï¼ˆä¿è¯100%å¬å›ï¼‰
        
        1. å¹¶è¡Œæ‰§è¡Œå››è·¯å¬å›ï¼ˆè¯­ä¹‰ã€å…³é”®è¯ã€å®ä½“ã€N-gramåŸæ–‡ï¼‰
        2. RRF èåˆå–å¹¶é›†
        3. é‡æ’åº
        
        å…³é”®æ”¹è¿›ï¼šN-gram ä½œä¸ºç¬¬å››è·¯å‚ä¸ RRF èåˆï¼Œè€Œä¸æ˜¯ä»…ä½œä¸ºåå¤‡ï¼Œ
        ç¡®ä¿å³ä½¿å…¶ä»–ä¸‰è·¯æ¼æ‰çš„å†…å®¹ä¹Ÿèƒ½é€šè¿‡åŸæ–‡åŒ¹é…æ‰¾åˆ°ï¼ˆ100%ä¸é—å¿˜ï¼‰ã€‚
        """
        self.stats = []
        
        # 1. å¹¶è¡Œæ‰§è¡Œå››è·¯å¬å›ï¼ˆN-gram ä½œä¸ºæ­£å¼çš„ç¬¬å››è·¯ï¼‰
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
                executor.submit(self._vector_recall, query, top_k * 2): 'vector',
                executor.submit(self._keyword_recall, keywords, top_k * 2): 'keyword',
                executor.submit(self._entity_recall, entities, top_k * 2): 'entity',
                executor.submit(self._ngram_recall, query, top_k * 2): 'ngram',
            }
            
            all_results: Dict[str, List[Tuple[str, float]]] = {}
            for future in as_completed(futures, timeout=15.0):
                source = futures[future]
                try:
                    all_results[source] = future.result()
                except Exception as e:
                    all_results[source] = []
                    _safe_print(f"[Retriever] {source} å¬å›å¤±è´¥: {e}")
        
        # 2. RRF å››è·¯èåˆï¼ˆN-gram æƒé‡è¾ƒé«˜ï¼Œç¡®ä¿100%å¬å›ä¼˜å…ˆçº§ï¼‰
        fused = reciprocal_rank_fusion(
            [
                all_results.get('vector', []),
                all_results.get('keyword', []),
                all_results.get('entity', []),
                all_results.get('ngram', []),
            ],
            k=self.config.get('rrf_k', 60),
            weights=[
                self.config.get('vector_weight', 1.0),
                self.config.get('keyword_weight', 1.2),
                self.config.get('entity_weight', 1.0),
                self.config.get('ngram_weight', 1.5),  # N-gramæƒé‡æ›´é«˜ï¼Œç¡®ä¿ç²¾ç¡®åŒ¹é…ä¼˜å…ˆ
            ]
        )
        
        # 3. æ„å»ºç»“æœå¯¹è±¡
        results: List[RetrievalResult] = []
        for doc_id, score in fused[:top_k * 2]:
            content = self.get_content(doc_id)
            if content:
                results.append(RetrievalResult(
                    id=doc_id,
                    content=content,
                    score=score,
                    source_layer=RetrievalLayer.L7_RERANK
                ))
        
        # 4. ç²¾æ’ + é‡æ’åº
        if self.config['l7_enabled'] and results:
            results = self._rerank(results, query, entities, keywords)
        
        return results[:top_k]
    
    def _ngram_recall(self, query: str, top_k: int) -> List[Tuple[str, float]]:
        """è·¯å¾„ 4: N-gram åŸæ–‡å¬å›ï¼ˆ100% ç²¾ç¡®åŒ¹é…ä¿è¯ï¼‰
        
        é€šè¿‡æ‰«æåŸæ–‡è¿›è¡Œå­ä¸²åŒ¹é…ï¼Œç¡®ä¿ä»»ä½•è¯´è¿‡çš„è¯éƒ½èƒ½è¢«æ‰¾åˆ°ã€‚
        è¿™æ˜¯"100%ä¸é—å¿˜"æ‰¿è¯ºçš„æ ¸å¿ƒä¿éšœã€‚
        """
        if not self.ngram_index:
            return []
        
        start = time.time()
        
        # ä½¿ç”¨å¹¶è¡Œæ‰«æåŠ é€Ÿ
        if self.config.get('fallback_parallel', True) and hasattr(self.ngram_index, 'raw_search_parallel'):
            doc_ids = self.ngram_index.raw_search_parallel(
                query,
                max_results=top_k,
                num_workers=self.config.get('fallback_workers', 4)
            )
        else:
            doc_ids = self.ngram_index.raw_search(query, max_results=top_k)
        
        # N-gram åŒ¹é…åˆ°çš„ç»“æœç»™äºˆè¾ƒé«˜åˆ†æ•°ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
        results = [(doc_id, 0.85) for doc_id in doc_ids]
        
        self._record_stats(RetrievalLayer.L4_NGRAM_INDEX, 0, len(results), start)
        return results
    
    def _vector_recall(self, query: str, top_k: int) -> List[Tuple[str, float]]:
        """è·¯å¾„ 1: è¯­ä¹‰å‘é‡å¬å›
        
        å…¼å®¹ä¸¤ç§å‘é‡ç´¢å¼•ï¼š
        - VectorIndex: search(query: str) - å†…éƒ¨è‡ªåŠ¨ encode
        - VectorIndexIVF: search(embedding: List[float]) - éœ€è¦å¤–éƒ¨ encode
        """
        if not self.vector_index or not getattr(self.vector_index, 'enabled', True):
            return []
        
        start = time.time()
        results = []
        
        try:
            # æ£€æŸ¥ç´¢å¼•ç±»å‹ï¼Œå…¼å®¹ä¸åŒçš„ API
            if hasattr(self.vector_index, 'encode'):
                # VectorIndex: æ”¯æŒå­—ç¬¦ä¸²æŸ¥è¯¢ï¼ˆå†…éƒ¨æœ‰ encode æ–¹æ³•ï¼‰
                results = self.vector_index.search(query, top_k=top_k)
            else:
                # VectorIndexIVF: éœ€è¦ä¼ å…¥å‘é‡
                if hasattr(self, 'embedding_backend') and self.embedding_backend:
                    query_embedding = self.embedding_backend.encode(query)
                    results = self.vector_index.search(query_embedding, top_k=top_k)
                else:
                    _safe_print("[Retriever] Warning: No embedding_backend for VectorIndexIVF")
                    return []
        except Exception as e:
            _safe_print(f"[Retriever] Vector recall failed: {e}")
            results = []
        
        self._record_stats(RetrievalLayer.L5_VECTOR_COARSE, 0, len(results), start)
        return results
    
    def _keyword_recall(self, keywords: Optional[List[str]], top_k: int) -> List[Tuple[str, float]]:
        """è·¯å¾„ 2: å…³é”®è¯å€’æ’ç´¢å¼•å¬å›ï¼ˆ100% å¬å›ï¼‰"""
        if not self.inverted_index or not keywords:
            return []
        
        start = time.time()
        
        # ä½¿ç”¨å¸ƒéš†è¿‡æ»¤å™¨é¢„è¿‡æ»¤
        filtered_keywords = keywords
        if self.bloom_filter:
            filtered_keywords = [kw for kw in keywords if kw in self.bloom_filter]
        
        if not filtered_keywords:
            return []
        
        # è·å–æ¯ä¸ªå…³é”®è¯åŒ¹é…çš„æ–‡æ¡£
        doc_keyword_counts: Dict[str, int] = defaultdict(int)
        for kw in filtered_keywords:
            # ä½¿ç”¨ search_any å¦‚æœå­˜åœ¨ï¼Œå¦åˆ™ä½¿ç”¨ search
            if hasattr(self.inverted_index, 'search_any'):
                matched_docs = self.inverted_index.search_any([kw])
            else:
                matched_docs = self.inverted_index.search(kw)
            for doc_id in matched_docs:
                doc_keyword_counts[doc_id] += 1
        
        # è®¡ç®—åˆ†æ•°ï¼šåŒ¹é…å…³é”®è¯æ•° / æ€»å…³é”®è¯æ•° * åŸºç¡€åˆ†
        base_score = 0.8
        results = []
        for doc_id, match_count in doc_keyword_counts.items():
            score = base_score * (match_count / len(filtered_keywords))
            results.append((doc_id, score))
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: -x[1])
        
        self._record_stats(RetrievalLayer.L2_INVERTED_INDEX, 0, len(results), start)
        return results[:top_k]
    
    def _entity_recall(self, entities: Optional[List[str]], top_k: int) -> List[Tuple[str, float]]:
        """è·¯å¾„ 3: å®ä½“ç´¢å¼•å¬å›"""
        if not self.entity_index or not entities:
            return []
        
        start = time.time()
        doc_ids: Set[str] = set()
        
        for entity in entities:
            entity_results = self.entity_index.get_related_turns(entity)
            for indexed_entity in entity_results:
                doc_ids.update(indexed_entity.turn_references)
        
        results = [(doc_id, 0.7) for doc_id in list(doc_ids)[:top_k]]
        
        self._record_stats(RetrievalLayer.L3_ENTITY_INDEX, 0, len(results), start)
        return results
    
    def _raw_text_fallback(self, query: str, top_k: int) -> List[Tuple[str, float]]:
        """åŸæ–‡å…œåº•æœç´¢ï¼ˆ100% ä¿è¯ï¼Œä»…åœ¨å…¶ä»–è·¯å¾„æ— ç»“æœæ—¶ä½¿ç”¨ï¼‰"""
        if not self.ngram_index:
            return []
        
        start = time.time()
        
        if self.config.get('fallback_parallel', True) and hasattr(self.ngram_index, 'raw_search_parallel'):
            doc_ids = self.ngram_index.raw_search_parallel(
                query,
                max_results=top_k,
                num_workers=self.config.get('fallback_workers', 4)
            )
        else:
            doc_ids = self.ngram_index.raw_search(query, max_results=top_k)
        
        results = [(doc_id, 0.3) for doc_id in doc_ids]  # å…œåº•ç»“æœåˆ†æ•°è¾ƒä½
        
        self._record_stats(RetrievalLayer.L4_NGRAM_INDEX, 0, len(results), start)
        return results
    
    def _legacy_retrieve(
        self,
        query: str,
        entities: Optional[List[str]] = None,
        keywords: Optional[List[str]] = None,
        top_k: int = 10,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[RetrievalResult]:
        """åŸæœ‰ä¸²è¡Œå…«å±‚æ£€ç´¢ï¼ˆå‘åå…¼å®¹ï¼‰"""
        self.stats = []
        candidates: Set[str] = set()  # å€™é€‰IDé›†åˆ
        results: List[RetrievalResult] = []
        
        # L1: å¸ƒéš†è¿‡æ»¤å™¨ï¼ˆé¢„è¿‡æ»¤å…³é”®è¯ï¼‰
        filtered_keywords = keywords or []
        if self.config['l1_enabled'] and self.bloom_filter and keywords:
            start = time.time()
            # å¸ƒéš†è¿‡æ»¤å™¨ç”¨äºå¿«é€Ÿæ’é™¤ä¸å­˜åœ¨çš„å…³é”®è¯
            # ä½¿ç”¨ in è¿ç®—ç¬¦æ£€æŸ¥ï¼ˆBloomFilter å®ç°äº† __contains__ï¼‰
            filtered_keywords = [
                kw for kw in keywords 
                if kw in self.bloom_filter
            ]
            self._record_stats(RetrievalLayer.L1_BLOOM_FILTER, len(keywords), len(filtered_keywords), start)
        
        # L2: å€’æ’ç´¢å¼•
        if self.config['l2_enabled'] and self.inverted_index:
            start = time.time()
            input_count = len(candidates)
            
            # ä½¿ç”¨è¿‡æ»¤åçš„å…³é”®è¯ï¼ˆå¦‚æœL1å¯ç”¨ï¼‰æˆ–åŸå§‹å…³é”®è¯
            search_keywords = filtered_keywords if filtered_keywords else keywords
            if search_keywords:
                # ä½¿ç”¨ search_any æ¥æœç´¢ä»»ä¸€å…³é”®è¯
                inverted_results = self.inverted_index.search_any(search_keywords)
                candidates.update(inverted_results)
            
            self._record_stats(RetrievalLayer.L2_INVERTED_INDEX, input_count, len(candidates), start)
        
        # L3: å®ä½“ç´¢å¼•
        if self.config['l3_enabled'] and self.entity_index and entities:
            start = time.time()
            input_count = len(candidates)
            
            for entity in entities:
                entity_results = self.entity_index.get_related_turns(entity)
                # entity_results æ˜¯ IndexedEntity åˆ—è¡¨ï¼Œéœ€è¦æå– turn_references
                for indexed_entity in entity_results:
                    candidates.update(indexed_entity.turn_references)
            
            self._record_stats(RetrievalLayer.L3_ENTITY_INDEX, input_count, len(candidates), start)
        
        # L4: N-gramç´¢å¼•
        if self.config['l4_enabled'] and self.ngram_index:
            start = time.time()
            input_count = len(candidates)
            
            ngram_results = self.ngram_index.search(query)
            # ngram_results æ˜¯ turn_id åˆ—è¡¨ï¼Œä¸æ˜¯å…ƒç»„
            for doc_id in ngram_results:
                candidates.add(doc_id)
            
            self._record_stats(RetrievalLayer.L4_NGRAM_INDEX, input_count, len(candidates), start)
        
        # L5: å‘é‡ç²—ç­›
        # æ£€æŸ¥å‘é‡ç´¢å¼•æ˜¯å¦å­˜åœ¨ä¸”å¯ç”¨ï¼ˆæ”¯æŒ Lite æ¨¡å¼ï¼‰
        vector_enabled = self.vector_index and getattr(self.vector_index, 'enabled', True)
        if self.config['l5_enabled'] and vector_enabled:
            start = time.time()
            input_count = len(candidates)
            
            vector_results = self.vector_index.search(
                query, 
                top_k=self.config['l5_top_k']
            )
            
            # åˆå¹¶å‘é‡ç»“æœ
            for doc_id, score in vector_results:
                candidates.add(doc_id)
                # è·å–å®é™…å†…å®¹
                content = self.get_content(doc_id)
                results.append(RetrievalResult(
                    id=doc_id,
                    content=content,
                    score=score,
                    source_layer=RetrievalLayer.L5_VECTOR_COARSE
                ))
            
            self._record_stats(RetrievalLayer.L5_VECTOR_COARSE, input_count, len(results), start)
        
        # å…³é”®ï¼šå°† L2-L4 æ”¶é›†çš„å€™é€‰ ID ä¹Ÿè½¬æ¢ä¸ºç»“æœ
        # è¿™ç¡®ä¿äº† Lite æ¨¡å¼ï¼ˆæ— å‘é‡ç´¢å¼•ï¼‰ä¹Ÿèƒ½è¿”å›ç»“æœ
        seen_ids = {r.id for r in results}
        for doc_id in candidates:
            if doc_id not in seen_ids:
                content = self.get_content(doc_id)
                if content:  # åªæ·»åŠ æœ‰å†…å®¹çš„ç»“æœ
                    results.append(RetrievalResult(
                        id=doc_id,
                        content=content,
                        score=0.5,  # å…³é”®è¯/å®ä½“åŒ¹é…çš„åŸºç¡€åˆ†æ•°
                        source_layer=RetrievalLayer.L4_NGRAM_INDEX
                    ))
                    seen_ids.add(doc_id)
        
        # L6: å‘é‡ç²¾æ’ï¼ˆå®Œæ•´å®ç° - é‡æ–°è®¡ç®—ç²¾ç¡®å‘é‡è·ç¦»ï¼‰
        if self.config['l6_enabled'] and vector_enabled and results:
            start = time.time()
            input_count = len(results)
            
            # å®Œæ•´å®ç°ï¼šé‡æ–°è®¡ç®—æŸ¥è¯¢ä¸å€™é€‰æ–‡æ¡£çš„ç²¾ç¡®ä½™å¼¦ç›¸ä¼¼åº¦
            results = self._vector_fine_ranking(query, results)
            results = results[:self.config['l6_top_k']]
            
            for r in results:
                r.source_layer = RetrievalLayer.L6_VECTOR_FINE
            
            self._record_stats(RetrievalLayer.L6_VECTOR_FINE, input_count, len(results), start)
        
        # L7: é‡æ’åº
        if self.config['l7_enabled'] and results:
            start = time.time()
            input_count = len(results)
            
            results = self._rerank(results, query, entities, keywords)
            results = results[:self.config['l7_top_k']]
            
            for r in results:
                r.source_layer = RetrievalLayer.L7_RERANK
            
            self._record_stats(RetrievalLayer.L7_RERANK, input_count, len(results), start)
        
        # L8: LLMè¿‡æ»¤
        if self.config['l8_enabled'] and self.llm_client and results:
            start = time.time()
            input_count = len(results)
            
            results = self._llm_filter(results, query)
            results = results[:self.config['l8_top_k']]
            
            self._record_stats(RetrievalLayer.L8_LLM_FILTER, input_count, len(results), start)
        
        # ç»ˆæå…œåº•ï¼šå¦‚æœæ‰€æœ‰å±‚éƒ½æ²¡æ‰¾åˆ°ç»“æœï¼Œä½¿ç”¨ N-gram åŸæ–‡æœç´¢
        # è¿™ç¡®ä¿äº†"100%ä¸é—å¿˜"â€”â€”åªè¦åŸæ–‡ä¸­åŒ…å«æŸ¥è¯¢å†…å®¹å°±èƒ½æ‰¾åˆ°
        if not results and self.ngram_index:
            start = time.time()
            # è°ƒç”¨ raw_search ç›´æ¥æœç´¢åŸæ–‡
            if hasattr(self.ngram_index, 'raw_search'):
                fallback_ids = self.ngram_index.raw_search(query, max_results=top_k)
            else:
                fallback_ids = self.ngram_index.search(query)
            
            for doc_id in fallback_ids[:top_k]:
                content = self.get_content(doc_id)
                if content:
                    results.append(RetrievalResult(
                        id=doc_id,
                        content=content,
                        score=0.3,  # å…œåº•æœç´¢çš„åŸºç¡€åˆ†æ•°
                        source_layer=RetrievalLayer.L4_NGRAM_INDEX,
                        metadata={'fallback': True}  # æ ‡è®°ä¸ºå…œåº•ç»“æœ
                    ))
            
            if results:
                self._record_stats(RetrievalLayer.L4_NGRAM_INDEX, 0, len(results), start)
        
        return results[:top_k]
    
    def _vector_fine_ranking(
        self,
        query: str,
        results: List[RetrievalResult]
    ) -> List[RetrievalResult]:
        """L6 å‘é‡ç²¾æ’ - ä½¿ç”¨å·²å­˜å‚¨çš„å‘é‡è®¡ç®—ç²¾ç¡®ç›¸ä¼¼åº¦
        
        ä¼˜åŒ–ï¼šç›´æ¥ä» FAISS ç´¢å¼•è·å–å·²å­˜å‚¨çš„æ–‡æ¡£å‘é‡ï¼Œ
        é¿å…é‡å¤è°ƒç”¨ encode() APIï¼ˆè¿™ä¼šæ¶ˆè€—å¤§é‡é…é¢ï¼‰ã€‚
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            results: L5 ç²—ç­›åçš„å€™é€‰ç»“æœ
        
        Returns:
            æŒ‰ç²¾ç¡®ç›¸ä¼¼åº¦é‡æ–°æ’åºçš„ç»“æœ
        """
        if not self.vector_index or not results:
            # æ²¡æœ‰å‘é‡ç´¢å¼•ï¼Œå›é€€åˆ°ç®€å•æ’åº
            return sorted(results, key=lambda r: -r.score)
        
        try:
            import numpy as np
            
            # 1. è·å–æŸ¥è¯¢å‘é‡ï¼ˆåªéœ€è¦ç¼–ç ä¸€æ¬¡ï¼‰
            query_embedding = self.vector_index.encode(query)
            query_embedding = query_embedding / np.linalg.norm(query_embedding)  # å½’ä¸€åŒ–
            
            # 2. æ‰¹é‡è·å–å€™é€‰æ–‡æ¡£çš„å·²å­˜å‚¨å‘é‡ï¼ˆä¸è°ƒç”¨ APIï¼ï¼‰
            doc_ids = [r.id for r in results]
            stored_vectors = {}
            if hasattr(self.vector_index, 'get_vectors_by_doc_ids'):
                stored_vectors = self.vector_index.get_vectors_by_doc_ids(doc_ids)
            
            # 3. å¯¹æ¯ä¸ªå€™é€‰æ–‡æ¡£è®¡ç®—ç²¾ç¡®ç›¸ä¼¼åº¦
            for result in results:
                doc_id = result.id
                
                # ä¼˜å…ˆä½¿ç”¨å·²å­˜å‚¨çš„å‘é‡
                if doc_id in stored_vectors:
                    doc_embedding = stored_vectors[doc_id]
                    doc_embedding = doc_embedding / np.linalg.norm(doc_embedding)  # å½’ä¸€åŒ–
                    
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    cosine_sim = float(np.dot(query_embedding, doc_embedding))
                    
                    # æ›´æ–°åˆ†æ•°
                    result.score = 0.7 * cosine_sim + 0.3 * result.score
                    result.metadata['l6_cosine_sim'] = cosine_sim
                    result.metadata['l6_source'] = 'stored_vector'
                else:
                    # æ²¡æœ‰å­˜å‚¨çš„å‘é‡ï¼ˆå¯èƒ½æ˜¯æ—§æ•°æ®ï¼‰ï¼Œä¿æŒåŸåˆ†æ•°
                    result.metadata['l6_source'] = 'no_vector'
            
            # 4. æŒ‰æ–°åˆ†æ•°æ’åº
            return sorted(results, key=lambda r: -r.score)
            
        except Exception as e:
            # æ•´ä½“å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ’åº
            _safe_print(f"[EightLayerRetriever] L6 ç²¾æ’å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ’åº: {e}")
            return sorted(results, key=lambda r: -r.score)
    
    def _rerank(
        self,
        results: List[RetrievalResult],
        query: str,
        entities: Optional[List[str]],
        keywords: Optional[List[str]]
    ) -> List[RetrievalResult]:
        """é‡æ’åº"""
        for result in results:
            bonus = 0.0
            
            # å®ä½“åŒ¹é…åŠ åˆ†
            if entities:
                matched_entities = set(result.entities) & set(entities)
                bonus += len(matched_entities) * 0.1
            
            # å…³é”®è¯åŒ¹é…åŠ åˆ†
            if keywords:
                for kw in keywords:
                    if kw.lower() in result.content.lower():
                        bonus += 0.05
            
            # æ–°é²œåº¦åŠ åˆ†ï¼ˆå¦‚æœæœ‰æ—¶é—´æˆ³ï¼‰
            if 'timestamp' in result.metadata:
                recency = time.time() - result.metadata['timestamp']
                if recency < 3600:  # 1å°æ—¶å†…
                    bonus += 0.1
                elif recency < 86400:  # 1å¤©å†…
                    bonus += 0.05
            
            result.score += bonus
        
        return sorted(results, key=lambda r: -r.score)
    
    def _llm_filter(
        self,
        results: List[RetrievalResult],
        query: str
    ) -> List[RetrievalResult]:
        """ä½¿ç”¨LLMè¿‡æ»¤ä¸ç›¸å…³ç»“æœ"""
        if not self.llm_client:
            return results
        
        prompt = f"""åˆ¤æ–­ä»¥ä¸‹è®°å¿†æ˜¯å¦ä¸æŸ¥è¯¢ç›¸å…³ã€‚

æŸ¥è¯¢ï¼š{query}

è®°å¿†åˆ—è¡¨ï¼š
{chr(10).join([f"{i+1}. {r.content[:100]}" for i, r in enumerate(results)])}

è¯·è¿”å›ç›¸å…³è®°å¿†çš„ç¼–å·ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼Œå¦‚æœéƒ½ä¸ç›¸å…³è¿”å›"æ— "ï¼š"""
        
        try:
            response = self.llm_client.complete(prompt, max_tokens=50)
            
            if response.strip() == "æ— ":
                return []
            
            # è§£æè¿”å›çš„ç¼–å·
            indices = [int(x.strip()) - 1 for x in response.split(',') if x.strip().isdigit()]
            return [results[i] for i in indices if 0 <= i < len(results)]
        
        except Exception:
            return results
    
    def _record_stats(self, layer: RetrievalLayer, input_count: int, output_count: int, start_time: float):
        """è®°å½•å±‚çº§ç»Ÿè®¡"""
        self.stats.append(LayerStats(
            layer=layer,
            input_count=input_count,
            output_count=output_count,
            time_ms=(time.time() - start_time) * 1000,
            filtered_count=input_count - output_count if input_count > output_count else 0
        ))
    
    def get_stats_summary(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        total_time = sum(s.time_ms for s in self.stats)
        return {
            'total_time_ms': total_time,
            'layers': [
                {
                    'layer': s.layer.value,
                    'input': s.input_count,
                    'output': s.output_count,
                    'time_ms': s.time_ms,
                    'filtered': s.filtered_count
                }
                for s in self.stats
            ]
        }

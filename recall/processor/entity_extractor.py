"""å®ä½“æå–å™¨ - NLPé©±åŠ¨çš„å®ä½“è¯†åˆ«"""

import re
import os
from typing import List, Set
from dataclasses import dataclass


# Windows GBK ç¼–ç å…¼å®¹çš„å®‰å…¨æ‰“å°å‡½æ•°
def _safe_print(msg: str) -> None:
    """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œæ›¿æ¢ emoji ä¸º ASCII ç­‰ä»·ç‰©ä»¥é¿å… Windows GBK ç¼–ç é”™è¯¯"""
    emoji_map = {
        'ğŸ“¥': '[IN]', 'ğŸ“¤': '[OUT]', 'ğŸ”': '[SEARCH]', 'âœ…': '[OK]', 'âŒ': '[FAIL]',
        'âš ï¸': '[WARN]', 'ğŸ’¾': '[SAVE]', 'ğŸ—ƒï¸': '[DB]', 'ğŸ§¹': '[CLEAN]', 'ğŸ“Š': '[STATS]',
        'ğŸ”„': '[SYNC]', 'ğŸ“¦': '[PKG]', 'ğŸš€': '[START]', 'ğŸ¯': '[TARGET]', 'ğŸ’¡': '[HINT]',
        'ğŸ”§': '[FIX]', 'ğŸ“': '[NOTE]', 'ğŸ‰': '[DONE]', 'â±ï¸': '[TIME]', 'ğŸŒ': '[NET]',
        'ğŸ§ ': '[BRAIN]', 'ğŸ’¬': '[CHAT]', 'ğŸ·ï¸': '[TAG]', 'ğŸ“': '[DIR]', 'ğŸ”’': '[LOCK]',
        'ğŸŒ±': '[PLANT]', 'ğŸ—‘ï¸': '[DEL]', 'ğŸ’«': '[MAGIC]', 'ğŸ­': '[MASK]', 'ğŸ“–': '[BOOK]',
        'âš¡': '[FAST]', 'ğŸ”¥': '[HOT]', 'ğŸ’': '[GEM]', 'ğŸŒŸ': '[STAR]', 'ğŸ¨': '[ART]'
    }
    for emoji, ascii_equiv in emoji_map.items():
        msg = msg.replace(emoji, ascii_equiv)
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode('ascii', errors='replace').decode('ascii'))


@dataclass
class ExtractedEntity:
    """æå–çš„å®ä½“"""
    name: str
    entity_type: str  # PERSON, LOCATION, ITEM, ORG, CODE_SYMBOL
    confidence: float
    source_text: str


class EntityExtractor:
    """å®ä½“æå–å™¨"""
    
    def __init__(self):
        # å»¶è¿ŸåŠ è½½
        self._nlp = None
        self._jieba = None
        
        # åœç”¨è¯
        self.stopwords = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'æœ‰', 'è¿™', 'é‚£', 'å°±', 'éƒ½', 
            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would'
        }
    
    @property
    def nlp(self):
        """æ‡’åŠ è½½ spaCy æ¨¡å‹"""
        if self._nlp is None:
            self._nlp = self._load_spacy_model()
        return self._nlp
    
    @property
    def jieba(self):
        """æ‡’åŠ è½½ jiebaï¼ˆç¼“å­˜åˆ°é¡¹ç›®ç›®å½•ï¼‰"""
        if self._jieba is None:
            import jieba
            # è®¾ç½® jieba ç¼“å­˜åˆ°é¡¹ç›®ç›®å½•ï¼Œä¸æ±¡æŸ“ç³»ç»Ÿ
            from ..init import RecallInit
            cache_dir = os.path.join(RecallInit.get_data_root(), 'cache')
            os.makedirs(cache_dir, exist_ok=True)
            jieba.dt.tmp_dir = cache_dir  # è®¾ç½®ä¸´æ—¶ç›®å½•
            self._jieba = jieba
        return self._jieba
    
    def _load_spacy_model(self):
        """åŠ è½½ spaCy æ¨¡å‹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬åœ°ç›®å½•ï¼ˆä¸æ±¡æŸ“å…¨å±€ï¼‰"""
        import spacy
        
        # è‡ªå®šä¹‰æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆéš”ç¦»åˆ°é¡¹ç›®ç›®å½• ./recall_data/models/ï¼‰
        from ..init import RecallInit
        model_cache_dir = os.path.join(RecallInit.get_data_root(), 'models', 'spacy')
        os.makedirs(model_cache_dir, exist_ok=True)
        
        last_error = "æœªæ‰¾åˆ°æ¨¡å‹"  # åˆå§‹åŒ–é”™è¯¯ä¿¡æ¯
        
        # ä¼˜å…ˆå°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
        for model_name in ['zh_core_web_sm', 'en_core_web_sm']:
            local_model_path = os.path.join(model_cache_dir, model_name)
            
            # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰æ¨¡å‹
            if os.path.exists(local_model_path):
                try:
                    return spacy.load(local_model_path)
                except Exception as e:
                    last_error = str(e)
            
            # å°è¯•ä»å…¨å±€åŠ è½½ï¼ˆå¦‚æœç”¨æˆ·å·²å®‰è£…ï¼‰
            try:
                return spacy.load(model_name)
            except Exception as e:
                # è®°å½•å…·ä½“é”™è¯¯ä¾¿äºè°ƒè¯•
                last_error = str(e)
        
        # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨ç©ºç™½æ¨¡å‹ï¼ˆåŸºç¡€åŠŸèƒ½ä»å¯ç”¨ï¼‰
        _safe_print(f"[Recall] è­¦å‘Šï¼šæ— æ³•åŠ è½½ NLP æ¨¡å‹ï¼Œå®ä½“è¯†åˆ«åŠŸèƒ½å°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ (åŸå› : {last_error})")
        return spacy.blank('zh')  # ç©ºç™½æ¨¡å‹ï¼Œåªæœ‰åˆ†è¯ï¼Œæ²¡æœ‰NER
    
    def extract(self, text: str) -> List[ExtractedEntity]:
        """æå–å®ä½“"""
        entities = []
        
        # é™åˆ¶å¤„ç†é•¿åº¦ï¼Œé¿å…æ€§èƒ½é—®é¢˜
        max_len = 10000
        truncated_text = text[:max_len] if len(text) > max_len else text
        
        # 1. ä½¿ç”¨spaCyæå–å‘½åå®ä½“
        doc = self.nlp(truncated_text)
        for ent in doc.ents:
            entity_type = self._map_spacy_label(ent.label_)
            if entity_type:
                entities.append(ExtractedEntity(
                    name=ent.text,
                    entity_type=entity_type,
                    confidence=0.8,
                    source_text=ent.sent.text if ent.sent else truncated_text[:100]
                ))
        
        # 2. ä¸­æ–‡ä¸“åæå–ï¼ˆå¼•å·å†…å®¹ã€ä¹¦åå·å†…å®¹ï¼‰
        quoted = re.findall(r'[ã€Œã€"\'](.*?)[ã€ã€"\']', truncated_text)
        for name in quoted:
            if 2 <= len(name) <= 20:
                entities.append(ExtractedEntity(
                    name=name,
                    entity_type='ITEM' if len(name) <= 4 else 'MISC',
                    confidence=0.6,
                    source_text=truncated_text[:100]
                ))
        
        # 3. ä»£ç ç¬¦å·æå–ï¼ˆé™åˆ¶é•¿åº¦é¿å…åŒ¹é…è¶…é•¿å­—ç¬¦ä¸²ï¼‰
        # åªåŒ¹é… 3-50 å­—ç¬¦çš„ç¬¦å·
        code_symbols = re.findall(r'\b([A-Z][a-zA-Z0-9_]{2,49})\b', truncated_text)  # CamelCase
        code_symbols += re.findall(r'\b([a-z_][a-zA-Z0-9_]{2,49})\b', truncated_text)  # snake_case
        for symbol in set(code_symbols):
            if not symbol.lower() in self.stopwords:
                entities.append(ExtractedEntity(
                    name=symbol,
                    entity_type='CODE_SYMBOL',
                    confidence=0.5,
                    source_text=truncated_text[:100]
                ))
        
        # å»é‡
        seen: Set[str] = set()
        unique_entities = []
        for e in entities:
            if e.name.lower() not in seen:
                seen.add(e.name.lower())
                unique_entities.append(e)
        
        return unique_entities
    
    def extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        keywords = []
        
        # é™åˆ¶å¤„ç†é•¿åº¦
        max_len = 10000
        truncated_text = text[:max_len] if len(text) > max_len else text
        
        # jiebaåˆ†è¯æå–
        words = self.jieba.cut(truncated_text)
        for word in words:
            # é™åˆ¶å…³é”®è¯é•¿åº¦åœ¨ 2-50 å­—ç¬¦
            if 2 <= len(word) <= 50 and word not in self.stopwords:
                keywords.append(word)
        
        # è‹±æ–‡å…³é”®è¯ï¼ˆé™åˆ¶é•¿åº¦ 3-50 å­—ç¬¦ï¼‰
        english_words = re.findall(r'[a-zA-Z]{3,50}', truncated_text)
        keywords.extend([w.lower() for w in english_words if w.lower() not in self.stopwords])
        
        return list(set(keywords))
    
    def _map_spacy_label(self, label: str) -> str:
        """æ˜ å°„spaCyæ ‡ç­¾åˆ°æˆ‘ä»¬çš„ç±»å‹"""
        mapping = {
            'PERSON': 'PERSON',
            'PER': 'PERSON',
            'GPE': 'LOCATION',
            'LOC': 'LOCATION',
            'ORG': 'ORG',
            'PRODUCT': 'ITEM',
            'WORK_OF_ART': 'ITEM',
        }
        return mapping.get(label, None)

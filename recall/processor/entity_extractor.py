"""å®ä½“æå–å™¨ - NLPé©±åŠ¨çš„å®ä½“è¯†åˆ«"""

import re
import os
from typing import List, Set, Dict
from dataclasses import dataclass, field


# Windows GBK ç¼–ç å…¼å®¹çš„å®‰å…¨æ‰“å°å‡½æ•°
def _safe_print(msg: str) -> None:
    """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œæ›¿æ¢ emoji ä¸º ASCII ç­‰ä»·ç‰©ä»¥é¿å… Windows GBK ç¼–ç é”™è¯¯"""
    emoji_map = {
        'ğŸ“¥': '[IN]', 'ğŸ“¤': '[OUT]', 'ğŸ”': '[SEARCH]', 'âœ…': '[OK]', 'âŒ': '[FAIL]',
        'âš ï¸': '[WARN]', 'ğŸ’¾': '[SAVE]', 'ğŸ—ƒï¸': '[DB]', 'ğŸ§¹': '[CLEAN]', 'ğŸ“Š': '[STATS]',
        'ğŸ”„': '[SYNC]', 'ğŸ“¦': '[PKG]', 'ğŸš€': '[START]', 'ğŸ¯': '[TARGET]', 'ğŸ’¡': '[HINT]',
        'ğŸ”§': '[FIX]', 'ğŸ“': '[NOTE]', 'ğŸ‰': '[DONE]', 'â±ï¸': '[TIME]', 'ğŸŒ': '[NET]',
        'ğŸ§ ': '[BRAIN]', 'ğŸ’¬': '[CHAT]', 'ğŸ·ï¸': '[TAG]', 'ğŸ“': '[DIR]', 'ğŸ”’': '[LOCK]',
        'ğŸŒ±': '[PLANT]', 'ğŸ—‘ï¸': '[DEL]', 'ğŸ’«': '[MAGIC]', 'ğŸ­': '[MASK]', 'ğŸ“–': '[BOOK]',
        'âš¡': '[FAST]', 'ğŸ”¥': '[HOT]', 'ğŸ’': '[GEM]', 'ğŸŒŸ': '[STAR]', 'ğŸ¨': '[ART]'
    }
    for emoji, ascii_equiv in emoji_map.items():
        msg = msg.replace(emoji, ascii_equiv)
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode('ascii', errors='replace').decode('ascii'))


@dataclass
class ExtractedEntity:
    """æå–çš„å®ä½“"""
    name: str
    entity_type: str  # PERSON, LOCATION, ITEM, ORG, CODE_SYMBOL
    confidence: float
    source_text: str
    aliases: List[str] = field(default_factory=list)  # å®ä½“åˆ«å


class EntityExtractor:
    """å®ä½“æå–å™¨"""
    
    def __init__(self):
        # å»¶è¿ŸåŠ è½½
        self._nlp = None
        self._jieba = None
        
        # åœç”¨è¯ï¼ˆæ‰©å±•ç‰ˆï¼‰
        self.stopwords = {
            # ä¸­æ–‡åœç”¨è¯
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'æœ‰', 'è¿™', 'é‚£', 'å°±', 'éƒ½', 'ä¹Ÿ', 'è¿˜', 'è¦',
            'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ',
            'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¯ä»¥', 'èƒ½å¤Ÿ', 'åº”è¯¥', 'æ¯”å¦‚', 'æ¯”å¦‚è¯´', 'ç„¶å', 'æ‰€ä»¥',
            'å› ä¸º', 'ä½†æ˜¯', 'å¦‚æœ', 'è™½ç„¶', 'ä¸è¿‡', 'è€Œä¸”', 'æˆ–è€…', 'ä»¥åŠ', 'é€šè¿‡',
            'ä¸€ä¸ª', 'ä¸€äº›', 'å¾ˆå¤š', 'éå¸¸', 'ç‰¹åˆ«', 'å…¶å®', 'åŸºæœ¬ä¸Š', 'å¤§æ¦‚', 'å¯èƒ½',
            'è¯¸å¦‚', 'ä¾‹å¦‚', 'åŒ…æ‹¬', 'ç­‰ç­‰', 'ä¹‹ç±»', 'è¿™ç§', 'é‚£ç§', 'å„ç§', 'æŸäº›',
            'ä¸­å¤', 'ç›®å‰', 'ç°åœ¨', 'ä»¥å‰', 'ä¹‹å‰', 'ä¹‹å', 'åæ¥', 'å½“æ—¶', 'é‚£æ—¶',
            # è‹±æ–‡åœç”¨è¯
            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
            'can', 'could', 'should', 'may', 'might', 'must',
            'this', 'that', 'these', 'those', 'it', 'its',
            'and', 'or', 'but', 'if', 'then', 'else', 'when', 'where', 'how', 'why',
            'for', 'to', 'from', 'with', 'by', 'at', 'in', 'on', 'of',
        }
        
        # éœ€è¦è¿‡æ»¤çš„è¯¯è¯†åˆ«è¯ï¼ˆjiebaåˆ†è¯æˆ–spaCyçš„å¸¸è§é”™è¯¯ï¼‰
        self.filter_words = {
            'è¯¸å¦‚', 'ä¾‹å¦‚', 'åŒ…æ‹¬', 'ç­‰ç­‰', 'ä¹‹ç±»', 'å’Œéªæ²³', 'æ¯”å¦‚è¯´',
            'ä¸­å¤', 'ç›®å‰', 'ç°åœ¨', 'ä»¥å‰', 'ä¹‹å‰', 'ä¹‹å', 'åæ¥', 'å½“æ—¶',
        }
        
        # å·²çŸ¥çš„å¹³å°/äº§å“/å“ç‰Œåï¼ˆç”¨äºå¢å¼ºè¯†åˆ«ï¼‰
        self.known_entities = {
            # ç”µå•†å¹³å°
            'é—²é±¼': 'ORG', 'æ·˜å®': 'ORG', 'äº¬ä¸œ': 'ORG', 'æ‹¼å¤šå¤š': 'ORG',
            'éªæ²³å±‹': 'ORG', 'ç…¤ç‚‰': 'ORG', 'é›…è™': 'ORG', 'ebay': 'ORG', 
            'amazon': 'ORG', 'etsy': 'ORG', 'shopify': 'ORG',
            # æŠ€æœ¯å¹³å°
            'github': 'ORG', 'n8n': 'ORG', 'æ‰£å­': 'ORG', 'coze': 'ORG',
            'discord': 'ORG', 'reddit': 'ORG', 'twitter': 'ORG', 'x': 'ORG',
            # å¨ƒåœˆå“ç‰Œ
            'bjd': 'CONCEPT', 'mjd': 'CONCEPT', 'mamachapp': 'ORG', 'azone': 'ORG',
            'volks': 'ORG', 'dollfie': 'ITEM',
            # AIç›¸å…³
            'ai': 'CONCEPT', 'chatgpt': 'ORG', 'claude': 'ORG', 'deepseek': 'ORG',
            'openai': 'ORG', 'anthropic': 'ORG',
        }
    
    @property
    def nlp(self):
        """æ‡’åŠ è½½ spaCy æ¨¡å‹"""
        if self._nlp is None:
            self._nlp = self._load_spacy_model()
        return self._nlp
    
    @property
    def jieba(self):
        """æ‡’åŠ è½½ jiebaï¼ˆç¼“å­˜åˆ°é¡¹ç›®ç›®å½•ï¼‰"""
        if self._jieba is None:
            import jieba
            # è®¾ç½® jieba ç¼“å­˜åˆ°é¡¹ç›®ç›®å½•ï¼Œä¸æ±¡æŸ“ç³»ç»Ÿ
            from ..init import RecallInit
            cache_dir = os.path.join(RecallInit.get_data_root(), 'cache')
            os.makedirs(cache_dir, exist_ok=True)
            jieba.dt.tmp_dir = cache_dir  # è®¾ç½®ä¸´æ—¶ç›®å½•
            self._jieba = jieba
        return self._jieba
    
    def _load_spacy_model(self):
        """åŠ è½½ spaCy æ¨¡å‹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬åœ°ç›®å½•ï¼ˆä¸æ±¡æŸ“å…¨å±€ï¼‰"""
        import spacy
        
        # è‡ªå®šä¹‰æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆéš”ç¦»åˆ°é¡¹ç›®ç›®å½• ./recall_data/models/ï¼‰
        from ..init import RecallInit
        model_cache_dir = os.path.join(RecallInit.get_data_root(), 'models', 'spacy')
        os.makedirs(model_cache_dir, exist_ok=True)
        
        last_error = "æœªæ‰¾åˆ°æ¨¡å‹"  # åˆå§‹åŒ–é”™è¯¯ä¿¡æ¯
        
        # ä¼˜å…ˆå°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
        for model_name in ['zh_core_web_sm', 'en_core_web_sm']:
            local_model_path = os.path.join(model_cache_dir, model_name)
            
            # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰æ¨¡å‹
            if os.path.exists(local_model_path):
                try:
                    return spacy.load(local_model_path)
                except Exception as e:
                    last_error = str(e)
            
            # å°è¯•ä»å…¨å±€åŠ è½½ï¼ˆå¦‚æœç”¨æˆ·å·²å®‰è£…ï¼‰
            try:
                return spacy.load(model_name)
            except Exception as e:
                # è®°å½•å…·ä½“é”™è¯¯ä¾¿äºè°ƒè¯•
                last_error = str(e)
        
        # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨ç©ºç™½æ¨¡å‹ï¼ˆåŸºç¡€åŠŸèƒ½ä»å¯ç”¨ï¼‰
        _safe_print(f"[Recall] è­¦å‘Šï¼šæ— æ³•åŠ è½½ NLP æ¨¡å‹ï¼Œå®ä½“è¯†åˆ«åŠŸèƒ½å°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ (åŸå› : {last_error})")
        return spacy.blank('zh')  # ç©ºç™½æ¨¡å‹ï¼Œåªæœ‰åˆ†è¯ï¼Œæ²¡æœ‰NER
    
    def extract(self, text: str) -> List[ExtractedEntity]:
        """æå–å®ä½“ - å¢å¼ºç‰ˆï¼Œç»“åˆå¤šç§æ–¹æ³•"""
        entities = []
        
        # é™åˆ¶å¤„ç†é•¿åº¦ï¼Œé¿å…æ€§èƒ½é—®é¢˜
        max_len = 10000
        truncated_text = text[:max_len] if len(text) > max_len else text
        text_lower = truncated_text.lower()
        
        # 1. å·²çŸ¥å®ä½“è¯å…¸åŒ¹é…ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        for name, entity_type in self.known_entities.items():
            if name.lower() in text_lower:
                # æ‰¾åˆ°åŸæ–‡ä¸­çš„å®é™…å¤§å°å†™å½¢å¼
                idx = text_lower.find(name.lower())
                if idx >= 0:
                    original_name = truncated_text[idx:idx+len(name)]
                    entities.append(ExtractedEntity(
                        name=original_name,
                        entity_type=entity_type,
                        confidence=0.9,
                        source_text=truncated_text[max(0,idx-20):idx+len(name)+20]
                    ))
        
        # 2. ä½¿ç”¨spaCyæå–å‘½åå®ä½“
        doc = self.nlp(truncated_text)
        for ent in doc.ents:
            # æ¸…ç†å®ä½“æ–‡æœ¬ï¼ˆå»é™¤å‰åå¤šä½™å­—ç¬¦ï¼‰
            clean_name = ent.text.strip()
            # è¿‡æ»¤æ‰å¤ªçŸ­æˆ–å¤ªé•¿çš„
            if len(clean_name) < 2 or len(clean_name) > 30:
                continue
            # è¿‡æ»¤æ‰çº¯æ•°å­—
            if clean_name.isdigit():
                continue
            # è¿‡æ»¤æ‰åœç”¨è¯å¼€å¤´çš„é”™è¯¯è¯†åˆ«ï¼ˆå¦‚"æ¯”å¦‚è¯´n"ï¼‰
            skip = False
            for sw in ['æ¯”å¦‚è¯´', 'æ¯”å¦‚', 'å°±æ˜¯', 'å…¶å®', 'ç„¶å']:
                if clean_name.startswith(sw):
                    # å°è¯•æå–åé¢çš„éƒ¨åˆ†
                    rest = clean_name[len(sw):].strip()
                    if rest and len(rest) >= 2:
                        clean_name = rest
                    else:
                        skip = True
                    break
            if skip:
                continue
                
            entity_type = self._map_spacy_label(ent.label_)
            if entity_type:
                entities.append(ExtractedEntity(
                    name=clean_name,
                    entity_type=entity_type,
                    confidence=0.8,
                    source_text=ent.sent.text if ent.sent else truncated_text[:100]
                ))
        
        # 3. ä¸­æ–‡ä¸“åæå–ï¼ˆå¼•å·å†…å®¹ã€ä¹¦åå·å†…å®¹ï¼‰
        quoted = re.findall(r'[ã€Œã€"\'ã€Š](.*?)[ã€ã€"\'ã€‹]', truncated_text)
        for name in quoted:
            name = name.strip()
            if 2 <= len(name) <= 20 and name.lower() not in self.stopwords:
                entities.append(ExtractedEntity(
                    name=name,
                    entity_type='ITEM' if len(name) <= 4 else 'CONCEPT',
                    confidence=0.7,
                    source_text=truncated_text[:100]
                ))
        
        # 4. è‹±æ–‡ä¸“æœ‰åè¯æå–ï¼ˆé¦–å­—æ¯å¤§å†™çš„è¯ã€å…¨å¤§å†™ç¼©å†™ï¼‰
        # åŒ¹é…å¦‚ GitHub, AI, BJD, MJD, SillyTavern ç­‰
        english_proper = re.findall(r'\b([A-Z][a-zA-Z0-9]{1,20})\b', truncated_text)
        english_abbr = re.findall(r'\b([A-Z]{2,10})\b', truncated_text)
        for word in set(english_proper + english_abbr):
            if word.lower() not in self.stopwords and len(word) >= 2:
                # åˆ¤æ–­ç±»å‹
                if word.isupper() and len(word) <= 5:
                    etype = 'CONCEPT'  # ç¼©å†™è¯å¦‚ AI, BJD
                else:
                    etype = 'ORG'  # é¦–å­—æ¯å¤§å†™å¯èƒ½æ˜¯ç»„ç»‡/äº§å“
                entities.append(ExtractedEntity(
                    name=word,
                    entity_type=etype,
                    confidence=0.6,
                    source_text=truncated_text[:100]
                ))
        
        # 5. æ··åˆè¯æå–ï¼ˆå¦‚ n8n, mamachapp ç­‰å°å†™ä¸“æœ‰åè¯ï¼‰
        # åŒ¹é… å­—æ¯+æ•°å­— æ··åˆ æˆ– è¿ç»­å°å†™ä½†çœ‹èµ·æ¥åƒä¸“æœ‰åè¯çš„
        mixed_words = re.findall(r'\b([a-z]+[0-9]+[a-z0-9]*|[0-9]+[a-z]+[a-z0-9]*)\b', truncated_text, re.IGNORECASE)
        for word in set(mixed_words):
            if word.lower() not in self.stopwords and len(word) >= 2:
                entities.append(ExtractedEntity(
                    name=word,
                    entity_type='ORG',
                    confidence=0.6,
                    source_text=truncated_text[:100]
                ))
        
        # 6. jieba åè¯çŸ­è¯­æå–ï¼ˆè¡¥å……ä¸­æ–‡å®ä½“ï¼‰
        try:
            import jieba.posseg as pseg
            words = pseg.cut(truncated_text)
            for word, flag in words:
                # nr=äººå, ns=åœ°å, nt=æœºæ„, nz=å…¶ä»–ä¸“å, n=åè¯
                if flag in ('nr', 'ns', 'nt', 'nz') and len(word) >= 2:
                    if word not in self.stopwords:
                        entity_type = {
                            'nr': 'PERSON',
                            'ns': 'LOCATION', 
                            'nt': 'ORG',
                            'nz': 'CONCEPT'
                        }.get(flag, 'CONCEPT')
                        entities.append(ExtractedEntity(
                            name=word,
                            entity_type=entity_type,
                            confidence=0.65,
                            source_text=truncated_text[:100]
                        ))
        except Exception:
            pass  # jieba.posseg å¯èƒ½ä¸å¯ç”¨
        
        # å»é‡ï¼ˆä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„ï¼‰+ è¿‡æ»¤è¯¯è¯†åˆ«
        seen: Dict[str, ExtractedEntity] = {}
        for e in entities:
            key = e.name.lower()
            # è¿‡æ»¤è¯¯è¯†åˆ«è¯
            if e.name in self.filter_words or key in self.filter_words:
                continue
            # è¿‡æ»¤å¤ªçŸ­çš„ï¼ˆå•å­—ç¬¦ï¼‰
            if len(e.name) < 2:
                continue
            # è¿‡æ»¤ä»¥"å’Œ"å¼€å¤´çš„é”™è¯¯åˆ†è¯ï¼ˆå¦‚"å’Œéªæ²³"ï¼‰
            if e.name.startswith('å’Œ') and len(e.name) > 2:
                continue
            if key not in seen or e.confidence > seen[key].confidence:
                seen[key] = e
        
        return list(seen.values())
    
    def extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        keywords = []
        
        # é™åˆ¶å¤„ç†é•¿åº¦
        max_len = 10000
        truncated_text = text[:max_len] if len(text) > max_len else text
        
        # jiebaåˆ†è¯æå–
        words = self.jieba.cut(truncated_text)
        for word in words:
            # é™åˆ¶å…³é”®è¯é•¿åº¦åœ¨ 2-50 å­—ç¬¦
            if 2 <= len(word) <= 50 and word not in self.stopwords:
                keywords.append(word)
        
        # è‹±æ–‡å…³é”®è¯ï¼ˆé™åˆ¶é•¿åº¦ 3-50 å­—ç¬¦ï¼‰
        english_words = re.findall(r'[a-zA-Z]{3,50}', truncated_text)
        keywords.extend([w.lower() for w in english_words if w.lower() not in self.stopwords])
        
        return list(set(keywords))
    
    def _map_spacy_label(self, label: str) -> str:
        """æ˜ å°„spaCyæ ‡ç­¾åˆ°æˆ‘ä»¬çš„ç±»å‹ï¼ˆæ‰©å±•ç‰ˆï¼‰"""
        mapping = {
            # äººç‰©
            'PERSON': 'PERSON',
            'PER': 'PERSON',
            # åœ°ç‚¹
            'GPE': 'LOCATION',      # åœ°ç¼˜æ”¿æ²»å®ä½“ï¼ˆå›½å®¶ã€åŸå¸‚ç­‰ï¼‰
            'LOC': 'LOCATION',      # éGPEåœ°ç‚¹
            'FAC': 'LOCATION',      # è®¾æ–½
            # ç»„ç»‡
            'ORG': 'ORG',
            'NORP': 'ORG',          # å›½ç±ã€å®—æ•™ã€æ”¿æ²»å›¢ä½“
            # ç‰©å“/ä½œå“
            'PRODUCT': 'ITEM',
            'WORK_OF_ART': 'ITEM',
            'EVENT': 'CONCEPT',     # äº‹ä»¶
            'LAW': 'CONCEPT',       # æ³•å¾‹æ–‡ä»¶
            # æ—¶é—´ï¼ˆå¯é€‰ä¿ç•™ï¼‰
            # 'DATE': 'TIME',
            # 'TIME': 'TIME',
            # æ•°é‡ï¼ˆé€šå¸¸ä¸éœ€è¦ï¼‰
            # 'MONEY': 'NUMBER',
            # 'QUANTITY': 'NUMBER',
            # 'CARDINAL': 'NUMBER',
            # 'ORDINAL': 'NUMBER',
            # 'PERCENT': 'NUMBER',
        }
        return mapping.get(label, None)

"""ç»Ÿä¸€åˆ†æå™¨ - Recall æ€§èƒ½ä¼˜åŒ– (v4.2)

è®¾è®¡ç†å¿µï¼š
1. ä¸€æ¬¡ LLM è°ƒç”¨å®Œæˆå¤šä¸ªåˆ†æä»»åŠ¡ï¼ˆçŸ›ç›¾æ£€æµ‹ã€å…³ç³»æå–ã€å®ä½“æ‘˜è¦ï¼‰
2. æä¾›å®Œæ•´ä¸Šä¸‹æ–‡ï¼Œæå‡åˆ†æè´¨é‡
3. å‘åå…¼å®¹ï¼šå¯é€‰å¯ç”¨ï¼Œä¸å½±å“ç°æœ‰åŠŸèƒ½
4. æ™ºèƒ½å›é€€ï¼šLLM å¤±è´¥æ—¶è‡ªåŠ¨å›é€€åˆ°ç‹¬ç«‹æ¨¡å—
"""

from __future__ import annotations

import json
import os
import re
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from enum import Enum

from ..utils.llm_client import LLMClient


class AnalysisTask(str, Enum):
    """åˆ†æä»»åŠ¡ç±»å‹"""
    CONTRADICTION = "contradiction"     # çŸ›ç›¾æ£€æµ‹
    RELATION = "relation"               # å…³ç³»æå–
    ENTITY_SUMMARY = "entity_summary"   # å®ä½“æ‘˜è¦


@dataclass
class UnifiedAnalysisInput:
    """ç»Ÿä¸€åˆ†æè¾“å…¥"""
    content: str                                # å½“å‰å†…å®¹
    entities: List[str] = field(default_factory=list)  # å·²è¯†åˆ«å®ä½“
    existing_memories: List[str] = field(default_factory=list)  # å·²æœ‰è®°å¿†ï¼ˆç”¨äºçŸ›ç›¾æ£€æµ‹ï¼‰
    tasks: List[AnalysisTask] = field(default_factory=list)  # è¦æ‰§è¡Œçš„ä»»åŠ¡
    
    # å¯é€‰ä¸Šä¸‹æ–‡
    user_id: str = "default"
    character_id: str = "default"


@dataclass
class UnifiedAnalysisResult:
    """ç»Ÿä¸€åˆ†æç»“æœ"""
    success: bool = False
    
    # çŸ›ç›¾æ£€æµ‹ç»“æœ
    contradictions: List[Dict[str, Any]] = field(default_factory=list)
    
    # å…³ç³»æå–ç»“æœ
    relations: List[Dict[str, Any]] = field(default_factory=list)
    
    # å®ä½“æ‘˜è¦ç»“æœ
    entity_summaries: Dict[str, str] = field(default_factory=dict)
    
    # é”™è¯¯ä¿¡æ¯
    error: Optional[str] = None
    
    # æ‰§è¡Œçš„ä»»åŠ¡
    tasks_executed: List[str] = field(default_factory=list)
    
    # åŸå§‹ LLM å“åº”ï¼ˆè°ƒè¯•ç”¨ï¼‰
    raw_response: Optional[str] = None


# ç»Ÿä¸€åˆ†æ Prompt æ¨¡æ¿
UNIFIED_ANALYSIS_PROMPT = '''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±å’Œè®°å¿†åˆ†æä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹å¯¹è¯å†…å®¹è¿›è¡Œç»¼åˆåˆ†æã€‚

## å½“å‰å¯¹è¯å†…å®¹
{content}

## å·²è¯†åˆ«çš„å®ä½“åˆ—è¡¨
{entities}

## å·²æœ‰çš„è®°å¿†å†…å®¹ï¼ˆç”¨äºçŸ›ç›¾æ£€æµ‹ï¼‰
{existing_memories}

## åˆ†æä»»åŠ¡
è¯·å®Œæˆä»¥ä¸‹åˆ†æä»»åŠ¡ï¼š{tasks_description}

## è¾“å‡ºæ ¼å¼
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š

```json
{{
  "contradictions": [
    {{
      "type": "ç›´æ¥çŸ›ç›¾|æ—¶æ€çŸ›ç›¾|é€»è¾‘çŸ›ç›¾",
      "old_fact": "å·²æœ‰è®°å¿†ä¸­çš„å†…å®¹",
      "new_fact": "æ–°å†…å®¹ä¸­çš„çŸ›ç›¾ç‚¹",
      "confidence": 0.8,
      "explanation": "çŸ›ç›¾è¯´æ˜"
    }}
  ],
  "relations": [
    {{
      "source": "å®ä½“A",
      "target": "å®ä½“B",
      "relation_type": "RELATION_TYPE",
      "fact": "è‡ªç„¶è¯­è¨€æè¿°",
      "confidence": 0.8
    }}
  ],
  "entity_summaries": {{
    "å®ä½“å": "è¯¥å®ä½“çš„ç®€æ´æ‘˜è¦ï¼ˆ2-3å¥è¯ï¼‰"
  }}
}}
```

## é‡è¦è¯´æ˜
1. åªåˆ†æè¦æ±‚çš„ä»»åŠ¡ï¼Œæœªè¦æ±‚çš„ä»»åŠ¡è¾“å‡ºç©ºæ•°ç»„/å¯¹è±¡
2. å…³ç³»ç±»å‹ä½¿ç”¨ SCREAMING_SNAKE_CASE æ ¼å¼ï¼ˆå¦‚ WORKS_AT, FRIENDS_WITHï¼‰
3. åªæå–å®ä½“åˆ—è¡¨ä¸­å­˜åœ¨çš„å®ä½“ä¹‹é—´çš„å…³ç³»
4. çŸ›ç›¾æ£€æµ‹åªåœ¨å‘ç°æ˜ç¡®çŸ›ç›¾æ—¶æ‰æ·»åŠ è®°å½•
5. è¯·ç›´æ¥è¾“å‡º JSONï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—è¯´æ˜'''


# Windows GBK ç¼–ç å…¼å®¹çš„å®‰å…¨æ‰“å°å‡½æ•°
def _safe_print(msg: str) -> None:
    """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œæ›¿æ¢ emoji ä¸º ASCII ç­‰ä»·ç‰©ä»¥é¿å… Windows GBK ç¼–ç é”™è¯¯"""
    emoji_map = {
        'ğŸ“¥': '[IN]', 'ğŸ“¤': '[OUT]', 'ğŸ”': '[SEARCH]', 'âœ…': '[OK]', 'âŒ': '[FAIL]',
        'âš ï¸': '[WARN]', 'ğŸ’¾': '[SAVE]', 'ğŸ—ƒï¸': '[DB]', 'ğŸ§¹': '[CLEAN]', 'ğŸ“Š': '[STATS]',
        'ğŸ”„': '[SYNC]', 'ğŸ“¦': '[PKG]', 'ğŸš€': '[START]', 'ğŸ¯': '[TARGET]', 'ğŸ’¡': '[HINT]',
        'ğŸ”§': '[FIX]', 'ğŸ“': '[NOTE]', 'ğŸ‰': '[DONE]', 'â±ï¸': '[TIME]', 'ğŸŒ': '[NET]',
        'ğŸ§ ': '[BRAIN]', 'ğŸ’¬': '[CHAT]', 'ğŸ·ï¸': '[TAG]', 'ğŸ“': '[DIR]', 'ğŸ”’': '[LOCK]',
        'ğŸŒ±': '[PLANT]', 'ğŸ—‘ï¸': '[DEL]', 'ğŸ’«': '[MAGIC]', 'ğŸ­': '[MASK]', 'ğŸ“–': '[BOOK]',
        'âš¡': '[FAST]', 'ğŸ”¥': '[HOT]', 'ğŸ’': '[GEM]', 'ğŸŒŸ': '[STAR]', 'ğŸ¨': '[ART]'
    }
    for emoji, ascii_equiv in emoji_map.items():
        msg = msg.replace(emoji, ascii_equiv)
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode('ascii', errors='replace').decode('ascii'))


class UnifiedAnalyzer:
    """ç»Ÿä¸€åˆ†æå™¨
    
    é€šè¿‡ä¸€æ¬¡ LLM è°ƒç”¨å®Œæˆå¤šä¸ªåˆ†æä»»åŠ¡ï¼š
    - çŸ›ç›¾æ£€æµ‹
    - å…³ç³»æå–
    - å®ä½“æ‘˜è¦
    
    ä½¿ç”¨æ–¹å¼ï¼š
        analyzer = UnifiedAnalyzer(llm_client=llm_client)
        
        result = analyzer.analyze(UnifiedAnalysisInput(
            content="ç”¨æˆ·è¯´çš„è¯...",
            entities=["å¼ ä¸‰", "æå››"],
            existing_memories=["ä¹‹å‰çš„è®°å¿†1", "ä¹‹å‰çš„è®°å¿†2"],
            tasks=[AnalysisTask.CONTRADICTION, AnalysisTask.RELATION]
        ))
        
        if result.success:
            print(f"çŸ›ç›¾: {result.contradictions}")
            print(f"å…³ç³»: {result.relations}")
    """
    
    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        enabled: bool = True
    ):
        """åˆå§‹åŒ–ç»Ÿä¸€åˆ†æå™¨
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            enabled: æ˜¯å¦å¯ç”¨ï¼ˆFalse æ—¶æ‰€æœ‰è°ƒç”¨ç›´æ¥è¿”å›ç©ºç»“æœï¼‰
        """
        self.llm_client = llm_client
        self.enabled = enabled
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        self.max_tokens = int(os.environ.get('UNIFIED_ANALYSIS_MAX_TOKENS', '4000'))
    
    def analyze(self, input: UnifiedAnalysisInput) -> UnifiedAnalysisResult:
        """æ‰§è¡Œç»Ÿä¸€åˆ†æ
        
        Args:
            input: åˆ†æè¾“å…¥
            
        Returns:
            UnifiedAnalysisResult: åˆ†æç»“æœ
        """
        if not self.enabled or not self.llm_client:
            return UnifiedAnalysisResult(
                success=False,
                error="ç»Ÿä¸€åˆ†æå™¨æœªå¯ç”¨æˆ– LLM å®¢æˆ·ç«¯æœªé…ç½®"
            )
        
        if not input.tasks:
            return UnifiedAnalysisResult(
                success=False,
                error="æœªæŒ‡å®šåˆ†æä»»åŠ¡"
            )
        
        try:
            # æ„å»º Prompt
            prompt = self._build_prompt(input)
            
            # è°ƒç”¨ LLM
            response = self.llm_client.complete(prompt, max_tokens=self.max_tokens)
            
            # è§£æç»“æœ
            result = self._parse_response(response, input.tasks)
            result.raw_response = response
            
            return result
            
        except Exception as e:
            _safe_print(f"[Recall][UnifiedAnalyzer] åˆ†æå¤±è´¥: {e}")
            return UnifiedAnalysisResult(
                success=False,
                error=f"ç»Ÿä¸€åˆ†æå¤±è´¥: {str(e)}"
            )
    
    def _build_prompt(self, input: UnifiedAnalysisInput) -> str:
        """æ„å»ºåˆ†æ Prompt"""
        # ä»»åŠ¡æè¿°
        task_descriptions = []
        if AnalysisTask.CONTRADICTION in input.tasks:
            task_descriptions.append("1. çŸ›ç›¾æ£€æµ‹ï¼šæ£€æŸ¥æ–°å†…å®¹æ˜¯å¦ä¸å·²æœ‰è®°å¿†å­˜åœ¨çŸ›ç›¾")
        if AnalysisTask.RELATION in input.tasks:
            task_descriptions.append("2. å…³ç³»æå–ï¼šæå–å®ä½“ä¹‹é—´çš„å…³ç³»")
        if AnalysisTask.ENTITY_SUMMARY in input.tasks:
            task_descriptions.append("3. å®ä½“æ‘˜è¦ï¼šä¸ºä¸»è¦å®ä½“ç”Ÿæˆç®€æ´æ‘˜è¦")
        
        tasks_description = "\n".join(task_descriptions) if task_descriptions else "æ— "
        
        # æ ¼å¼åŒ–å®ä½“åˆ—è¡¨
        entities_str = ", ".join(input.entities) if input.entities else "ï¼ˆæ— ï¼‰"
        
        # æ ¼å¼åŒ–å·²æœ‰è®°å¿†
        if input.existing_memories:
            memories_str = "\n".join([f"- {m[:200]}..." if len(m) > 200 else f"- {m}" 
                                      for m in input.existing_memories[:10]])
        else:
            memories_str = "ï¼ˆæ— ï¼‰"
        
        return UNIFIED_ANALYSIS_PROMPT.format(
            content=input.content,
            entities=entities_str,
            existing_memories=memories_str,
            tasks_description=tasks_description
        )
    
    def _parse_response(
        self,
        response: str,
        tasks: List[AnalysisTask]
    ) -> UnifiedAnalysisResult:
        """è§£æ LLM å“åº”"""
        result = UnifiedAnalysisResult(success=True)
        
        try:
            # æå– JSON éƒ¨åˆ†
            json_str = self._extract_json(response)
            data = json.loads(json_str)
            
            # è§£æçŸ›ç›¾æ£€æµ‹ç»“æœ
            if AnalysisTask.CONTRADICTION in tasks:
                result.contradictions = data.get('contradictions', [])
                result.tasks_executed.append('contradiction')
            
            # è§£æå…³ç³»æå–ç»“æœ
            if AnalysisTask.RELATION in tasks:
                result.relations = data.get('relations', [])
                result.tasks_executed.append('relation')
            
            # è§£æå®ä½“æ‘˜è¦ç»“æœ
            if AnalysisTask.ENTITY_SUMMARY in tasks:
                result.entity_summaries = data.get('entity_summaries', {})
                result.tasks_executed.append('entity_summary')
            
        except json.JSONDecodeError as e:
            result.success = False
            result.error = f"JSON è§£æå¤±è´¥: {str(e)}"
        except Exception as e:
            result.success = False
            result.error = f"å“åº”è§£æå¤±è´¥: {str(e)}"
        
        return result
    
    def _extract_json(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå– JSON"""
        # å°è¯•ç›´æ¥è§£æ
        text = text.strip()
        if text.startswith('{'):
            return text
        
        # å°è¯•æå– ```json ... ``` å—
        json_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', text)
        if json_match:
            return json_match.group(1).strip()
        
        # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
        start = text.find('{')
        end = text.rfind('}')
        if start != -1 and end != -1 and end > start:
            return text[start:end+1]
        
        raise ValueError("æ— æ³•ä»å“åº”ä¸­æå– JSON")

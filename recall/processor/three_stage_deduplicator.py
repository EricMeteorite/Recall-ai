"""ä¸‰é˜¶æ®µå»é‡ç³»ç»Ÿ - Recall 4.0 Phase 2

è®¾è®¡ç†å¿µï¼š
1. é˜¶æ®µ 1: ç¡®å®šæ€§åŒ¹é… - ç²¾ç¡®åŒ¹é… + MinHash + LSHï¼ˆO(1) å¿«é€Ÿï¼‰
2. é˜¶æ®µ 2: è¯­ä¹‰åŒ¹é… - Embedding ç›¸ä¼¼åº¦ï¼ˆæˆæœ¬è¾ƒä½ï¼‰
3. é˜¶æ®µ 3: LLM ç¡®è®¤ - ä»…ç”¨äºè¾¹ç•Œæƒ…å†µï¼ˆæˆæœ¬è¾ƒé«˜ï¼‰

è¶…è¶Š Graphiti çš„ä¸¤é˜¶æ®µå»é‡ï¼Œå¢åŠ è¯­ä¹‰åŒ¹é…å±‚ï¼Œæ˜¾è‘—é™ä½ LLM æˆæœ¬ã€‚
"""

from __future__ import annotations

import os
import json
import hashlib
import re
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Set, Optional, Tuple, Any, Callable
from enum import Enum
from collections import defaultdict


# Windows GBK ç¼–ç å…¼å®¹çš„å®‰å…¨æ‰“å°å‡½æ•°
def _safe_print(msg: str) -> None:
    """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œæ›¿æ¢ emoji ä¸º ASCII ç­‰ä»·ç‰©ä»¥é¿å… Windows GBK ç¼–ç é”™è¯¯"""
    emoji_map = {
        'ğŸ“¥': '[IN]', 'ğŸ“¤': '[OUT]', 'ğŸ”': '[SEARCH]', 'âœ…': '[OK]', 'âŒ': '[FAIL]',
        'âš ï¸': '[WARN]', 'ğŸ’¾': '[SAVE]', 'ğŸ—ƒï¸': '[DB]', 'ğŸ§¹': '[CLEAN]', 'ğŸ“Š': '[STATS]',
        'ğŸ”„': '[SYNC]', 'ğŸ“¦': '[PKG]', 'ğŸš€': '[START]', 'ğŸ¯': '[TARGET]', 'ğŸ’¡': '[HINT]',
        'ğŸ”§': '[FIX]', 'ğŸ“': '[NOTE]', 'ğŸ‰': '[DONE]', 'â±ï¸': '[TIME]', 'ğŸŒ': '[NET]',
        'ğŸ§ ': '[BRAIN]', 'ğŸ’¬': '[CHAT]', 'ğŸ·ï¸': '[TAG]', 'ğŸ“': '[DIR]', 'ğŸ”’': '[LOCK]',
        'ğŸŒ±': '[PLANT]', 'ğŸ—‘ï¸': '[DEL]', 'ğŸ’«': '[MAGIC]', 'ğŸ­': '[MASK]', 'ğŸ“–': '[BOOK]',
        'âš¡': '[FAST]', 'ğŸ”¥': '[HOT]', 'ğŸ’': '[GEM]', 'ğŸŒŸ': '[STAR]', 'ğŸ¨': '[ART]'
    }
    for emoji, ascii_equiv in emoji_map.items():
        msg = msg.replace(emoji, ascii_equiv)
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode('ascii', errors='replace').decode('ascii'))


# å¯é€‰å¯¼å…¥
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False


class MatchType(str, Enum):
    """åŒ¹é…ç±»å‹"""
    EXACT = "exact"         # ç²¾ç¡®åŒ¹é…
    FUZZY = "fuzzy"         # MinHash/LSH æ¨¡ç³ŠåŒ¹é…
    SEMANTIC = "semantic"   # è¯­ä¹‰ç›¸ä¼¼
    LLM = "llm"             # LLM ç¡®è®¤
    NEW = "new"             # æ–°å®ä½“


@dataclass
class DedupItem:
    """å¾…å»é‡çš„é¡¹ç›®"""
    id: str
    name: str
    content: str = ""
    item_type: str = "entity"
    attributes: Dict[str, Any] = field(default_factory=dict)
    embedding: Optional[List[float]] = None
    
    def get_text(self) -> str:
        """è·å–ç”¨äºå»é‡çš„æ–‡æœ¬"""
        return self.content or self.name


@dataclass 
class DedupMatch:
    """å»é‡åŒ¹é…ç»“æœ"""
    new_item: DedupItem
    matched_item: Optional[DedupItem]
    match_type: MatchType
    confidence: float
    reason: str = ""


@dataclass
class DedupResult:
    """å»é‡ç»“æœ"""
    matches: List[DedupMatch] = field(default_factory=list)
    new_items: List[DedupItem] = field(default_factory=list)
    pending_items: List[Tuple[DedupItem, List[DedupItem]]] = field(default_factory=list)
    
    # ç»Ÿè®¡
    total_count: int = 0
    matched_count: int = 0
    new_count: int = 0
    pending_count: int = 0
    
    def add_match(
        self,
        new_item: DedupItem,
        matched_item: DedupItem,
        match_type: MatchType,
        confidence: float,
        reason: str = ""
    ):
        """æ·»åŠ åŒ¹é…"""
        self.matches.append(DedupMatch(
            new_item=new_item,
            matched_item=matched_item,
            match_type=match_type,
            confidence=confidence,
            reason=reason
        ))
        self.matched_count += 1
    
    def add_new(self, item: DedupItem):
        """æ·»åŠ æ–°é¡¹ç›®"""
        self.new_items.append(item)
        self.new_count += 1
    
    def add_pending(self, item: DedupItem, candidates: List[DedupItem]):
        """æ·»åŠ å¾…ç¡®è®¤é¡¹ç›®"""
        self.pending_items.append((item, candidates))
        self.pending_count += 1
    
    def move_to_match(
        self,
        item: DedupItem,
        matched_item: DedupItem,
        match_type: MatchType
    ):
        """å°†å¾…ç¡®è®¤é¡¹ç§»åŠ¨åˆ°åŒ¹é…åˆ—è¡¨"""
        # ä» pending ä¸­ç§»é™¤
        self.pending_items = [
            (i, c) for i, c in self.pending_items
            if i.id != item.id
        ]
        self.pending_count = len(self.pending_items)
        
        # æ·»åŠ åˆ°åŒ¹é…
        self.add_match(item, matched_item, match_type, 0.9, "LLM confirmed")
    
    def move_to_new(self, item: DedupItem):
        """å°†å¾…ç¡®è®¤é¡¹ç§»åŠ¨åˆ°æ–°é¡¹ç›®åˆ—è¡¨"""
        self.pending_items = [
            (i, c) for i, c in self.pending_items
            if i.id != item.id
        ]
        self.pending_count = len(self.pending_items)
        self.add_new(item)


@dataclass
class DedupConfig:
    """å»é‡é…ç½®"""
    # é˜¶æ®µ 1: ç¡®å®šæ€§åŒ¹é…
    exact_match_enabled: bool = True
    fuzzy_match_enabled: bool = True
    jaccard_threshold: float = 0.7          # Jaccard ç›¸ä¼¼åº¦é˜ˆå€¼
    minhash_num_perm: int = 128             # MinHash æ’åˆ—æ•°
    lsh_threshold: float = 0.5              # LSH é˜ˆå€¼
    
    # é˜¶æ®µ 2: è¯­ä¹‰åŒ¹é…
    semantic_enabled: bool = True
    semantic_threshold: float = 0.85        # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé«˜äºæ­¤è§†ä¸ºç›¸åŒï¼‰
    semantic_low_threshold: float = 0.70    # è¯­ä¹‰ä½é˜ˆå€¼ï¼ˆä½äºæ­¤è§†ä¸ºä¸åŒï¼‰
    
    # é˜¶æ®µ 3: LLM ç¡®è®¤
    llm_enabled: bool = False               # æ˜¯å¦å¯ç”¨ LLM ç¡®è®¤
    llm_threshold: float = 0.75             # è¿›å…¥ LLM ç¡®è®¤çš„é˜ˆå€¼èŒƒå›´ï¼ˆ0.70-0.85 ä¹‹é—´ï¼‰
    llm_batch_size: int = 5                 # LLM æ‰¹é‡ç¡®è®¤å¤§å°
    
    @classmethod
    def default(cls) -> 'DedupConfig':
        """é»˜è®¤é…ç½®"""
        return cls()
    
    @classmethod
    def strict(cls) -> 'DedupConfig':
        """ä¸¥æ ¼æ¨¡å¼ï¼ˆæ›´é«˜é˜ˆå€¼ï¼‰"""
        return cls(
            jaccard_threshold=0.8,
            semantic_threshold=0.90,
            semantic_low_threshold=0.75
        )
    
    @classmethod
    def lenient(cls) -> 'DedupConfig':
        """å®½æ¾æ¨¡å¼ï¼ˆæ›´ä½é˜ˆå€¼ï¼‰"""
        return cls(
            jaccard_threshold=0.6,
            semantic_threshold=0.80,
            semantic_low_threshold=0.65
        )


class MinHasher:
    """MinHash å®ç°ï¼ˆç”¨äºå¿«é€Ÿç›¸ä¼¼åº¦ä¼°ç®—ï¼‰"""
    
    def __init__(self, num_perm: int = 128, seed: int = 42):
        self.num_perm = num_perm
        self.seed = seed
        
        # ç”Ÿæˆå“ˆå¸Œå‚æ•°
        if HAS_NUMPY:
            np.random.seed(seed)
            max_hash = 2**32 - 1
            self.a = np.random.randint(1, max_hash, num_perm, dtype=np.uint64)
            self.b = np.random.randint(0, max_hash, num_perm, dtype=np.uint64)
        else:
            import random
            random.seed(seed)
            max_hash = 2**32 - 1
            self.a = [random.randint(1, max_hash) for _ in range(num_perm)]
            self.b = [random.randint(0, max_hash) for _ in range(num_perm)]
        
        self.max_hash = 2**32 - 1
    
    def get_shingles(self, text: str, k: int = 3) -> Set[str]:
        """è·å– k-shingles"""
        text = text.lower().strip()
        if len(text) < k:
            return {text}
        return {text[i:i+k] for i in range(len(text) - k + 1)}
    
    def minhash(self, shingles: Set[str]) -> List[int]:
        """è®¡ç®— MinHash ç­¾å"""
        signature = [self.max_hash] * self.num_perm
        
        for shingle in shingles:
            h = int(hashlib.md5(shingle.encode()).hexdigest(), 16) % self.max_hash
            
            for i in range(self.num_perm):
                if HAS_NUMPY:
                    hash_val = int((self.a[i] * h + self.b[i]) % self.max_hash)
                else:
                    hash_val = (self.a[i] * h + self.b[i]) % self.max_hash
                signature[i] = min(signature[i], hash_val)
        
        return signature
    
    def jaccard_from_signatures(
        self,
        sig1: List[int],
        sig2: List[int]
    ) -> float:
        """ä»ç­¾åä¼°ç®— Jaccard ç›¸ä¼¼åº¦"""
        if len(sig1) != len(sig2):
            raise ValueError("ç­¾åé•¿åº¦ä¸ä¸€è‡´")
        
        matches = sum(1 for a, b in zip(sig1, sig2) if a == b)
        return matches / len(sig1)


class LSHIndex:
    """LSH ç´¢å¼•ï¼ˆç”¨äºå¿«é€Ÿå€™é€‰æ£€ç´¢ï¼‰"""
    
    def __init__(self, num_bands: int = 16, rows_per_band: int = 8):
        self.num_bands = num_bands
        self.rows_per_band = rows_per_band
        self.buckets: Dict[int, Dict[int, Set[str]]] = defaultdict(lambda: defaultdict(set))
    
    def add(self, item_id: str, signature: List[int]):
        """æ·»åŠ é¡¹ç›®åˆ°ç´¢å¼•"""
        for band_idx in range(self.num_bands):
            start = band_idx * self.rows_per_band
            end = start + self.rows_per_band
            band = tuple(signature[start:end])
            bucket_hash = hash(band)
            self.buckets[band_idx][bucket_hash].add(item_id)
    
    def query(self, signature: List[int]) -> Set[str]:
        """æŸ¥è¯¢å€™é€‰é¡¹"""
        candidates = set()
        
        for band_idx in range(self.num_bands):
            start = band_idx * self.rows_per_band
            end = start + self.rows_per_band
            band = tuple(signature[start:end])
            bucket_hash = hash(band)
            
            if bucket_hash in self.buckets[band_idx]:
                candidates.update(self.buckets[band_idx][bucket_hash])
        
        return candidates


class ThreeStageDeduplicator:
    """ä¸‰é˜¶æ®µå»é‡ç³»ç»Ÿ
    
    é˜¶æ®µ 1: ç¡®å®šæ€§åŒ¹é…
        - ç²¾ç¡®åŒ¹é…ï¼ˆå½’ä¸€åŒ–åï¼‰
        - MinHash + LSH è¿‘ä¼¼åŒ¹é…
        
    é˜¶æ®µ 2: è¯­ä¹‰åŒ¹é…
        - Embedding å‘é‡ç›¸ä¼¼åº¦
        
    é˜¶æ®µ 3: LLM ç¡®è®¤
        - ä»…ç”¨äºè¾¹ç•Œæƒ…å†µï¼ˆsemantic_low < sim < semantic_highï¼‰
    
    ä½¿ç”¨æ–¹å¼ï¼š
        dedup = ThreeStageDeduplicator(embedding_backend=backend)
        result = dedup.deduplicate(new_items, existing_items)
    """
    
    # LLM ç¡®è®¤ Prompt
    DEDUP_PROMPT = '''è¯·åˆ¤æ–­ä»¥ä¸‹ä¸¤ä¸ªå®ä½“/æ¦‚å¿µæ˜¯å¦æŒ‡ä»£åŒä¸€äº‹ç‰©ã€‚

å®ä½“ Aï¼š{item_a}
å®ä½“ Bï¼š{item_b}

è¯·å›ç­”ï¼š
- å¦‚æœæ˜¯åŒä¸€äº‹ç‰©ï¼Œå›ç­” "YES"
- å¦‚æœä¸æ˜¯åŒä¸€äº‹ç‰©ï¼Œå›ç­” "NO"
- å¦‚æœä¸ç¡®å®šï¼Œå›ç­” "UNCERTAIN"

åªå›ç­” YESã€NO æˆ– UNCERTAINï¼Œä¸è¦è§£é‡Šã€‚'''

    def __init__(
        self,
        config: Optional[DedupConfig] = None,
        embedding_backend: Any = None,      # EmbeddingBackend å®ä¾‹
        llm_client: Any = None,             # LLMClient å®ä¾‹
        budget_manager: Any = None          # BudgetManager å®ä¾‹
    ):
        """åˆå§‹åŒ–å»é‡å™¨
        
        Args:
            config: å»é‡é…ç½®
            embedding_backend: Embedding åç«¯ï¼ˆç”¨äºè¯­ä¹‰åŒ¹é…ï¼‰
            llm_client: LLM å®¢æˆ·ç«¯ï¼ˆç”¨äºè¾¹ç•Œæƒ…å†µç¡®è®¤ï¼‰
            budget_manager: é¢„ç®—ç®¡ç†å™¨
        """
        self.config = config or DedupConfig.default()
        self.embedding_backend = embedding_backend
        self.llm_client = llm_client
        self.budget_manager = budget_manager
        
        # MinHash å’Œ LSH
        self.minhasher = MinHasher(num_perm=self.config.minhash_num_perm)
        self.lsh_index = LSHIndex()
        
        # ç´¢å¼•å­˜å‚¨
        self._exact_map: Dict[str, DedupItem] = {}          # å½’ä¸€åŒ–åç§° -> é¡¹ç›®
        self._signature_map: Dict[str, List[int]] = {}      # ID -> MinHash ç­¾å
        self._item_map: Dict[str, DedupItem] = {}           # ID -> é¡¹ç›®
    
    def _normalize(self, text: str) -> str:
        """æ–‡æœ¬å½’ä¸€åŒ–"""
        if not text:
            return ""
        
        # è½¬å°å†™
        text = text.lower()
        
        # ç§»é™¤æ ‡ç‚¹å’Œå¤šä½™ç©ºç™½
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def build_index(self, items: List[DedupItem]):
        """æ„å»ºç´¢å¼•
        
        Args:
            items: ç°æœ‰é¡¹ç›®åˆ—è¡¨
        """
        for item in items:
            # ç²¾ç¡®åŒ¹é…ç´¢å¼•
            normalized = self._normalize(item.name)
            if normalized:
                self._exact_map[normalized] = item
            
            # MinHash + LSH ç´¢å¼•
            if self.config.fuzzy_match_enabled:
                shingles = self.minhasher.get_shingles(item.get_text())
                signature = self.minhasher.minhash(shingles)
                self._signature_map[item.id] = signature
                self.lsh_index.add(item.id, signature)
            
            # é¡¹ç›®æ˜ å°„
            self._item_map[item.id] = item
    
    def deduplicate(
        self,
        new_items: List[DedupItem],
        existing_items: Optional[List[DedupItem]] = None
    ) -> DedupResult:
        """æ‰§è¡Œå»é‡
        
        Args:
            new_items: æ–°é¡¹ç›®åˆ—è¡¨
            existing_items: ç°æœ‰é¡¹ç›®åˆ—è¡¨ï¼ˆå¦‚æœæœªé¢„å…ˆæ„å»ºç´¢å¼•ï¼‰
            
        Returns:
            DedupResult: å»é‡ç»“æœ
        """
        # å¦‚æœæä¾›äº†ç°æœ‰é¡¹ç›®ï¼Œæ„å»ºç´¢å¼•
        if existing_items:
            self.build_index(existing_items)
        
        result = DedupResult()
        result.total_count = len(new_items)
        
        for item in new_items:
            match = self._deduplicate_single(item)
            
            if match.match_type == MatchType.NEW:
                result.add_new(item)
            elif match.match_type in (MatchType.EXACT, MatchType.FUZZY, MatchType.SEMANTIC, MatchType.LLM):
                result.matches.append(match)
                result.matched_count += 1
            else:
                # å¾…ç¡®è®¤
                result.add_pending(item, [match.matched_item] if match.matched_item else [])
        
        # é˜¶æ®µ 3: LLM æ‰¹é‡ç¡®è®¤å¾…ç¡®è®¤é¡¹
        if result.pending_items and self.config.llm_enabled and self.llm_client:
            self._llm_batch_confirm(result)
        
        return result
    
    def _deduplicate_single(self, item: DedupItem) -> DedupMatch:
        """å¯¹å•ä¸ªé¡¹ç›®è¿›è¡Œå»é‡"""
        # === é˜¶æ®µ 1: ç¡®å®šæ€§åŒ¹é… ===
        
        # 1.1 ç²¾ç¡®åŒ¹é…
        if self.config.exact_match_enabled:
            normalized = self._normalize(item.name)
            if normalized in self._exact_map:
                matched = self._exact_map[normalized]
                return DedupMatch(
                    new_item=item,
                    matched_item=matched,
                    match_type=MatchType.EXACT,
                    confidence=1.0,
                    reason="ç²¾ç¡®åŒ¹é…ï¼ˆå½’ä¸€åŒ–åï¼‰"
                )
        
        # 1.2 MinHash + LSH æ¨¡ç³ŠåŒ¹é…
        if self.config.fuzzy_match_enabled:
            shingles = self.minhasher.get_shingles(item.get_text())
            signature = self.minhasher.minhash(shingles)
            
            candidates = self.lsh_index.query(signature)
            
            if candidates:
                best_match = None
                best_jaccard = 0.0
                
                for candidate_id in candidates:
                    if candidate_id in self._signature_map:
                        candidate_sig = self._signature_map[candidate_id]
                        jaccard = self.minhasher.jaccard_from_signatures(signature, candidate_sig)
                        
                        if jaccard > best_jaccard:
                            best_jaccard = jaccard
                            best_match = self._item_map.get(candidate_id)
                
                if best_match and best_jaccard >= self.config.jaccard_threshold:
                    return DedupMatch(
                        new_item=item,
                        matched_item=best_match,
                        match_type=MatchType.FUZZY,
                        confidence=best_jaccard,
                        reason=f"MinHash+LSH åŒ¹é… (Jaccard={best_jaccard:.3f})"
                    )
        
        # === é˜¶æ®µ 2: è¯­ä¹‰åŒ¹é… ===
        if self.config.semantic_enabled and self.embedding_backend:
            semantic_match = self._semantic_match(item)
            if semantic_match:
                return semantic_match
        
        # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…
        return DedupMatch(
            new_item=item,
            matched_item=None,
            match_type=MatchType.NEW,
            confidence=0.0,
            reason="æœªæ‰¾åˆ°åŒ¹é…"
        )
    
    def _semantic_match(self, item: DedupItem) -> Optional[DedupMatch]:
        """è¯­ä¹‰åŒ¹é…"""
        if not self.embedding_backend or not self._item_map:
            return None
        
        try:
            # è·å–æ–°é¡¹ç›®çš„ embeddingï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
            if item.embedding:
                item_embedding = item.embedding
            else:
                # ä½¿ç”¨å¸¦ç¼“å­˜çš„ç¼–ç ï¼Œå‡å°‘ API è°ƒç”¨
                if hasattr(self.embedding_backend, 'encode_with_cache'):
                    item_embedding = self.embedding_backend.encode_with_cache(item.get_text())
                else:
                    item_embedding = self.embedding_backend.encode(item.get_text())
                if hasattr(item_embedding, 'tolist'):
                    item_embedding = item_embedding.tolist()
            
            # ä¸æ‰€æœ‰ç°æœ‰é¡¹ç›®è®¡ç®—ç›¸ä¼¼åº¦
            best_match = None
            best_similarity = 0.0
            
            for existing_id, existing_item in self._item_map.items():
                # è·å–ç°æœ‰é¡¹ç›®çš„ embeddingï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰
                if existing_item.embedding:
                    existing_embedding = existing_item.embedding
                else:
                    # ä½¿ç”¨å¸¦ç¼“å­˜çš„ç¼–ç ï¼Œå‡å°‘ API è°ƒç”¨
                    if hasattr(self.embedding_backend, 'encode_with_cache'):
                        existing_embedding = self.embedding_backend.encode_with_cache(existing_item.get_text())
                    else:
                        existing_embedding = self.embedding_backend.encode(existing_item.get_text())
                    if hasattr(existing_embedding, 'tolist'):
                        existing_embedding = existing_embedding.tolist()
                    existing_item.embedding = existing_embedding
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = self._cosine_similarity(item_embedding, existing_embedding)
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = existing_item
            
            # æ ¹æ®é˜ˆå€¼åˆ¤æ–­
            if best_similarity >= self.config.semantic_threshold:
                # é«˜ç›¸ä¼¼åº¦ï¼Œç›´æ¥åŒ¹é…
                return DedupMatch(
                    new_item=item,
                    matched_item=best_match,
                    match_type=MatchType.SEMANTIC,
                    confidence=best_similarity,
                    reason=f"è¯­ä¹‰åŒ¹é… (similarity={best_similarity:.3f})"
                )
            elif best_similarity >= self.config.semantic_low_threshold:
                # è¾¹ç•Œæƒ…å†µï¼Œéœ€è¦ LLM ç¡®è®¤
                if self.config.llm_enabled:
                    return DedupMatch(
                        new_item=item,
                        matched_item=best_match,
                        match_type=MatchType.NEW,  # æš‚æ—¶æ ‡è®°ä¸ºæ–°ï¼Œç­‰å¾… LLM ç¡®è®¤
                        confidence=best_similarity,
                        reason=f"è¾¹ç•Œæƒ…å†µï¼Œéœ€ LLM ç¡®è®¤ (similarity={best_similarity:.3f})"
                    )
            
            # ä½ç›¸ä¼¼åº¦ï¼Œè§†ä¸ºæ–°é¡¹ç›®
            return None
        
        except Exception as e:
            _safe_print(f"[ThreeStageDeduplicator] è¯­ä¹‰åŒ¹é…å¤±è´¥: {e}")
            return None
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if HAS_NUMPY:
            a = np.array(vec1)
            b = np.array(vec2)
            return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8))
        else:
            dot = sum(a * b for a, b in zip(vec1, vec2))
            norm1 = sum(a * a for a in vec1) ** 0.5
            norm2 = sum(b * b for b in vec2) ** 0.5
            return dot / (norm1 * norm2 + 1e-8)
    
    def _llm_batch_confirm(self, result: DedupResult):
        """LLM æ‰¹é‡ç¡®è®¤è¾¹ç•Œæƒ…å†µ"""
        if not self.llm_client or not result.pending_items:
            return
        
        # æ£€æŸ¥é¢„ç®—
        if self.budget_manager:
            estimated_cost = len(result.pending_items) * 0.001  # ç²—ç•¥ä¼°ç®—
            if not self.budget_manager.can_afford(estimated_cost, operation="dedup"):
                _safe_print("[ThreeStageDeduplicator] é¢„ç®—ä¸è¶³ï¼Œè·³è¿‡ LLM ç¡®è®¤")
                # å°†æ‰€æœ‰å¾…ç¡®è®¤é¡¹æ ‡è®°ä¸ºæ–°é¡¹ç›®
                for item, _ in result.pending_items[:]:
                    result.move_to_new(item)
                return
        
        confirmed = []
        
        for item, candidates in result.pending_items[:]:
            if not candidates:
                result.move_to_new(item)
                continue
            
            candidate = candidates[0]
            
            try:
                prompt = self.DEDUP_PROMPT.format(
                    item_a=f"{item.name}: {item.content[:100]}" if item.content else item.name,
                    item_b=f"{candidate.name}: {candidate.content[:100]}" if candidate.content else candidate.name
                )
                
                response = self.llm_client.complete(
                    prompt=prompt,
                    max_tokens=10,
                    temperature=0
                ).strip().upper()
                
                if response == "YES":
                    result.move_to_match(item, candidate, MatchType.LLM)
                else:
                    result.move_to_new(item)
                
                # è®°å½•é¢„ç®—
                if self.budget_manager:
                    self.budget_manager.record_usage(
                        operation="dedup_confirm",
                        tokens_in=len(prompt) // 4,
                        tokens_out=5,
                        model=self.llm_client.model
                    )
                    
            except Exception as e:
                _safe_print(f"[ThreeStageDeduplicator] LLM ç¡®è®¤å¤±è´¥: {e}")
                result.move_to_new(item)
    
    def add_to_index(self, item: DedupItem):
        """å°†é¡¹ç›®æ·»åŠ åˆ°ç´¢å¼•ï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰"""
        # ç²¾ç¡®åŒ¹é…ç´¢å¼•
        normalized = self._normalize(item.name)
        if normalized:
            self._exact_map[normalized] = item
        
        # MinHash + LSH ç´¢å¼•
        if self.config.fuzzy_match_enabled:
            shingles = self.minhasher.get_shingles(item.get_text())
            signature = self.minhasher.minhash(shingles)
            self._signature_map[item.id] = signature
            self.lsh_index.add(item.id, signature)
        
        # é¡¹ç›®æ˜ å°„
        self._item_map[item.id] = item
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_indexed': len(self._item_map),
            'exact_index_size': len(self._exact_map),
            'signature_index_size': len(self._signature_map),
            'config': {
                'jaccard_threshold': self.config.jaccard_threshold,
                'semantic_threshold': self.config.semantic_threshold,
                'semantic_low_threshold': self.config.semantic_low_threshold,
                'llm_enabled': self.config.llm_enabled
            }
        }

"""æŒä¹…ä¸Šä¸‹æ–‡è¿½è¸ªå™¨ - è¿½è¸ªå’Œç®¡ç†æŒä¹…æ€§å‰ææ¡ä»¶

è¿™ä¸ªæ¨¡å—è§£å†³çš„é—®é¢˜ï¼š
- æŸäº›ä¿¡æ¯ä¸€æ—¦ç¡®ç«‹ï¼Œå°±åº”è¯¥æˆä¸ºåç»­æ‰€æœ‰å¯¹è¯çš„"é»˜è®¤å‰ææ¡ä»¶"
- æ¯”å¦‚"ç”¨æˆ·æ˜¯å¤§å­¦æ¯•ä¸šç”Ÿæƒ³åˆ›ä¸š" â€”â€” åç»­æ‰€æœ‰å»ºè®®éƒ½åŸºäºè¿™ä¸ªå‰æ
- æ¯”å¦‚"Windowså¼€å‘ï¼ŒUbuntuéƒ¨ç½²" â€”â€” åç»­æ‰€æœ‰ä»£ç å»ºè®®éƒ½åŸºäºè¿™ä¸ªç¯å¢ƒ

æ ¸å¿ƒæ¦‚å¿µï¼š
- PersistentContext: æŒä¹…æ€§ä¸Šä¸‹æ–‡/æ¡ä»¶
- ä¸åŒäºä¼ç¬”ï¼ˆæœªè§£å†³çš„çº¿ç´¢ï¼‰ï¼Œè¿™æ˜¯å·²ç¡®ç«‹çš„èƒŒæ™¯è®¾å®š
- ä¸åŒäºæ™®é€šè®°å¿†ï¼ˆéœ€è¦æ£€ç´¢ï¼‰ï¼Œè¿™æ˜¯æ¯æ¬¡å¯¹è¯éƒ½åº”è¯¥è‡ªåŠ¨åŒ…å«çš„å‰æ

æ™ºèƒ½å»é‡æœºåˆ¶ï¼ˆv3.0.1 æ–°å¢ï¼‰ï¼š
- ç¬¬ä¸€çº§ï¼šEmbedding å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦å¿«é€Ÿç­›é€‰
- ç¬¬äºŒçº§ï¼šLLM æå–æ—¶é¡ºå¸¦åˆ¤æ–­æ˜¯å¦ä¸å·²æœ‰æ¡ä»¶é‡å¤
- ç¬¬ä¸‰çº§ï¼šå®šæœŸæ‰¹é‡æ•´ç†åˆå¹¶
"""

import os
import json
import time
import re
import numpy as np
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Any, Optional, Tuple
from enum import Enum


class ContextType(Enum):
    """ä¸Šä¸‹æ–‡ç±»å‹"""
    # ç”¨æˆ·èº«ä»½ç›¸å…³
    USER_IDENTITY = "user_identity"      # ç”¨æˆ·èº«ä»½ï¼ˆå¦‚ï¼šå¤§å­¦æ¯•ä¸šç”Ÿã€ç¨‹åºå‘˜ï¼‰
    USER_GOAL = "user_goal"              # ç”¨æˆ·ç›®æ ‡ï¼ˆå¦‚ï¼šæƒ³åˆ›ä¸šã€å­¦ä¹ Pythonï¼‰
    USER_PREFERENCE = "user_preference"  # ç”¨æˆ·åå¥½ï¼ˆå¦‚ï¼šå–œæ¬¢ç®€æ´çš„è§£é‡Šï¼‰
    
    # ç¯å¢ƒç›¸å…³
    ENVIRONMENT = "environment"          # æŠ€æœ¯ç¯å¢ƒï¼ˆå¦‚ï¼šWindowså¼€å‘ã€Ubuntuéƒ¨ç½²ï¼‰
    PROJECT = "project"                  # é¡¹ç›®ä¿¡æ¯ï¼ˆå¦‚ï¼šæ­£åœ¨å¼€å‘Recall-aiï¼‰
    TIME_CONSTRAINT = "time_constraint"  # æ—¶é—´çº¦æŸï¼ˆå¦‚ï¼šæˆªæ­¢æ—¥æœŸã€ç‰¹å®šæ—¶é—´æ®µï¼‰
    
    # è§’è‰²æ‰®æ¼”ç›¸å…³
    CHARACTER_TRAIT = "character_trait"  # è§’è‰²ç‰¹å¾ï¼ˆå¦‚ï¼šè§’è‰²æ€§æ ¼è®¾å®šï¼‰
    WORLD_SETTING = "world_setting"      # ä¸–ç•Œè§‚è®¾å®šï¼ˆå¦‚ï¼šé­”æ³•ä¸–ç•Œï¼‰
    RELATIONSHIP = "relationship"        # å…³ç³»è®¾å®šï¼ˆå¦‚ï¼šç”¨æˆ·æ˜¯è§’è‰²çš„æœ‹å‹ï¼‰
    EMOTIONAL_STATE = "emotional_state"  # æƒ…ç»ªçŠ¶æ€ï¼ˆå¦‚ï¼šè§’è‰²å½“å‰å¿ƒæƒ…ï¼‰
    SKILL_ABILITY = "skill_ability"      # æŠ€èƒ½èƒ½åŠ›ï¼ˆå¦‚ï¼šè§’è‰²ä¼šä»€ä¹ˆæŠ€èƒ½ï¼‰
    ITEM_PROP = "item_prop"              # ç‰©å“é“å…·ï¼ˆå¦‚ï¼šè§’è‰²æºå¸¦çš„ç‰©å“ï¼‰
    
    # é€šç”¨
    ASSUMPTION = "assumption"            # å‡è®¾å‰æ
    CONSTRAINT = "constraint"            # çº¦æŸæ¡ä»¶
    CUSTOM = "custom"                    # è‡ªå®šä¹‰


@dataclass
class PersistentContext:
    """æŒä¹…æ€§ä¸Šä¸‹æ–‡/æ¡ä»¶"""
    id: str
    content: str                         # æ¡ä»¶å†…å®¹
    context_type: ContextType            # æ¡ä»¶ç±»å‹
    user_id: str = "default"
    
    # å…ƒä¿¡æ¯
    created_at: float = field(default_factory=time.time)
    source_turn: Optional[str] = None    # æ¥æºè®°å¿†ID
    confidence: float = 0.8              # ç½®ä¿¡åº¦
    
    # ç”Ÿå‘½å‘¨æœŸ
    is_active: bool = True               # æ˜¯å¦æ´»è·ƒ
    expires_at: Optional[float] = None   # è¿‡æœŸæ—¶é—´ï¼ˆNone=æ°¸ä¸è¿‡æœŸï¼‰
    last_used: float = field(default_factory=time.time)
    use_count: int = 0
    
    # å…³è”
    related_entities: List[str] = field(default_factory=list)
    keywords: List[str] = field(default_factory=list)
    
    # æ™ºèƒ½å»é‡ - Embedding å‘é‡ç¼“å­˜ï¼ˆå¯é€‰å­—æ®µï¼Œå‘åå…¼å®¹ï¼‰
    # å­˜å‚¨ä¸º list ä»¥ä¾¿ JSON åºåˆ—åŒ–ï¼Œä½¿ç”¨æ—¶è½¬æ¢ä¸º numpy array
    _embedding: Optional[List[float]] = field(default=None, repr=False)
    
    @property
    def embedding(self) -> Optional[np.ndarray]:
        """è·å– embedding å‘é‡ï¼ˆnumpy arrayï¼‰"""
        if self._embedding is not None:
            return np.array(self._embedding, dtype='float32')
        return None
    
    @embedding.setter
    def embedding(self, value):
        """è®¾ç½® embedding å‘é‡"""
        if value is not None:
            if isinstance(value, np.ndarray):
                self._embedding = value.tolist()
            else:
                self._embedding = list(value)
        else:
            self._embedding = None
    
    def to_dict(self) -> Dict[str, Any]:
        d = asdict(self)
        d['context_type'] = self.context_type.value
        # _embedding å­—æ®µåœ¨ asdict ä¸­ä¼šè‡ªåŠ¨åŒ…å«
        return d
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'PersistentContext':
        data = data.copy()  # é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        data['context_type'] = ContextType(data['context_type'])
        # å…¼å®¹æ—§æ•°æ®ï¼ˆæ²¡æœ‰ _embedding å­—æ®µï¼‰
        if '_embedding' not in data:
            data['_embedding'] = None
        # ç§»é™¤å½’æ¡£æ—¶æ·»åŠ çš„é¢å¤–å­—æ®µï¼ˆè¿™äº›å­—æ®µä¸æ˜¯ PersistentContext çš„å±æ€§ï¼‰
        data.pop('archived_at', None)
        data.pop('archive_reason', None)
        return cls(**data)


class ContextTracker:
    """æŒä¹…ä¸Šä¸‹æ–‡è¿½è¸ªå™¨
    
    å¢é•¿æ§åˆ¶ç­–ç•¥ï¼š
    1. æ¯ä¸ªç±»å‹æœ€å¤šä¿ç•™ max_per_type ä¸ªæ¡ä»¶ï¼ˆé»˜è®¤10ä¸ªï¼‰
    2. ç›¸ä¼¼å†…å®¹è‡ªåŠ¨åˆå¹¶ï¼ˆå¢åŠ ç½®ä¿¡åº¦è€Œéæ–°å¢ï¼‰- ä½¿ç”¨ Embedding è¯­ä¹‰ç›¸ä¼¼åº¦
    3. ç½®ä¿¡åº¦è¡°å‡ï¼šé•¿æœŸæœªè¢«å¼•ç”¨çš„æ¡ä»¶ç½®ä¿¡åº¦ä¸‹é™
    4. è¶…å‡ºæ•°é‡æ—¶ï¼Œæ·˜æ±°ç½®ä¿¡åº¦æœ€ä½çš„
    5. æœ‰LLMæ—¶ï¼Œå¯ä»¥æ™ºèƒ½å‹ç¼©å¤šä¸ªæ¡ä»¶
    6. ä½ç½®ä¿¡åº¦æ¡ä»¶è‡ªåŠ¨å½’æ¡£åˆ° archive/contexts.jsonl
    
    æ™ºèƒ½å»é‡æœºåˆ¶ï¼š
    - ç¬¬ä¸€çº§ï¼šEmbedding å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ>0.85 ç›´æ¥åˆå¹¶ï¼Œ0.70-0.85 å¯èƒ½ç›¸ä¼¼ï¼‰
    - ç¬¬äºŒçº§ï¼šLLM æå–æ—¶åˆ¤æ–­æ˜¯å¦ä¸å·²æœ‰æ¡ä»¶é‡å¤
    - ç¬¬ä¸‰çº§ï¼šå®šæœŸæ‰¹é‡æ•´ç†åˆå¹¶
    """
    
    # é…ç½®å¸¸é‡ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæ”¯æŒçƒ­æ›´æ–°ï¼‰
    @staticmethod
    def _get_limits_config():
        """è·å–é™åˆ¶é…ç½®ï¼ˆæ¯æ¬¡è°ƒç”¨æ—¶è¯»å–ï¼Œæ”¯æŒçƒ­æ›´æ–°ï¼‰"""
        return {
            'max_per_type': int(os.environ.get('CONTEXT_MAX_PER_TYPE', '10')),
            'max_total': int(os.environ.get('CONTEXT_MAX_TOTAL', '100')),
            'decay_days': int(os.environ.get('CONTEXT_DECAY_DAYS', '14')),
            'decay_rate': float(os.environ.get('CONTEXT_DECAY_RATE', '0.05')),
            'min_confidence': float(os.environ.get('CONTEXT_MIN_CONFIDENCE', '0.1')),
        }
    
    # å…¼å®¹æ—§ä»£ç çš„å±æ€§è®¿é—®
    @property
    def MAX_PER_TYPE(self) -> int:
        return self._get_limits_config()['max_per_type']
    
    @property
    def MAX_TOTAL(self) -> int:
        return self._get_limits_config()['max_total']
    
    @property
    def DECAY_DAYS(self) -> int:
        return self._get_limits_config()['decay_days']
    
    @property
    def DECAY_RATE(self) -> float:
        return self._get_limits_config()['decay_rate']
    
    @property
    def MIN_CONFIDENCE(self) -> float:
        return self._get_limits_config()['min_confidence']
    
    SIMILARITY_THRESHOLD = 0.6  # è¯é‡å ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
    FLOAT_EPSILON = 1e-9  # æµ®ç‚¹æ•°æ¯”è¾ƒå®¹å·®
    
    # æ™ºèƒ½å»é‡é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæ”¯æŒçƒ­æ›´æ–°ï¼‰
    @staticmethod
    def _get_dedup_config() -> Dict[str, Any]:
        """è·å–å»é‡é…ç½®ï¼ˆæ¯æ¬¡è°ƒç”¨æ—¶è¯»å–ï¼Œæ”¯æŒçƒ­æ›´æ–°ï¼‰
        
        Returns:
            Dict with keys: enabled, high_threshold, low_threshold
        """
        return {
            'enabled': os.environ.get('DEDUP_EMBEDDING_ENABLED', 'true').lower() in ('true', '1', 'yes'),
            'high_threshold': float(os.environ.get('DEDUP_HIGH_THRESHOLD', '0.85')),
            'low_threshold': float(os.environ.get('DEDUP_LOW_THRESHOLD', '0.70'))
        }
    
    def __init__(self, base_path: Optional[str] = None, llm_client: Optional[Any] = None, 
                 embedding_backend: Optional[Any] = None, storage_dir: Optional[str] = None):
        """åˆå§‹åŒ–æŒä¹…ä¸Šä¸‹æ–‡è¿½è¸ªå™¨
        
        å­˜å‚¨ç»“æ„ï¼š{base_path}/{user_id}/{character_id}/contexts.json
        ä¸ MultiTenantStorage å’Œ ForeshadowingTracker ä¿æŒä¸€è‡´çš„è·¯å¾„ç»“æ„ã€‚
        
        Args:
            base_path: æ•°æ®æ ¹ç›®å½•ï¼ˆæ–°å‚æ•°ï¼Œæ¨èä½¿ç”¨ï¼‰
            llm_client: LLM å®¢æˆ·ç«¯ï¼ˆç”¨äºæ™ºèƒ½æå–å’Œå‹ç¼©ï¼‰
            embedding_backend: Embedding åç«¯ï¼ˆç”¨äºæ™ºèƒ½å»é‡ï¼‰ï¼Œå¦‚æœä¸º None ä¼šå°è¯•è‡ªåŠ¨è·å–
            storage_dir: æ—§å‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰ï¼Œå¦‚æœæä¾›åˆ™è‡ªåŠ¨è¿ç§»åˆ°æ–°ç»“æ„
        """
        # å‘åå…¼å®¹ï¼šå¦‚æœæä¾›äº† storage_dir è€Œé base_path
        if storage_dir and not base_path:
            # æ—§çš„ storage_dir æ ¼å¼æ˜¯ {data_root}/data/contexts
            # æ–°çš„ base_path æ ¼å¼æ˜¯ {data_root}/data
            if storage_dir.endswith('contexts'):
                base_path = os.path.dirname(storage_dir)
            else:
                base_path = storage_dir
        
        self.base_path = base_path
        self.llm_client = llm_client
        # æŒ‰ {user_id}/{character_id} åˆ†éš”çš„ä¸Šä¸‹æ–‡å­˜å‚¨
        self.contexts: Dict[str, List[PersistentContext]] = {}  # cache_key -> contexts
        
        # Embedding åç«¯ï¼ˆç”¨äºæ™ºèƒ½å»é‡ï¼‰
        self.embedding_backend = embedding_backend
        
        if base_path:
            os.makedirs(base_path, exist_ok=True)
        self._load_all()
        
        # æ¡ä»¶æ£€æµ‹æ¨¡å¼ï¼ˆç”¨äºè‡ªåŠ¨ä»å¯¹è¯ä¸­æå–æ¡ä»¶ï¼‰
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å¼ï¼Œé¿å…è¯¯åŒ¹é…
        self.detection_patterns = {
            ContextType.USER_IDENTITY: [
                r'æˆ‘æ˜¯(?:ä¸€ä¸ª|ä¸€å|ä¸€ä½)?([^ï¼Œã€‚,.\n]{2,20}?)(?:ï¼Œ|ã€‚|,|\.|$)',  # æˆ‘æ˜¯XXX
                r'æˆ‘ç›®å‰æ˜¯(?:ä¸€ä¸ª|ä¸€å)?([^ï¼Œã€‚,.\n]{2,20}?)(?:ï¼Œ|ã€‚|,|\.|$)',   # æˆ‘ç›®å‰æ˜¯XXX
            ],
            ContextType.USER_GOAL: [
                r'æˆ‘æƒ³(?:è¦)?(?:å­¦ä¹ |åš|å¼€å‘|åˆ›å»º|å®ç°|å®Œæˆ)([^ï¼Œã€‚,.\n]{2,30}?)(?:ï¼Œ|ã€‚|,|\.|$)',  # æˆ‘æƒ³åšXXX
                r'æˆ‘çš„ç›®æ ‡æ˜¯([^ï¼Œã€‚,.\n]{2,30}?)(?:ï¼Œ|ã€‚|,|\.|$)',  # æˆ‘çš„ç›®æ ‡æ˜¯XXX
                r'æˆ‘(?:æ­£åœ¨|æ‰“ç®—|å‡†å¤‡)(?:å­¦ä¹ |åš|å¼€å‘|åˆ›å»º)([^ï¼Œã€‚,.\n]{2,30}?)(?:ï¼Œ|ã€‚|,|\.|$)',
            ],
            ContextType.ENVIRONMENT: [
                r'(?:æˆ‘)?åœ¨(Windows|Linux|Mac|Ubuntu|CentOS|MacOS)(?:ä¸Š|ç³»ç»Ÿ)?(?:å¼€å‘|éƒ¨ç½²|è¿è¡Œ)',  # åœ¨Windowsä¸Šå¼€å‘
                r'(?:å¼€å‘|éƒ¨ç½²|è¿è¡Œ)(?:ç¯å¢ƒ|å¹³å°)(?:æ˜¯|ç”¨çš„æ˜¯)(Windows|Linux|Mac|Ubuntu)',
                r'(Windows|Linux|Mac|Ubuntu)(?:ä¸Š)?å¼€å‘.{0,10}?(Windows|Linux|Mac|Ubuntu)(?:ä¸Š)?éƒ¨ç½²',  # Windowså¼€å‘Ubuntuéƒ¨ç½²
            ],
            ContextType.PROJECT: [
                r'(?:æˆ‘|æˆ‘ä»¬)(?:æ­£åœ¨|åœ¨)?(?:å¼€å‘|åš|å†™)(?:ä¸€ä¸ª|è¿™ä¸ª)?([^ï¼Œã€‚,.\n]{2,20}?)(?:é¡¹ç›®|ç³»ç»Ÿ|ç¨‹åº|åº”ç”¨)',
                r'è¿™ä¸ªé¡¹ç›®(?:æ˜¯|å«)([^ï¼Œã€‚,.\n]{2,20}?)(?:ï¼Œ|ã€‚|,|\.|$)',
            ],
        }
        
        # æœ€å°åŒ¹é…é•¿åº¦ï¼ˆé¿å…åŒ¹é…å¤ªçŸ­çš„æ— æ„ä¹‰ç‰‡æ®µï¼‰
        self.min_content_length = 5
        
        # LLM æå–æç¤º
        self.extraction_prompt = """åˆ†æä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œæå–å‡ºåº”è¯¥ä½œä¸º"æŒä¹…å‰ææ¡ä»¶"çš„ä¿¡æ¯ã€‚

ã€é‡è¦ã€‘åªæå–å¯¹è¯ä¸­**æ˜ç¡®æåˆ°**çš„ä¿¡æ¯ï¼Œ**ä¸è¦æ¨æµ‹ã€è„‘è¡¥æˆ–æ·»åŠ å¯¹è¯ä¸­æ²¡æœ‰çš„å†…å®¹**ã€‚
å¦‚æœå¯¹è¯å†…å®¹å¾ˆçŸ­æˆ–æ²¡æœ‰æ˜ç¡®çš„èƒŒæ™¯ä¿¡æ¯ï¼Œè¯·è¿”å›ç©ºæ•°ç»„ []ã€‚

æŒä¹…å‰ææ¡ä»¶æ˜¯æŒ‡ï¼šä¸€æ—¦ç¡®ç«‹å°±åº”è¯¥åœ¨åç»­æ‰€æœ‰å¯¹è¯ä¸­é»˜è®¤æˆç«‹çš„èƒŒæ™¯ä¿¡æ¯ã€‚

åˆé€‚çš„ä¾‹å­ï¼ˆå¯¹è¯ä¸­æ˜ç¡®æåˆ°çš„ï¼‰ï¼š
- å¯¹è¯è¯´"æˆ‘æ˜¯ä¸€ä¸ªå¤§å­¦æ¯•ä¸šç”Ÿï¼Œæƒ³åˆ›ä¸š" â†’ å¯ä»¥æå–
- å¯¹è¯è¯´"è§’è‰²æ‹¿èµ·äº†é­”æ³•å‰‘" â†’ å¯ä»¥æå–"è§’è‰²æºå¸¦é­”æ³•å‰‘"
- å¯¹è¯è¯´"éœ²è¥¿æœ‰é»‘å®¢æŠ€èƒ½" â†’ å¯ä»¥æå–æŠ€èƒ½ä¿¡æ¯

ä¸åˆé€‚çš„ä¾‹å­ï¼ˆå¯¹è¯ä¸­æ²¡æœ‰æ˜ç¡®æåˆ°çš„ï¼‰ï¼š
- å¯¹è¯åªæ˜¯æ™®é€šèŠå¤© â†’ ä¸è¦ç¼–é€ è§’è‰²ç‰¹å¾
- å¯¹è¯æ²¡æœ‰æåˆ°å…·ä½“æŠ€èƒ½ â†’ ä¸è¦çŒœæµ‹è§’è‰²å¯èƒ½æœ‰ä»€ä¹ˆæŠ€èƒ½
- å¯¹è¯æ²¡æœ‰æåˆ°å…³ç³» â†’ ä¸è¦æ¨æµ‹è§’è‰²ä¹‹é—´çš„å…³ç³»

å¯¹è¯å†…å®¹ï¼š
{content}

è¯·ä»¥JSONæ ¼å¼è¿”å›æå–çš„æ¡ä»¶ï¼ˆå¦‚æœæ²¡æœ‰æ˜ç¡®çš„æ¡ä»¶åˆ™è¿”å›ç©ºæ•°ç»„ []ï¼‰ï¼š
[
  {{"type": "user_identity|user_goal|user_preference|environment|project|time_constraint|character_trait|world_setting|relationship|emotional_state|skill_ability|item_prop|assumption|constraint|custom", "content": "æ¡ä»¶å†…å®¹ï¼ˆå¿…é¡»æ˜¯å¯¹è¯ä¸­æ˜ç¡®æåˆ°çš„ï¼‰", "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"]}}
]

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„æ¡ä»¶ï¼Œè¿”å› []ã€‚"""
    
    def _sanitize_path_component(self, name: str) -> str:
        """æ¸…ç†è·¯å¾„ç»„ä»¶ä¸­çš„éæ³•å­—ç¬¦"""
        return "".join(c if c.isalnum() or c in ('_', '-') else '_' for c in name)
    
    def _get_cache_key(self, user_id: str, character_id: str) -> str:
        """è·å–å†…éƒ¨ç¼“å­˜é”®"""
        return f"{user_id}/{character_id}"
    
    def _get_storage_path(self, user_id: str, character_id: str) -> str:
        """è·å–å­˜å‚¨è·¯å¾„
        
        æ–°ç»“æ„ï¼š{base_path}/{user_id}/{character_id}/contexts.json
        """
        safe_user_id = self._sanitize_path_component(user_id)
        safe_char_id = self._sanitize_path_component(character_id)
        return os.path.join(self.base_path, safe_user_id, safe_char_id, 'contexts.json')
    
    def _load_all(self):
        """åŠ è½½æ‰€æœ‰ç”¨æˆ·/è§’è‰²çš„ä¸Šä¸‹æ–‡ï¼ˆæ–°ç»“æ„ï¼‰"""
        if not self.base_path or not os.path.exists(self.base_path):
            return
        
        # æ–°ç»“æ„ï¼šéå† {base_path}/{user_id}/{character_id}/contexts.json
        for user_id in os.listdir(self.base_path):
            user_path = os.path.join(self.base_path, user_id)
            if not os.path.isdir(user_path) or user_id.startswith('.'):
                continue
            for character_id in os.listdir(user_path):
                char_path = os.path.join(user_path, character_id)
                if not os.path.isdir(char_path):
                    continue
                ctx_file = os.path.join(char_path, 'contexts.json')
                if os.path.exists(ctx_file):
                    self._load_user(user_id, character_id)
    
    def _load_user(self, user_id: str, character_id: str = "default"):
        """åŠ è½½ç”¨æˆ·/è§’è‰²çš„ä¸Šä¸‹æ–‡"""
        cache_key = self._get_cache_key(user_id, character_id)
        filepath = self._get_storage_path(user_id, character_id)
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.contexts[cache_key] = [
                        PersistentContext.from_dict(item) for item in data
                    ]
            except Exception as e:
                print(f"[Recall] åŠ è½½ä¸Šä¸‹æ–‡æ•°æ®å¤±è´¥ ({user_id}/{character_id}): {e}")
                self.contexts[cache_key] = []
    
    def _save_user(self, user_id: str, character_id: str = "default"):
        """ä¿å­˜ç”¨æˆ·/è§’è‰²çš„ä¸Šä¸‹æ–‡"""
        cache_key = self._get_cache_key(user_id, character_id)
        filepath = self._get_storage_path(user_id, character_id)
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        contexts = self.contexts.get(cache_key, [])
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump([c.to_dict() for c in contexts], f, ensure_ascii=False, indent=2)
    
    # =========================
    # å½’æ¡£æœºåˆ¶ï¼ˆä½ç½®ä¿¡åº¦æ¡ä»¶è‡ªåŠ¨å½’æ¡£ï¼‰
    # =========================
    
    MAX_ARCHIVE_FILE_SIZE = 10 * 1024 * 1024  # 10MB å½’æ¡£æ–‡ä»¶å¤§å°ä¸Šé™
    
    def _get_archive_path(self, user_id: str, character_id: str) -> str:
        """è·å–å½’æ¡£æ–‡ä»¶è·¯å¾„
        
        è·¯å¾„æ ¼å¼ï¼š{base_path}/{user_id}/{character_id}/archive/contexts.jsonl
        """
        safe_user_id = self._sanitize_path_component(user_id)
        safe_char_id = self._sanitize_path_component(character_id)
        archive_dir = os.path.join(self.base_path, safe_user_id, safe_char_id, 'archive')
        os.makedirs(archive_dir, exist_ok=True)
        return os.path.join(archive_dir, 'contexts.jsonl')
    
    def _archive_context(self, ctx: 'PersistentContext', user_id: str, character_id: str,
                         reason: str = 'low_confidence') -> bool:
        """å°†å•ä¸ªæ¡ä»¶å½’æ¡£åˆ° JSONL æ–‡ä»¶
        
        å½’æ¡£æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«å®Œæ•´æ¡ä»¶æ•°æ®å’Œå½’æ¡£æ—¶é—´
        
        Args:
            ctx: è¦å½’æ¡£çš„æ¡ä»¶
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            reason: å½’æ¡£åŸå›  ('low_confidence', 'type_overflow', 'total_overflow')
        
        Returns:
            bool: å½’æ¡£æ˜¯å¦æˆåŠŸ
        """
        import logging
        logger = logging.getLogger(__name__)
        
        try:
            archive_path = self._get_archive_path(user_id, character_id)
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¦‚æœè¶…è¿‡ä¸Šé™åˆ™è½®è½¬
            if os.path.exists(archive_path):
                file_size = os.path.getsize(archive_path)
                if file_size >= self.MAX_ARCHIVE_FILE_SIZE:
                    # è½®è½¬æ–‡ä»¶ï¼šcontexts.jsonl -> contexts_001.jsonl
                    base_name = archive_path.rsplit('.', 1)[0]
                    suffix = 1
                    while os.path.exists(f"{base_name}_{suffix:03d}.jsonl"):
                        suffix += 1
                    os.rename(archive_path, f"{base_name}_{suffix:03d}.jsonl")
                    logger.info(f"[ContextTracker] å½’æ¡£æ–‡ä»¶è½®è½¬: contexts.jsonl -> contexts_{suffix:03d}.jsonl")
            
            # å‡†å¤‡å½’æ¡£æ•°æ®
            archive_data = ctx.to_dict()
            archive_data['archived_at'] = time.time()
            archive_data['archive_reason'] = reason
            
            # è¿½åŠ å†™å…¥ JSONL
            with open(archive_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(archive_data, ensure_ascii=False) + '\n')
            
            logger.debug(f"[ContextTracker] å·²å½’æ¡£æ¡ä»¶: {ctx.id} (ç½®ä¿¡åº¦={ctx.confidence:.2f}, åŸå› ={reason})")
            return True
            
        except Exception as e:
            logger.error(f"[ContextTracker] å½’æ¡£æ¡ä»¶å¤±è´¥: {e}")
            return False
    
    def get_context_by_id(self, context_id: str, user_id: str = "default", 
                          character_id: str = "default") -> Optional['PersistentContext']:
        """æ ¹æ®IDè·å–æ¡ä»¶ï¼ˆåŒ…æ‹¬å·²å½’æ¡£çš„ï¼‰
        
        ä¼˜å…ˆä»æ´»è·ƒæ¡ä»¶ä¸­æŸ¥æ‰¾ï¼Œæ‰¾ä¸åˆ°å†å»å½’æ¡£ä¸­æ‰¾ï¼ˆåŒ…æ‹¬åˆ†å·æ–‡ä»¶ï¼‰
        """
        # å…ˆä»æ´»è·ƒæ¡ä»¶ä¸­æ‰¾
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key in self.contexts:
            for ctx in self.contexts[cache_key]:
                if ctx.id == context_id:
                    return ctx
        
        # å†ä»å½’æ¡£ä¸­æ‰¾ï¼ˆæœç´¢æ‰€æœ‰å½’æ¡£æ–‡ä»¶ï¼ŒåŒ…æ‹¬åˆ†å·ï¼‰
        if self.base_path:
            safe_user_id = self._sanitize_path_component(user_id)
            safe_char_id = self._sanitize_path_component(character_id)
            archive_dir = os.path.join(self.base_path, safe_user_id, safe_char_id, 'archive')
            
            if os.path.exists(archive_dir):
                for filename in os.listdir(archive_dir):
                    if filename.startswith('contexts') and filename.endswith('.jsonl'):
                        archive_path = os.path.join(archive_dir, filename)
                        try:
                            with open(archive_path, 'r', encoding='utf-8') as f:
                                for line in f:
                                    if line.strip():
                                        data = json.loads(line)
                                        if data.get('id') == context_id:
                                            return PersistentContext.from_dict(data)
                        except Exception:
                            pass
        
        return None

    def get_archived_contexts(
        self,
        user_id: str = "default",
        character_id: str = "default",
        page: int = 1,
        page_size: int = 20,
        search: Optional[str] = None,
        context_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """è·å–å½’æ¡£çš„æŒä¹…æ¡ä»¶åˆ—è¡¨ï¼ˆåˆ†é¡µã€æœç´¢ã€ç­›é€‰ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            page: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
            page_size: æ¯é¡µæ•°é‡
            search: æœç´¢å…³é”®è¯ï¼ˆæœç´¢å†…å®¹ï¼‰
            context_type: ç±»å‹ç­›é€‰
            
        Returns:
            Dict: {
                'items': List[Dict],  # å½“å‰é¡µçš„æ¡ä»¶åˆ—è¡¨
                'total': int,         # æ€»æ•°é‡
                'page': int,          # å½“å‰é¡µ
                'page_size': int,     # æ¯é¡µæ•°é‡
                'total_pages': int    # æ€»é¡µæ•°
            }
        """
        all_archived = []
        
        if self.base_path:
            safe_user_id = self._sanitize_path_component(user_id)
            safe_char_id = self._sanitize_path_component(character_id)
            archive_dir = os.path.join(self.base_path, safe_user_id, safe_char_id, 'archive')
            
            if os.path.exists(archive_dir):
                # è¯»å–æ‰€æœ‰å½’æ¡£æ–‡ä»¶
                for filename in sorted(os.listdir(archive_dir), reverse=True):
                    if filename.startswith('contexts') and filename.endswith('.jsonl'):
                        archive_path = os.path.join(archive_dir, filename)
                        try:
                            with open(archive_path, 'r', encoding='utf-8') as f:
                                for line in f:
                                    if line.strip():
                                        data = json.loads(line)
                                        all_archived.append(data)
                        except Exception:
                            pass
        
        # æŒ‰å½’æ¡£æ—¶é—´å€’åºæ’åˆ—
        all_archived.sort(key=lambda x: x.get('archived_at', 0), reverse=True)
        
        # ç­›é€‰ï¼šç±»å‹
        if context_type:
            all_archived = [c for c in all_archived if c.get('context_type') == context_type]
        
        # ç­›é€‰ï¼šæœç´¢
        if search:
            search_lower = search.lower()
            all_archived = [c for c in all_archived if search_lower in c.get('content', '').lower()]
        
        # åˆ†é¡µ
        total = len(all_archived)
        total_pages = max(1, (total + page_size - 1) // page_size)
        page = max(1, min(page, total_pages))
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        items = all_archived[start_idx:end_idx]
        
        return {
            'items': items,
            'total': total,
            'page': page,
            'page_size': page_size,
            'total_pages': total_pages
        }

    def restore_from_archive(
        self,
        context_id: str,
        user_id: str = "default",
        character_id: str = "default"
    ) -> Optional['PersistentContext']:
        """ä»å½’æ¡£æ¢å¤æ¡ä»¶åˆ°æ´»è·ƒåˆ—è¡¨
        
        Args:
            context_id: æ¡ä»¶ID
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            
        Returns:
            Optional[PersistentContext]: æ¢å¤çš„æ¡ä»¶ï¼Œæœªæ‰¾åˆ°è¿”å› None
        """
        import logging
        logger = logging.getLogger(__name__)
        
        if not self.base_path:
            return None
        
        safe_user_id = self._sanitize_path_component(user_id)
        safe_char_id = self._sanitize_path_component(character_id)
        archive_dir = os.path.join(self.base_path, safe_user_id, safe_char_id, 'archive')
        
        if not os.path.exists(archive_dir):
            return None
        
        # åœ¨æ‰€æœ‰å½’æ¡£æ–‡ä»¶ä¸­æŸ¥æ‰¾å¹¶ç§»é™¤
        found_data = None
        for filename in os.listdir(archive_dir):
            if not (filename.startswith('contexts') and filename.endswith('.jsonl')):
                continue
            
            archive_path = os.path.join(archive_dir, filename)
            lines_to_keep = []
            
            try:
                with open(archive_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            data = json.loads(line)
                            if data.get('id') == context_id and found_data is None:
                                found_data = data
                            else:
                                lines_to_keep.append(line)
                
                if found_data:
                    # é‡å†™å½’æ¡£æ–‡ä»¶ï¼ˆç§»é™¤å·²æ¢å¤çš„æ¡ä»¶ï¼‰
                    with open(archive_path, 'w', encoding='utf-8') as f:
                        f.writelines(lines_to_keep)
                    break
            except Exception as e:
                logger.error(f"[ContextTracker] è¯»å–å½’æ¡£æ–‡ä»¶å¤±è´¥: {e}")
        
        if not found_data:
            return None
        
        # ç§»é™¤å½’æ¡£å­—æ®µï¼Œåˆ›å»º PersistentContext å¯¹è±¡
        found_data.pop('archived_at', None)
        found_data.pop('archive_reason', None)
        ctx = PersistentContext.from_dict(found_data)
        ctx.is_active = True
        
        # æ·»åŠ åˆ°æ´»è·ƒåˆ—è¡¨
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            self.contexts[cache_key] = []
        self.contexts[cache_key].append(ctx)
        self._save_user(user_id, character_id)
        
        logger.info(f"[ContextTracker] å·²ä»å½’æ¡£æ¢å¤æ¡ä»¶: {context_id}")
        return ctx

    def delete_archived(
        self,
        context_id: str,
        user_id: str = "default",
        character_id: str = "default"
    ) -> bool:
        """å½»åº•åˆ é™¤å½’æ¡£ä¸­çš„æ¡ä»¶
        
        Args:
            context_id: æ¡ä»¶ID
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        import logging
        logger = logging.getLogger(__name__)
        
        if not self.base_path:
            return False
        
        safe_user_id = self._sanitize_path_component(user_id)
        safe_char_id = self._sanitize_path_component(character_id)
        archive_dir = os.path.join(self.base_path, safe_user_id, safe_char_id, 'archive')
        
        if not os.path.exists(archive_dir):
            return False
        
        deleted = False
        for filename in os.listdir(archive_dir):
            if not (filename.startswith('contexts') and filename.endswith('.jsonl')):
                continue
            
            archive_path = os.path.join(archive_dir, filename)
            lines_to_keep = []
            
            try:
                with open(archive_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            data = json.loads(line)
                            if data.get('id') == context_id:
                                deleted = True
                            else:
                                lines_to_keep.append(line)
                
                if deleted:
                    with open(archive_path, 'w', encoding='utf-8') as f:
                        f.writelines(lines_to_keep)
                    logger.info(f"[ContextTracker] å·²å½»åº•åˆ é™¤å½’æ¡£æ¡ä»¶: {context_id}")
                    break
            except Exception as e:
                logger.error(f"[ContextTracker] åˆ é™¤å½’æ¡£æ¡ä»¶å¤±è´¥: {e}")
        
        return deleted

    def clear_archived(
        self,
        user_id: str = "default",
        character_id: str = "default"
    ) -> int:
        """æ¸…ç©ºæ‰€æœ‰å½’æ¡£æ¡ä»¶
        
        Args:
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            
        Returns:
            int: åˆ é™¤çš„æ¡ä»¶æ•°é‡
        """
        import logging
        logger = logging.getLogger(__name__)
        
        if not self.base_path:
            return 0
        
        safe_user_id = self._sanitize_path_component(user_id)
        safe_char_id = self._sanitize_path_component(character_id)
        archive_dir = os.path.join(self.base_path, safe_user_id, safe_char_id, 'archive')
        
        if not os.path.exists(archive_dir):
            return 0
        
        count = 0
        for filename in os.listdir(archive_dir):
            if filename.startswith('contexts') and filename.endswith('.jsonl'):
                archive_path = os.path.join(archive_dir, filename)
                try:
                    # ç»Ÿè®¡è¡Œæ•°
                    with open(archive_path, 'r', encoding='utf-8') as f:
                        count += sum(1 for line in f if line.strip())
                    # åˆ é™¤æ–‡ä»¶
                    os.remove(archive_path)
                except Exception as e:
                    logger.error(f"[ContextTracker] æ¸…ç©ºå½’æ¡£æ–‡ä»¶å¤±è´¥: {e}")
        
        logger.info(f"[ContextTracker] å·²æ¸…ç©ºå½’æ¡£æ¡ä»¶: {count} ä¸ª")
        return count

    def archive_context(
        self,
        context_id: str,
        user_id: str = "default",
        character_id: str = "default"
    ) -> bool:
        """æ‰‹åŠ¨å°†æ´»è·ƒæ¡ä»¶å½’æ¡£
        
        Args:
            context_id: æ¡ä»¶ID
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå½’æ¡£
        """
        import logging
        logger = logging.getLogger(__name__)
        
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return False
        
        # æŸ¥æ‰¾å¹¶ç§»é™¤
        ctx_to_archive = None
        for i, ctx in enumerate(self.contexts[cache_key]):
            if ctx.id == context_id:
                ctx_to_archive = self.contexts[cache_key].pop(i)
                break
        
        if not ctx_to_archive:
            return False
        
        # å½’æ¡£
        success = self._archive_context(ctx_to_archive, user_id, character_id, reason='manual')
        if success:
            self._save_user(user_id, character_id)
            logger.info(f"[ContextTracker] å·²æ‰‹åŠ¨å½’æ¡£æ¡ä»¶: {context_id}")
        
        return success

    def update_context(
        self,
        context_id: str,
        user_id: str = "default",
        character_id: str = "default",
        content: Optional[str] = None,
        context_type: Optional[str] = None,
        confidence: Optional[float] = None,
        keywords: Optional[List[str]] = None
    ) -> Optional['PersistentContext']:
        """æ›´æ–°æŒä¹…æ¡ä»¶çš„å­—æ®µ
        
        Args:
            context_id: æ¡ä»¶ID
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            content: æ–°å†…å®¹ï¼ˆå¯é€‰ï¼‰
            context_type: æ–°ç±»å‹ï¼ˆå¯é€‰ï¼‰
            confidence: æ–°ç½®ä¿¡åº¦ï¼ˆå¯é€‰ï¼‰
            keywords: æ–°å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Optional[PersistentContext]: æ›´æ–°åçš„æ¡ä»¶ï¼Œæœªæ‰¾åˆ°è¿”å› None
        """
        import logging
        logger = logging.getLogger(__name__)
        
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return None
        
        for ctx in self.contexts[cache_key]:
            if ctx.id == context_id:
                if content is not None:
                    ctx.content = content
                    # æ›´æ–° embedding
                    new_embedding = self._get_embedding(content)
                    if new_embedding:
                        ctx.embedding = new_embedding
                if context_type is not None:
                    try:
                        ctx.context_type = ContextType(context_type)
                    except ValueError:
                        ctx.context_type = ContextType.CUSTOM
                if confidence is not None:
                    ctx.confidence = max(0.0, min(1.0, confidence))
                if keywords is not None:
                    ctx.keywords = keywords
                
                self._save_user(user_id, character_id)
                logger.info(f"[ContextTracker] å·²æ›´æ–°æ¡ä»¶: {context_id}")
                return ctx
        
        return None

    def _get_embedding(self, text: str) -> Optional[List[float]]:
        """è·å–æ–‡æœ¬çš„Embeddingå‘é‡
        
        ä½¿ç”¨embedding_backendè·å–å‘é‡ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if not self.embedding_backend:
            return None
        
        try:
            # embedding_backend.embed() è¿”å› List[List[float]]
            embeddings = self.embedding_backend.embed([text])
            if embeddings and len(embeddings) > 0:
                return embeddings[0]
        except Exception as e:
            import logging
            logging.getLogger(__name__).debug(f"è·å–Embeddingå¤±è´¥: {e}")
        
        return None
    
    def _compute_embedding_similarity(self, emb1: List[float], emb2: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸ªEmbeddingå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        ä½¿ç”¨numpyé«˜æ•ˆè®¡ç®—ï¼ŒèŒƒå›´ [-1, 1]ï¼Œé€šå¸¸æ­£å¸¸æ–‡æœ¬ç›¸ä¼¼åº¦åœ¨ [0, 1]
        """
        if emb1 is None or emb2 is None:
            return 0.0
        if len(emb1) == 0 or len(emb2) == 0:
            return 0.0
        
        vec1 = np.array(emb1)
        vec2 = np.array(emb2)
        
        # ä½™å¼¦ç›¸ä¼¼åº¦ = dot(a, b) / (norm(a) * norm(b))
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(dot_product / (norm1 * norm2))
    
    def _compute_word_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„è¯é‡å ç›¸ä¼¼åº¦ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
        
        åŸºäºJaccardç›¸ä¼¼åº¦ï¼Œæ”¯æŒä¸­è‹±æ–‡
        """
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        # å¯¹äºä¸­æ–‡ï¼ŒæŒ‰å­—ç¬¦
        if len(words1) <= 2:  # å¯èƒ½æ˜¯ä¸­æ–‡
            words1 = set(text1.lower())
            words2 = set(text2.lower())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        return len(intersection) / len(union) if union else 0.0
    
    def _compute_similarity(self, text1: str, text2: str, 
                           emb1: Optional[List[float]] = None, 
                           emb2: Optional[List[float]] = None) -> Tuple[float, str]:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦
        
        ä¸‰çº§ç­–ç•¥ï¼š
        1. å¦‚æœä¸¤è¾¹éƒ½æœ‰Embeddingï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæœ€å‡†ç¡®ï¼‰
        2. å¦‚æœåªæœ‰ä¸€è¾¹æœ‰Embeddingï¼Œå°è¯•è·å–å¦ä¸€è¾¹çš„
        3. åå¤‡ï¼šä½¿ç”¨è¯é‡å ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
        
        Args:
            text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
            text2: ç¬¬äºŒä¸ªæ–‡æœ¬
            emb1: text1çš„Embeddingå‘é‡ï¼ˆå¯é€‰ï¼‰
            emb2: text2çš„Embeddingå‘é‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Tuple[float, str]: (ç›¸ä¼¼åº¦, ä½¿ç”¨çš„æ–¹æ³• "embedding"/"word")
        """
        dedup_config = self._get_dedup_config()
        
        # å¦‚æœå¯ç”¨äº†Embeddingå»é‡
        if dedup_config['enabled']:
            # ä¼˜åŒ–ï¼šåªæœ‰å½“æ–°å†…å®¹çš„ embedding å·²ç»è®¡ç®—å¥½æ—¶ï¼Œæ‰ä½¿ç”¨ embedding æ¯”è¾ƒ
            # ä¸è¦ä¸ºæ—§æ•°æ®ï¼ˆæ²¡æœ‰å­˜å‚¨ embeddingï¼‰è°ƒç”¨ APIï¼Œé¿å… N æ¬¡è°ƒç”¨
            # emb2 æ˜¯æ–°å†…å®¹çš„ embeddingï¼ˆè°ƒç”¨æ–¹åº”è¯¥é¢„å…ˆè®¡ç®—å¥½ï¼‰
            # emb1 æ˜¯ç°æœ‰æ¡ä»¶çš„ embeddingï¼ˆåº”è¯¥ä»å­˜å‚¨ä¸­è¯»å–ï¼Œå¦‚æœæ²¡æœ‰å°±è·³è¿‡ï¼‰
            
            # å¦‚æœä¸¤è¾¹éƒ½æœ‰Embeddingï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            if emb1 is not None and emb2 is not None:
                sim = self._compute_embedding_similarity(emb1, emb2)
                return (sim, "embedding")
        
        # åå¤‡ï¼šè¯é‡å 
        sim = self._compute_word_similarity(text1, text2)
        return (sim, "word")
    
    def _find_similar(self, content: str, user_id: str, character_id: str,
                     context_type: ContextType,
                     new_embedding: Optional[List[float]] = None) -> Tuple[Optional[PersistentContext], float, str]:
        """æŸ¥æ‰¾è¯­ä¹‰ç›¸ä¼¼çš„å·²æœ‰æ¡ä»¶
        
        ä½¿ç”¨ä¸‰çº§ç­–ç•¥ï¼š
        1. å®Œå…¨ç›¸åŒï¼šç›´æ¥è¿”å›
        2. Embeddingç›¸ä¼¼åº¦ >= HIGH_THRESHOLDï¼šé«˜åº¦ç›¸ä¼¼ï¼Œå¯è‡ªåŠ¨åˆå¹¶
        3. Embeddingç›¸ä¼¼åº¦ < LOW_THRESHOLDï¼šæ˜æ˜¾ä¸åŒï¼Œè·³è¿‡
        4. ä¸­é—´åŒºåŸŸï¼šè¿”å›ä½†æ ‡è®°ï¼Œå¯èƒ½éœ€è¦LLMåˆ¤æ–­
        
        Args:
            content: æ–°æ¡ä»¶å†…å®¹
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            context_type: æ¡ä»¶ç±»å‹
            new_embedding: æ–°æ¡ä»¶çš„Embeddingå‘é‡ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™é¿å…é‡å¤è®¡ç®—ï¼‰
            
        Returns:
            Tuple[Optional[PersistentContext], float, str]: 
                (ç›¸ä¼¼æ¡ä»¶æˆ–None, ç›¸ä¼¼åº¦, æ–¹æ³• "exact"/"embedding"/"word")
        """
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return (None, 0.0, "none")
        
        dedup_config = self._get_dedup_config()
        best_match: Optional[PersistentContext] = None
        best_sim = 0.0
        best_method = "none"
        
        for existing in self.contexts[cache_key]:
            if existing.context_type != context_type:
                continue
            if not existing.is_active:
                continue
            
            # 1. å®Œå…¨ç›¸åŒï¼ˆå¿«é€Ÿè·¯å¾„ï¼‰
            if existing.content.lower().strip() == content.lower().strip():
                return (existing, 1.0, "exact")
            
            # 2. è®¡ç®—ç›¸ä¼¼åº¦
            sim, method = self._compute_similarity(
                existing.content, content,
                emb1=existing.embedding,  # ä½¿ç”¨å·²å­˜å‚¨çš„Embedding
                emb2=new_embedding
            )
            
            # ä½¿ç”¨åŠ¨æ€é˜ˆå€¼ï¼ˆEmbeddingæ–¹æ³•ç”¨é…ç½®çš„é˜ˆå€¼ï¼Œè¯é‡å ç”¨å›ºå®šé˜ˆå€¼ï¼‰
            if method == "embedding":
                threshold = dedup_config['high_threshold']
            else:
                threshold = self.SIMILARITY_THRESHOLD
            
            # è®°å½•æœ€ä½³åŒ¹é…
            if sim > best_sim:
                best_sim = sim
                best_match = existing
                best_method = method
        
        # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°åˆå¹¶é˜ˆå€¼
        if best_match:
            if best_method == "embedding":
                if best_sim >= dedup_config['high_threshold']:
                    return (best_match, best_sim, best_method)
                elif best_sim < dedup_config['low_threshold']:
                    return (None, best_sim, best_method)  # æ˜æ˜¾ä¸åŒ
                else:
                    # ä¸­é—´åŒºåŸŸï¼Œè¿”å›åŒ¹é…ä½†éœ€è¦è¿›ä¸€æ­¥åˆ¤æ–­
                    return (best_match, best_sim, best_method + "_uncertain")
            else:
                # è¯é‡å æ–¹æ³•ï¼Œä½¿ç”¨å›ºå®šé˜ˆå€¼
                if best_sim >= self.SIMILARITY_THRESHOLD:
                    return (best_match, best_sim, best_method)
        
        return (None, best_sim, best_method)
    
    def _enforce_limits(self, user_id: str, character_id: str = "default"):
        """å¼ºåˆ¶æ‰§è¡Œæ•°é‡é™åˆ¶
        
        å¤„ç†æµç¨‹ï¼š
        1. åº”ç”¨ç½®ä¿¡åº¦è¡°å‡
        2. å½’æ¡£å¹¶åˆ é™¤ä½ç½®ä¿¡åº¦æ¡ä»¶
        3. æŒ‰ç±»å‹é™åˆ¶æ•°é‡ï¼ˆæº¢å‡ºçš„å½’æ¡£ï¼‰
        4. æŒ‰æ€»æ•°é™åˆ¶æ•°é‡ï¼ˆæº¢å‡ºçš„å½’æ¡£ï¼‰
        """
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return
        
        contexts = self.contexts[cache_key]
        
        # 1. åº”ç”¨ç½®ä¿¡åº¦è¡°å‡ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
        now = time.time()
        decay_threshold = now - (self.DECAY_DAYS * 24 * 3600)
        
        for ctx in contexts:
            if ctx.is_active and ctx.last_used < decay_threshold:
                # ä¹˜æ³•è¡°å‡ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰ï¼šæ¯æ¬¡è¡°å‡ DECAY_RATE æ¯”ä¾‹
                # ä¾‹å¦‚ DECAY_RATE=0.05 æ—¶ï¼Œæ¯æ¬¡è¡°å‡åä¿ç•™ 95% çš„ç½®ä¿¡åº¦
                # ä½äº MIN_CONFIDENCE æ—¶ä¼šè¢«ä¸‹é¢çš„é€»è¾‘å½’æ¡£
                ctx.confidence = max(0, ctx.confidence * (1 - self.DECAY_RATE))
        
        # 2. å½’æ¡£å¹¶åˆ é™¤ä½ç½®ä¿¡åº¦æ¡ä»¶
        to_archive = []
        for ctx in contexts:
            if ctx.confidence <= self.MIN_CONFIDENCE + self.FLOAT_EPSILON:  # ä½¿ç”¨ç±»å¸¸é‡å®¹å·®
                ctx.is_active = False
                to_archive.append(ctx)
        
        # å½’æ¡£ä½ç½®ä¿¡åº¦æ¡ä»¶
        for ctx in to_archive:
            self._archive_context(ctx, user_id, character_id, reason='low_confidence')
        
        # åªä¿ç•™æ´»è·ƒçš„
        contexts = [c for c in contexts if c.is_active]
        
        # 3. æ¯ç±»å‹æ•°é‡é™åˆ¶
        by_type: Dict[ContextType, List[PersistentContext]] = {}
        for ctx in contexts:
            if ctx.context_type not in by_type:
                by_type[ctx.context_type] = []
            by_type[ctx.context_type].append(ctx)
        
        # æ¯ç±»å‹åªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„ MAX_PER_TYPE ä¸ªï¼Œè¶…å‡ºçš„å½’æ¡£
        kept = []
        for ctx_type, type_contexts in by_type.items():
            # æŒ‰ç½®ä¿¡åº¦æ’åº
            type_contexts.sort(key=lambda c: -c.confidence)
            kept.extend(type_contexts[:self.MAX_PER_TYPE])
            # è¶…å‡ºçš„å½’æ¡£
            for ctx in type_contexts[self.MAX_PER_TYPE:]:
                self._archive_context(ctx, user_id, character_id, reason='type_overflow')
        
        # 4. æ€»æ•°é™åˆ¶
        if len(kept) > self.MAX_TOTAL:
            kept.sort(key=lambda c: -c.confidence)
            # è¶…å‡ºçš„å½’æ¡£
            for ctx in kept[self.MAX_TOTAL:]:
                self._archive_context(ctx, user_id, character_id, reason='total_overflow')
            kept = kept[:self.MAX_TOTAL]
        
        self.contexts[cache_key] = kept
        
        # ä¿å­˜æ›´æ”¹ï¼ˆç¡®ä¿å½’æ¡£åçš„çŠ¶æ€æŒä¹…åŒ–ï¼‰
        self._save_user(user_id, character_id)
    
    def add(self, content: str, context_type: ContextType, user_id: str = "default",
            character_id: str = "default",
            source_turn: str = None, keywords: List[str] = None,
            related_entities: List[str] = None) -> PersistentContext:
        """æ·»åŠ æŒä¹…ä¸Šä¸‹æ–‡
        
        æ™ºèƒ½å¤„ç†ï¼ˆä¸‰çº§å»é‡ç­–ç•¥ï¼‰ï¼š
        1. è®¡ç®—æ–°å†…å®¹çš„Embeddingï¼ˆå¦‚æœå¯ç”¨ï¼‰
        2. æŸ¥æ‰¾è¯­ä¹‰ç›¸ä¼¼çš„å·²æœ‰æ¡ä»¶
        3. é«˜ç›¸ä¼¼åº¦ï¼šè‡ªåŠ¨åˆå¹¶ï¼ˆå¢åŠ ç½®ä¿¡åº¦ï¼‰
        4. ä¸­ç­‰ç›¸ä¼¼åº¦ï¼šè°¨æ…åˆå¹¶ï¼ˆå¦‚æœå†…å®¹æ›´è¯¦ç»†åˆ™æ›´æ–°ï¼‰
        5. ä½ç›¸ä¼¼åº¦æˆ–æ— åŒ¹é…ï¼šåˆ›å»ºæ–°æ¡ä»¶
        6. è¶…å‡ºæ•°é‡é™åˆ¶æ—¶ï¼Œæ·˜æ±°ç½®ä¿¡åº¦æœ€ä½çš„
        
        æ³¨æ„ï¼šå¦‚æœè¯¥ç±»å‹å·²è¾¾åˆ°ä¸Šé™ä¸”æ–°æ¡ä»¶ç½®ä¿¡åº¦ä¸å¤Ÿé«˜ï¼Œ
        æ–°æ¡ä»¶å¯èƒ½ä¼šè¢«ç«‹å³æ·˜æ±°ï¼ˆä½†ä»ä¼šè¿”å›è¯¥å¯¹è±¡ï¼Œis_active=Falseï¼‰
        """
        import uuid
        
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            self.contexts[cache_key] = []
        
        # 1. é¢„è®¡ç®—æ–°å†…å®¹çš„Embeddingï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        new_embedding = self._get_embedding(content)
        
        # 2. æ£€æŸ¥æ˜¯å¦æœ‰è¯­ä¹‰ç›¸ä¼¼çš„å·²æœ‰æ¡ä»¶
        similar, sim_score, sim_method = self._find_similar(
            content, user_id, character_id, context_type, new_embedding=new_embedding
        )
        
        if similar:
            # å‘ç°ç›¸ä¼¼æ¡ä»¶ï¼Œè¿›è¡Œåˆå¹¶è€Œä¸æ˜¯åˆ›å»ºæ–°æ¡ä»¶
            content_preview = content[:35].replace('\n', ' ')
            similar_preview = similar.content[:35].replace('\n', ' ')
            print(f"[ContextTracker] ğŸ”„ å»é‡åˆå¹¶:")
            print(f"[ContextTracker]    æ–°: {content_preview}...")
            print(f"[ContextTracker]    æ—§: {similar_preview}...")
            print(f"[ContextTracker]    æ–¹æ³•={sim_method}, ç›¸ä¼¼åº¦={sim_score:.3f}")
            
            # åˆå¹¶ç­–ç•¥
            if sim_method == "exact":
                # å®Œå…¨ç›¸åŒï¼Œåªæ›´æ–°ä½¿ç”¨ä¿¡æ¯
                similar.use_count += 1
                similar.last_used = time.time()
                print(f"[ContextTracker]    âœ… å®Œå…¨ç›¸åŒï¼Œæ›´æ–°ä½¿ç”¨è®¡æ•°: {similar.use_count}")
            elif sim_method.endswith("_uncertain"):
                # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼Œè°¨æ…åˆå¹¶
                similar.confidence = min(1.0, similar.confidence + 0.05)  # è¾ƒå°å¢é‡
                similar.use_count += 1
                similar.last_used = time.time()
                # å¦‚æœæ–°å†…å®¹æ˜æ˜¾æ›´é•¿/æ›´è¯¦ç»†ï¼Œç”¨æ–°å†…å®¹æ›¿æ¢
                if len(content) > len(similar.content) * 1.2:
                    similar.content = content
                    # æ›´æ–°Embedding
                    if new_embedding:
                        similar.embedding = new_embedding
            else:
                # é«˜åº¦ç›¸ä¼¼ï¼Œæ­£å¸¸åˆå¹¶
                similar.confidence = min(1.0, similar.confidence + 0.1)
                similar.use_count += 1
                similar.last_used = time.time()
                # å¦‚æœæ–°å†…å®¹æ›´é•¿/æ›´è¯¦ç»†ï¼Œç”¨æ–°å†…å®¹æ›¿æ¢
                if len(content) > len(similar.content):
                    similar.content = content
                    # æ›´æ–°Embedding
                    if new_embedding:
                        similar.embedding = new_embedding
            
            self._save_user(user_id, character_id)
            return similar
        
        # 3. åˆ›å»ºæ–°æ¡ä»¶
        ctx = PersistentContext(
            id=f"ctx_{uuid.uuid4().hex[:12]}",
            content=content,
            context_type=context_type,
            user_id=user_id,
            source_turn=source_turn,
            keywords=keywords or [],
            related_entities=related_entities or []
        )
        
        # å­˜å‚¨Embeddingï¼ˆå¦‚æœæœ‰ï¼‰
        if new_embedding:
            ctx.embedding = new_embedding
        
        self.contexts[cache_key].append(ctx)
        
        # 4. å¼ºåˆ¶æ‰§è¡Œæ•°é‡é™åˆ¶
        self._enforce_limits(user_id, character_id)
        
        # æ£€æŸ¥æ–°æ¡ä»¶æ˜¯å¦è¢«æ·˜æ±°äº†
        if ctx not in self.contexts[cache_key]:
            # æ¡ä»¶è¢«æ·˜æ±°äº†ï¼ˆå› ä¸ºç½®ä¿¡åº¦ä¸å¤Ÿé«˜ï¼‰ï¼Œæ ‡è®°ä¸ºä¸æ´»è·ƒ
            ctx.is_active = False
            print(f"[ContextTracker] æ–°æ¡ä»¶å› æ•°é‡é™åˆ¶è¢«æ·˜æ±°: {content[:50]}...")
        else:
            print(f"[ContextTracker] åˆ›å»ºæ–°æ¡ä»¶: type={context_type.value}, content={content[:50]}...")
        
        self._save_user(user_id, character_id)
        return ctx
    
    def get_active(self, user_id: str = "default", character_id: str = "default") -> List[PersistentContext]:
        """è·å–æ‰€æœ‰æ´»è·ƒçš„æŒä¹…ä¸Šä¸‹æ–‡"""
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return []
        
        now = time.time()
        active = []
        
        for ctx in self.contexts[cache_key]:
            if not ctx.is_active:
                continue
            if ctx.expires_at and ctx.expires_at < now:
                ctx.is_active = False
                continue
            active.append(ctx)
        
        # æŒ‰ç±»å‹å’Œç½®ä¿¡åº¦æ’åº
        active.sort(key=lambda c: (-c.confidence, c.context_type.value))
        return active
    
    def get_by_type(self, context_type: ContextType, user_id: str = "default",
                    character_id: str = "default") -> List[PersistentContext]:
        """æŒ‰ç±»å‹è·å–ä¸Šä¸‹æ–‡"""
        return [c for c in self.get_active(user_id, character_id) if c.context_type == context_type]
    
    def deactivate(self, context_id: str, user_id: str = "default",
                   character_id: str = "default") -> bool:
        """åœç”¨æŸä¸ªä¸Šä¸‹æ–‡"""
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return False
        
        for ctx in self.contexts[cache_key]:
            if ctx.id == context_id:
                ctx.is_active = False
                self._save_user(user_id, character_id)
                return True
        return False
    
    def mark_used(self, context_id: str, user_id: str = "default", character_id: str = "default",
                  save: bool = True):
        """æ ‡è®°ä¸Šä¸‹æ–‡è¢«ä½¿ç”¨
        
        Args:
            context_id: æ¡ä»¶ID
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            save: æ˜¯å¦ç«‹å³ä¿å­˜ï¼ˆæ‰¹é‡æ“ä½œæ—¶è®¾ä¸ºFalseï¼Œæœ€åç»Ÿä¸€ä¿å­˜ï¼‰
        """
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return
        
        for ctx in self.contexts[cache_key]:
            if ctx.id == context_id:
                ctx.last_used = time.time()
                ctx.use_count += 1
                if save:
                    self._save_user(user_id, character_id)
                return
    
    def mark_used_batch(self, context_ids: List[str], user_id: str = "default",
                        character_id: str = "default"):
        """æ‰¹é‡æ ‡è®°ä¸Šä¸‹æ–‡è¢«ä½¿ç”¨ï¼ˆåªä¿å­˜ä¸€æ¬¡ï¼‰"""
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts or not context_ids:
            return
        
        now = time.time()
        id_set = set(context_ids)  # è½¬æˆ set æé«˜æŸ¥æ‰¾æ•ˆç‡
        for ctx in self.contexts[cache_key]:
            if ctx.id in id_set:
                ctx.last_used = now
                ctx.use_count += 1
        
        self._save_user(user_id, character_id)
    
    def extract_from_text(self, text: str, user_id: str = "default",
                          character_id: str = "default") -> List[PersistentContext]:
        """ä»æ–‡æœ¬ä¸­è‡ªåŠ¨æå–æŒä¹…ä¸Šä¸‹æ–‡
        
        ä¼˜å…ˆä½¿ç”¨ LLMï¼Œå¦‚æœæ²¡æœ‰ LLM åˆ™ä½¿ç”¨è§„åˆ™
        """
        text_preview = text[:60].replace('\n', ' ') if len(text) > 60 else text.replace('\n', ' ')
        print(f"[ContextTracker] ğŸ” å¼€å§‹æå–: user={user_id}, char={character_id}")
        print(f"[ContextTracker]    æ–‡æœ¬({len(text)}å­—): {text_preview}{'...' if len(text) > 60 else ''}")
        print(f"[ContextTracker]    æ¨¡å¼: {'LLM' if self.llm_client else 'è§„åˆ™'}")
        
        if self.llm_client:
            result = self._extract_with_llm(text, user_id, character_id)
        else:
            result = self._extract_with_rules(text, user_id, character_id)
        
        if result:
            print(f"[ContextTracker] âœ… æå–å®Œæˆ: æ–°å¢ {len(result)} æ¡æ¡ä»¶")
            for ctx in result:
                print(f"[ContextTracker]    ğŸŒ± [{ctx.context_type.value}] {ctx.content[:50]}{'...' if len(ctx.content) > 50 else ''}")
        else:
            print(f"[ContextTracker] â­ï¸ æå–å®Œæˆ: æœªå‘ç°æ–°æ¡ä»¶")
        
        return result
    
    def _extract_with_rules(self, text: str, user_id: str,
                            character_id: str = "default") -> List[PersistentContext]:
        """ä½¿ç”¨è§„åˆ™æå–"""
        extracted = []
        seen_contents = set()  # é¿å…é‡å¤
        
        for context_type, patterns in self.detection_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    # å°è¯•è·å–æ•è·ç»„çš„å†…å®¹ï¼Œå¦åˆ™ç”¨æ•´ä¸ªåŒ¹é…
                    content = match.group(0).strip()
                    
                    # æ¸…ç†å†…å®¹ï¼šå»é™¤é¦–å°¾æ ‡ç‚¹
                    content = content.strip('ï¼Œã€‚,. ')
                    
                    # è·³è¿‡å¤ªçŸ­æˆ–é‡å¤çš„å†…å®¹
                    if len(content) < self.min_content_length:
                        continue
                    
                    content_key = content.lower()
                    if content_key in seen_contents:
                        continue
                    seen_contents.add(content_key)
                    
                    # å¯¹äºç¯å¢ƒç±»å‹ï¼Œç‰¹æ®Šå¤„ç†ï¼šå®Œæ•´è®°å½•å¼€å‘å’Œéƒ¨ç½²ç¯å¢ƒ
                    if context_type == ContextType.ENVIRONMENT:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯"Windowså¼€å‘Ubuntuéƒ¨ç½²"è¿™ç§æ¨¡å¼
                        env_match = re.search(
                            r'(Windows|Linux|Mac|Ubuntu)(?:ä¸Š)?å¼€å‘.{0,10}?(Windows|Linux|Mac|Ubuntu)(?:ä¸Š)?éƒ¨ç½²',
                            text, re.IGNORECASE
                        )
                        if env_match:
                            content = f"å¼€å‘ç¯å¢ƒ: {env_match.group(1)}, éƒ¨ç½²ç¯å¢ƒ: {env_match.group(2)}"
                    
                    ctx = self.add(
                        content=content,
                        context_type=context_type,
                        user_id=user_id,
                        character_id=character_id
                    )
                    extracted.append(ctx)
        
        return extracted
    
    def _extract_with_llm(self, text: str, user_id: str,
                          character_id: str = "default") -> List[PersistentContext]:
        """ä½¿ç”¨ LLM æå–"""
        try:
            print(f"[ContextTracker] ğŸ¤– è°ƒç”¨ LLM æå–æ¡ä»¶...")
            prompt = self.extraction_prompt.format(content=text)
            response = self.llm_client.complete(prompt, max_tokens=500)
            print(f"[ContextTracker]    LLM å“åº”: {len(response)} å­—ç¬¦")
            
            # è§£æ JSON
            import json
            # å°è¯•æ‰¾åˆ° JSON æ•°ç»„
            json_match = re.search(r'\[[\s\S]*\]', response)
            if json_match:
                items = json.loads(json_match.group(0))
                print(f"[ContextTracker]    è§£æåˆ° {len(items)} æ¡å€™é€‰æ¡ä»¶")
                
                extracted = []
                for item in items:
                    try:
                        ctx = self.add(
                            content=item['content'],
                            context_type=ContextType(item['type']),
                            user_id=user_id,
                            character_id=character_id,
                            keywords=item.get('keywords', [])
                        )
                        extracted.append(ctx)
                    except (KeyError, ValueError) as e:
                        print(f"[ContextTracker]    âš ï¸ è·³è¿‡æ— æ•ˆæ¡ä»¶: {e}")
                        continue
                
                return extracted
            else:
                print(f"[ContextTracker]    âš ï¸ LLM å“åº”ä¸­æœªæ‰¾åˆ° JSON æ•°ç»„")
        except Exception as e:
            print(f"[ContextTracker] âŒ LLMæå–å¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™: {e}")
        
        # å›é€€åˆ°è§„åˆ™æå–
        return self._extract_with_rules(text, user_id, character_id)
    
    def format_for_prompt(self, user_id: str = "default", character_id: str = "default") -> str:
        """æ ¼å¼åŒ–ä¸ºæç¤ºè¯æ³¨å…¥
        
        æ³¨æ„ï¼šæœ¬æ–¹æ³•ä¸å†è‡ªåŠ¨è°ƒç”¨ _enforce_limits()ï¼Œé¿å…é‡å¤è¡°å‡æ£€æŸ¥ã€‚
        è°ƒç”¨æ–¹ï¼ˆå¦‚ engine.build_contextï¼‰åº”è´Ÿè´£åœ¨é€‚å½“æ—¶æœºè°ƒç”¨è¡°å‡æ£€æŸ¥ã€‚
        """
        active = self.get_active(user_id, character_id)
        if not active:
            return ""
        
        lines = ["<persistent_context>", "ã€æŒä¹…å‰ææ¡ä»¶ã€‘ä»¥ä¸‹æ˜¯å·²ç¡®ç«‹çš„èƒŒæ™¯è®¾å®šï¼Œè¯·åœ¨æ‰€æœ‰å›å¤ä¸­é»˜è®¤éµå®ˆï¼š"]
        
        # æŒ‰ç±»å‹åˆ†ç»„
        by_type: Dict[ContextType, List[PersistentContext]] = {}
        for ctx in active:
            if ctx.context_type not in by_type:
                by_type[ctx.context_type] = []
            by_type[ctx.context_type].append(ctx)
        
        type_names = {
            ContextType.USER_IDENTITY: "ç”¨æˆ·èº«ä»½",
            ContextType.USER_GOAL: "ç”¨æˆ·ç›®æ ‡",
            ContextType.USER_PREFERENCE: "ç”¨æˆ·åå¥½",
            ContextType.ENVIRONMENT: "æŠ€æœ¯ç¯å¢ƒ",
            ContextType.PROJECT: "é¡¹ç›®ä¿¡æ¯",
            ContextType.TIME_CONSTRAINT: "æ—¶é—´çº¦æŸ",
            ContextType.CHARACTER_TRAIT: "è§’è‰²ç‰¹å¾",
            ContextType.WORLD_SETTING: "ä¸–ç•Œè§‚",
            ContextType.RELATIONSHIP: "å…³ç³»è®¾å®š",
            ContextType.EMOTIONAL_STATE: "æƒ…ç»ªçŠ¶æ€",
            ContextType.SKILL_ABILITY: "æŠ€èƒ½èƒ½åŠ›",
            ContextType.ITEM_PROP: "ç‰©å“é“å…·",
            ContextType.ASSUMPTION: "å‡è®¾å‰æ",
            ContextType.CONSTRAINT: "çº¦æŸæ¡ä»¶",
            ContextType.CUSTOM: "å…¶ä»–",
        }
        
        for ctx_type, contexts in by_type.items():
            type_name = type_names.get(ctx_type, ctx_type.value)
            lines.append(f"\n[{type_name}]")
            for ctx in contexts:
                lines.append(f"â€¢ {ctx.content}")
        
        lines.append("</persistent_context>")
        return "\n".join(lines)
    
    def get_relevant(self, query: str, user_id: str = "default", character_id: str = "default",
                     top_k: int = 5) -> List[PersistentContext]:
        """è·å–ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡"""
        active = self.get_active(user_id, character_id)
        if not active:
            return []
        
        query_lower = query.lower()
        scored = []
        
        for ctx in active:
            score = 0
            
            # å…³é”®è¯åŒ¹é…
            for kw in ctx.keywords:
                if kw.lower() in query_lower:
                    score += 0.3
            
            # å†…å®¹åŒ¹é…
            if any(word in ctx.content.lower() for word in query_lower.split()):
                score += 0.2
            
            # å®ä½“åŒ¹é…
            for entity in ctx.related_entities:
                if entity.lower() in query_lower:
                    score += 0.4
            
            # åŸºç¡€åˆ†ï¼ˆæ‰€æœ‰æ´»è·ƒæ¡ä»¶éƒ½æœ‰ä¸€å®šç›¸å…³æ€§ï¼‰
            score += 0.1
            
            scored.append((ctx, score))
        
        # æ’åºå¹¶è¿”å›
        scored.sort(key=lambda x: -x[1])
        return [ctx for ctx, score in scored[:top_k]]
    
    def consolidate_contexts(self, user_id: str = "default", character_id: str = "default",
                             force: bool = False) -> int:
        """æ™ºèƒ½å‹ç¼©åˆå¹¶ç›¸ä¼¼çš„æŒä¹…æ¡ä»¶
        
        å½“æ¡ä»¶æ•°é‡è¶…è¿‡é˜ˆå€¼æˆ–å¼ºåˆ¶æ‰§è¡Œæ—¶ï¼Œä½¿ç”¨LLMå°†ç›¸ä¼¼æ¡ä»¶åˆå¹¶
        
        Returns:
            å‡å°‘çš„æ¡ä»¶æ•°é‡
        """
        cache_key = self._get_cache_key(user_id, character_id)
        
        if cache_key not in self.contexts:
            return 0
        
        contexts = [c for c in self.contexts[cache_key] if c.is_active]
        original_count = len(contexts)
        
        # åªæœ‰æ¡ä»¶æ•°é‡è¶…è¿‡é˜ˆå€¼æˆ–å¼ºåˆ¶æ‰§è¡Œæ—¶æ‰å‹ç¼©
        if not force and original_count < self.MAX_TOTAL * 0.8:
            return 0
        
        # æŒ‰ç±»å‹åˆ†ç»„
        by_type: Dict[ContextType, List[PersistentContext]] = {}
        for ctx in contexts:
            if ctx.context_type not in by_type:
                by_type[ctx.context_type] = []
            by_type[ctx.context_type].append(ctx)
        
        # å¯¹æ¯ä¸ªç±»å‹è¿›è¡Œå‹ç¼©
        for ctx_type, type_contexts in by_type.items():
            if len(type_contexts) <= 2:  # 2ä¸ªä»¥ä¸‹ä¸å‹ç¼©
                continue
            
            if self.llm_client:
                # ä½¿ç”¨ LLM å‹ç¼©
                self._consolidate_with_llm(type_contexts, ctx_type, user_id, character_id)
            else:
                # ç®€å•å‹ç¼©ï¼šä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„å‡ ä¸ª
                type_contexts.sort(key=lambda c: -c.confidence)
                for ctx in type_contexts[self.MAX_PER_TYPE:]:
                    ctx.is_active = False
        
        self._save_user(user_id, character_id)
        
        # è®¡ç®—å‡å°‘çš„æ•°é‡
        new_count = len([c for c in self.contexts[cache_key] if c.is_active])
        return original_count - new_count
    
    def _consolidate_with_llm(self, contexts: List[PersistentContext], 
                              ctx_type: ContextType, user_id: str,
                              character_id: str = "default"):
        """ä½¿ç”¨ LLM å‹ç¼©åˆå¹¶åŒç±»å‹çš„æ¡ä»¶"""
        try:
            # æ„å»ºæ¡ä»¶åˆ—è¡¨
            conditions = "\n".join([f"- {ctx.content}" for ctx in contexts])
            
            prompt = f"""ä½ éœ€è¦å°†ä»¥ä¸‹åŒç±»å‹çš„æ¡ä»¶åˆå¹¶å‹ç¼©ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼Œå»é™¤å†—ä½™ã€‚

æ¡ä»¶ç±»å‹: {ctx_type.value}
å½“å‰æ¡ä»¶ï¼š
{conditions}

è¯·å°†è¿™äº›æ¡ä»¶åˆå¹¶æˆä¸è¶…è¿‡{self.MAX_PER_TYPE}æ¡æ ¸å¿ƒæ¡ä»¶ã€‚
è¾“å‡ºæ ¼å¼ï¼ˆJSONæ•°ç»„ï¼‰:
[
  "åˆå¹¶åçš„æ¡ä»¶1",
  "åˆå¹¶åçš„æ¡ä»¶2"
]

åªè¾“å‡ºJSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
            
            response = self.llm_client.complete(prompt, max_tokens=300)
            
            # è§£æç»“æœ
            json_match = re.search(r'\[[\s\S]*\]', response)
            if json_match:
                merged = json.loads(json_match.group(0))
                
                # è¿‡æ»¤æœ‰æ•ˆå†…å®¹
                valid_contents = [c.strip() for c in merged[:self.MAX_PER_TYPE] if c and c.strip()]
                
                # å¦‚æœ LLM è¿”å›ç©ºç»“æœï¼Œä½¿ç”¨å›é€€ç­–ç•¥
                if not valid_contents:
                    raise ValueError("LLM è¿”å›ç©ºç»“æœ")
                
                # åœç”¨æ—§æ¡ä»¶ï¼ˆåªåœ¨ç¡®è®¤æœ‰æ–°æ¡ä»¶åå†åœç”¨ï¼‰
                for ctx in contexts:
                    ctx.is_active = False
                
                # æ·»åŠ åˆå¹¶åçš„æ¡ä»¶
                for content in valid_contents:
                    self.add(
                        content=content,
                        context_type=ctx_type,
                        user_id=user_id,
                        character_id=character_id
                    )
            else:
                raise ValueError("LLM å“åº”æ ¼å¼æ— æ•ˆ")
                        
        except Exception as e:
            print(f"[ContextTracker] LLMå‹ç¼©å¤±è´¥: {e}")
            # å›é€€ï¼šç®€å•ä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„
            contexts.sort(key=lambda c: -c.confidence)
            for ctx in contexts[self.MAX_PER_TYPE:]:
                ctx.is_active = False
    
    # get_by_id æ˜¯ get_context_by_id çš„åˆ«åï¼Œä¿æŒAPIä¸€è‡´æ€§
    def get_by_id(self, context_id: str, user_id: str = "default", 
                  character_id: str = "default") -> Optional[PersistentContext]:
        """get_context_by_id çš„åˆ«åï¼Œä¿æŒä¸ ForeshadowingTracker çš„ API ä¸€è‡´"""
        return self.get_context_by_id(context_id, user_id, character_id)
    
    def get_stats(self, user_id: str = "default", character_id: str = "default") -> Dict[str, Any]:
        """è·å–æŒä¹…æ¡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯"""
        cache_key = self._get_cache_key(user_id, character_id)
        
        if cache_key not in self.contexts:
            return {
                "total": 0,
                "active": 0,
                "by_type": {},
                "avg_confidence": 0.0,
                "oldest_days": 0
            }
        
        contexts = self.contexts[cache_key]
        active = [c for c in contexts if c.is_active]
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        by_type = {}
        for ctx in active:
            type_name = ctx.context_type.value
            if type_name not in by_type:
                by_type[type_name] = 0
            by_type[type_name] += 1
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_conf = sum(c.confidence for c in active) / len(active) if active else 0.0
        
        # è®¡ç®—æœ€è€çš„æ¡ä»¶
        now = time.time()
        oldest_days = 0
        if active:
            oldest = min(c.created_at for c in active)
            oldest_days = (now - oldest) / (24 * 3600)
        
        return {
            "total": len(contexts),
            "active": len(active),
            "by_type": by_type,
            "avg_confidence": round(avg_conf, 2),
            "oldest_days": round(oldest_days, 1),
            "limits": {
                "max_per_type": self.MAX_PER_TYPE,
                "max_total": self.MAX_TOTAL
            }
        }
"""æŒä¹…ä¸Šä¸‹æ–‡è¿½è¸ªå™¨ - è¿½è¸ªå’Œç®¡ç†æŒä¹…æ€§å‰ææ¡ä»¶

è¿™ä¸ªæ¨¡å—è§£å†³çš„é—®é¢˜ï¼š
- æŸäº›ä¿¡æ¯ä¸€æ—¦ç¡®ç«‹ï¼Œå°±åº”è¯¥æˆä¸ºåç»­æ‰€æœ‰å¯¹è¯çš„"é»˜è®¤å‰ææ¡ä»¶"
- æ¯”å¦‚"ç”¨æˆ·æ˜¯å¤§å­¦æ¯•ä¸šç”Ÿæƒ³åˆ›ä¸š" â€”â€” åç»­æ‰€æœ‰å»ºè®®éƒ½åŸºäºè¿™ä¸ªå‰æ
- æ¯”å¦‚"Windowså¼€å‘ï¼ŒUbuntuéƒ¨ç½²" â€”â€” åç»­æ‰€æœ‰ä»£ç å»ºè®®éƒ½åŸºäºè¿™ä¸ªç¯å¢ƒ

æ ¸å¿ƒæ¦‚å¿µï¼š
- PersistentContext: æŒä¹…æ€§ä¸Šä¸‹æ–‡/æ¡ä»¶
- ä¸åŒäºä¼ç¬”ï¼ˆæœªè§£å†³çš„çº¿ç´¢ï¼‰ï¼Œè¿™æ˜¯å·²ç¡®ç«‹çš„èƒŒæ™¯è®¾å®š
- ä¸åŒäºæ™®é€šè®°å¿†ï¼ˆéœ€è¦æ£€ç´¢ï¼‰ï¼Œè¿™æ˜¯æ¯æ¬¡å¯¹è¯éƒ½åº”è¯¥è‡ªåŠ¨åŒ…å«çš„å‰æ

æ™ºèƒ½å»é‡æœºåˆ¶ï¼ˆv3.0.1 æ–°å¢ï¼‰ï¼š
- ç¬¬ä¸€çº§ï¼šEmbedding å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦å¿«é€Ÿç­›é€‰
- ç¬¬äºŒçº§ï¼šLLM æå–æ—¶é¡ºå¸¦åˆ¤æ–­æ˜¯å¦ä¸å·²æœ‰æ¡ä»¶é‡å¤
- ç¬¬ä¸‰çº§ï¼šå®šæœŸæ‰¹é‡æ•´ç†åˆå¹¶
"""

import os
import json
import time
import re
import numpy as np
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Any, Optional, Tuple
from enum import Enum

# Windows GBK ç¼–ç å…¼å®¹çš„å®‰å…¨æ‰“å°å‡½æ•°
def _safe_print(msg: str) -> None:
    """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œæ›¿æ¢ emoji ä¸º ASCII ç­‰ä»·ç‰©ä»¥é¿å… Windows GBK ç¼–ç é”™è¯¯"""
    emoji_map = {
        'ğŸ“¥': '[IN]', 'ğŸ“¤': '[OUT]', 'ğŸ”': '[SEARCH]', 'âœ…': '[OK]', 'âŒ': '[FAIL]',
        'âš ï¸': '[WARN]', 'ğŸ’¾': '[SAVE]', 'ğŸ—ƒï¸': '[DB]', 'ğŸ§¹': '[CLEAN]', 'ğŸ“Š': '[STATS]',
        'ğŸ”„': '[SYNC]', 'ğŸ“¦': '[PKG]', 'ğŸš€': '[START]', 'ğŸ¯': '[TARGET]', 'ğŸ’¡': '[HINT]',
        'ğŸ”§': '[FIX]', 'ğŸ“': '[NOTE]', 'ğŸ‰': '[DONE]', 'â±ï¸': '[TIME]', 'ğŸŒ': '[NET]',
        'ğŸ§ ': '[BRAIN]', 'ğŸ’¬': '[CHAT]', 'ğŸ·ï¸': '[TAG]', 'ğŸ“': '[DIR]', 'ğŸ”’': '[LOCK]',
        'ğŸŒ±': '[PLANT]', 'ğŸ—‘ï¸': '[DEL]', 'ğŸ’«': '[MAGIC]', 'ğŸ­': '[MASK]', 'ğŸ“–': '[BOOK]',
        'âš¡': '[FAST]', 'ğŸ”¥': '[HOT]', 'ğŸ’': '[GEM]', 'ğŸŒŸ': '[STAR]', 'ğŸ¨': '[ART]'
    }
    for emoji, ascii_equiv in emoji_map.items():
        msg = msg.replace(emoji, ascii_equiv)
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode('ascii', errors='replace').decode('ascii'))




class ContextType(Enum):
    """ä¸Šä¸‹æ–‡ç±»å‹"""
    # ç”¨æˆ·èº«ä»½ç›¸å…³
    USER_IDENTITY = "user_identity"      # ç”¨æˆ·èº«ä»½ï¼ˆå¦‚ï¼šå¤§å­¦æ¯•ä¸šç”Ÿã€ç¨‹åºå‘˜ï¼‰
    USER_GOAL = "user_goal"              # ç”¨æˆ·ç›®æ ‡ï¼ˆå¦‚ï¼šæƒ³åˆ›ä¸šã€å­¦ä¹ Pythonï¼‰
    USER_PREFERENCE = "user_preference"  # ç”¨æˆ·åå¥½ï¼ˆå¦‚ï¼šå–œæ¬¢ç®€æ´çš„è§£é‡Šï¼‰
    
    # ç¯å¢ƒç›¸å…³
    ENVIRONMENT = "environment"          # æŠ€æœ¯ç¯å¢ƒï¼ˆå¦‚ï¼šWindowså¼€å‘ã€Ubuntuéƒ¨ç½²ï¼‰
    PROJECT = "project"                  # é¡¹ç›®ä¿¡æ¯ï¼ˆå¦‚ï¼šæ­£åœ¨å¼€å‘Recall-aiï¼‰
    TIME_CONSTRAINT = "time_constraint"  # æ—¶é—´çº¦æŸï¼ˆå¦‚ï¼šæˆªæ­¢æ—¥æœŸã€ç‰¹å®šæ—¶é—´æ®µï¼‰
    
    # è§’è‰²æ‰®æ¼”ç›¸å…³
    CHARACTER_TRAIT = "character_trait"  # è§’è‰²ç‰¹å¾ï¼ˆå¦‚ï¼šè§’è‰²æ€§æ ¼è®¾å®šï¼‰
    WORLD_SETTING = "world_setting"      # ä¸–ç•Œè§‚è®¾å®šï¼ˆå¦‚ï¼šé­”æ³•ä¸–ç•Œï¼‰
    RELATIONSHIP = "relationship"        # å…³ç³»è®¾å®šï¼ˆå¦‚ï¼šç”¨æˆ·æ˜¯è§’è‰²çš„æœ‹å‹ï¼‰
    EMOTIONAL_STATE = "emotional_state"  # æƒ…ç»ªçŠ¶æ€ï¼ˆå¦‚ï¼šè§’è‰²å½“å‰å¿ƒæƒ…ï¼‰
    SKILL_ABILITY = "skill_ability"      # æŠ€èƒ½èƒ½åŠ›ï¼ˆå¦‚ï¼šè§’è‰²ä¼šä»€ä¹ˆæŠ€èƒ½ï¼‰
    ITEM_PROP = "item_prop"              # ç‰©å“é“å…·ï¼ˆå¦‚ï¼šè§’è‰²æºå¸¦çš„ç‰©å“ï¼‰
    
    # é€šç”¨
    ASSUMPTION = "assumption"            # å‡è®¾å‰æ
    CONSTRAINT = "constraint"            # çº¦æŸæ¡ä»¶
    CUSTOM = "custom"                    # è‡ªå®šä¹‰


@dataclass
class PersistentContext:
    """æŒä¹…æ€§ä¸Šä¸‹æ–‡/æ¡ä»¶"""
    id: str
    content: str                         # æ¡ä»¶å†…å®¹
    context_type: ContextType            # æ¡ä»¶ç±»å‹
    user_id: str = "default"
    
    # å…ƒä¿¡æ¯
    created_at: float = field(default_factory=time.time)
    source_turn: Optional[str] = None    # æ¥æºè®°å¿†ID
    confidence: float = 0.8              # ç½®ä¿¡åº¦
    
    # ç”Ÿå‘½å‘¨æœŸ
    is_active: bool = True               # æ˜¯å¦æ´»è·ƒ
    expires_at: Optional[float] = None   # è¿‡æœŸæ—¶é—´ï¼ˆNone=æ°¸ä¸è¿‡æœŸï¼‰
    last_used: float = field(default_factory=time.time)
    use_count: int = 0
    
    # å…³è”
    related_entities: List[str] = field(default_factory=list)
    keywords: List[str] = field(default_factory=list)
    
    # æ™ºèƒ½å»é‡ - Embedding å‘é‡ç¼“å­˜ï¼ˆå¯é€‰å­—æ®µï¼Œå‘åå…¼å®¹ï¼‰
    # å­˜å‚¨ä¸º list ä»¥ä¾¿ JSON åºåˆ—åŒ–ï¼Œä½¿ç”¨æ—¶è½¬æ¢ä¸º numpy array
    _embedding: Optional[List[float]] = field(default=None, repr=False)
    
    @property
    def embedding(self) -> Optional[np.ndarray]:
        """è·å– embedding å‘é‡ï¼ˆnumpy arrayï¼‰"""
        if self._embedding is not None:
            return np.array(self._embedding, dtype='float32')
        return None
    
    @embedding.setter
    def embedding(self, value):
        """è®¾ç½® embedding å‘é‡"""
        if value is not None:
            if isinstance(value, np.ndarray):
                self._embedding = value.tolist()
            else:
                self._embedding = list(value)
        else:
            self._embedding = None
    
    def to_dict(self) -> Dict[str, Any]:
        d = asdict(self)
        d['context_type'] = self.context_type.value
        # _embedding å­—æ®µåœ¨ asdict ä¸­ä¼šè‡ªåŠ¨åŒ…å«
        return d
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'PersistentContext':
        data = data.copy()  # é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        data['context_type'] = ContextType(data['context_type'])
        # å…¼å®¹æ—§æ•°æ®ï¼ˆæ²¡æœ‰ _embedding å­—æ®µï¼‰
        if '_embedding' not in data:
            data['_embedding'] = None
        # ç§»é™¤å½’æ¡£æ—¶æ·»åŠ çš„é¢å¤–å­—æ®µï¼ˆè¿™äº›å­—æ®µä¸æ˜¯ PersistentContext çš„å±æ€§ï¼‰
        data.pop('archived_at', None)
        data.pop('archive_reason', None)
        return cls(**data)


class ContextTracker:
    """æŒä¹…ä¸Šä¸‹æ–‡è¿½è¸ªå™¨
    
    å¢é•¿æ§åˆ¶ç­–ç•¥ï¼š
    1. æ¯ä¸ªç±»å‹æœ€å¤šä¿ç•™ max_per_type ä¸ªæ¡ä»¶ï¼ˆé»˜è®¤10ä¸ªï¼‰
    2. ç›¸ä¼¼å†…å®¹è‡ªåŠ¨åˆå¹¶ï¼ˆå¢åŠ ç½®ä¿¡åº¦è€Œéæ–°å¢ï¼‰- ä½¿ç”¨ Embedding è¯­ä¹‰ç›¸ä¼¼åº¦
    3. ç½®ä¿¡åº¦è¡°å‡ï¼šé•¿æœŸæœªè¢«å¼•ç”¨çš„æ¡ä»¶ç½®ä¿¡åº¦ä¸‹é™
    4. è¶…å‡ºæ•°é‡æ—¶ï¼Œæ·˜æ±°ç½®ä¿¡åº¦æœ€ä½çš„
    5. æœ‰LLMæ—¶ï¼Œå¯ä»¥æ™ºèƒ½å‹ç¼©å¤šä¸ªæ¡ä»¶
    6. ä½ç½®ä¿¡åº¦æ¡ä»¶è‡ªåŠ¨å½’æ¡£åˆ° archive/contexts.jsonl
    
    æ™ºèƒ½å»é‡æœºåˆ¶ï¼š
    - ç¬¬ä¸€çº§ï¼šEmbedding å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ>0.85 ç›´æ¥åˆå¹¶ï¼Œ0.70-0.85 å¯èƒ½ç›¸ä¼¼ï¼‰
    - ç¬¬äºŒçº§ï¼šLLM æå–æ—¶åˆ¤æ–­æ˜¯å¦ä¸å·²æœ‰æ¡ä»¶é‡å¤
    - ç¬¬ä¸‰çº§ï¼šå®šæœŸæ‰¹é‡æ•´ç†åˆå¹¶
    """
    
    # é…ç½®å¸¸é‡ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæ”¯æŒçƒ­æ›´æ–°ï¼‰
    @staticmethod
    def _get_limits_config():
        """è·å–é™åˆ¶é…ç½®ï¼ˆæ¯æ¬¡è°ƒç”¨æ—¶è¯»å–ï¼Œæ”¯æŒçƒ­æ›´æ–°ï¼‰"""
        return {
            'max_per_type': int(os.environ.get('CONTEXT_MAX_PER_TYPE', '10')),
            'max_total': int(os.environ.get('CONTEXT_MAX_TOTAL', '100')),
            'decay_days': int(os.environ.get('CONTEXT_DECAY_DAYS', '14')),
            'decay_rate': float(os.environ.get('CONTEXT_DECAY_RATE', '0.05')),
            'min_confidence': float(os.environ.get('CONTEXT_MIN_CONFIDENCE', '0.1')),
        }
    
    # å…¼å®¹æ—§ä»£ç çš„å±æ€§è®¿é—®
    @property
    def MAX_PER_TYPE(self) -> int:
        return self._get_limits_config()['max_per_type']
    
    @property
    def MAX_TOTAL(self) -> int:
        return self._get_limits_config()['max_total']
    
    @property
    def DECAY_DAYS(self) -> int:
        return self._get_limits_config()['decay_days']
    
    @property
    def DECAY_RATE(self) -> float:
        return self._get_limits_config()['decay_rate']
    
    @property
    def MIN_CONFIDENCE(self) -> float:
        return self._get_limits_config()['min_confidence']
    
    @property
    def trigger_interval(self) -> int:
        """è§¦å‘é—´éš”ï¼ˆæ”¯æŒçƒ­æ›´æ–°ï¼‰"""
        return int(os.environ.get('CONTEXT_TRIGGER_INTERVAL', '5'))
    
    @property
    def max_context_turns(self) -> int:
        """æœ€å¤§ä¸Šä¸‹æ–‡è½®æ•°ï¼ˆæ”¯æŒçƒ­æ›´æ–°ï¼‰"""
        return int(os.environ.get('CONTEXT_MAX_CONTEXT_TURNS', '20'))
    
    SIMILARITY_THRESHOLD = 0.6  # è¯é‡å ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
    FLOAT_EPSILON = 1e-9  # æµ®ç‚¹æ•°æ¯”è¾ƒå®¹å·®
    
    # æ™ºèƒ½å»é‡é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæ”¯æŒçƒ­æ›´æ–°ï¼‰
    @staticmethod
    def _get_dedup_config() -> Dict[str, Any]:
        """è·å–å»é‡é…ç½®ï¼ˆæ¯æ¬¡è°ƒç”¨æ—¶è¯»å–ï¼Œæ”¯æŒçƒ­æ›´æ–°ï¼‰
        
        Returns:
            Dict with keys: enabled, high_threshold, low_threshold
        """
        return {
            'enabled': os.environ.get('DEDUP_EMBEDDING_ENABLED', 'true').lower() in ('true', '1', 'yes'),
            'high_threshold': float(os.environ.get('DEDUP_HIGH_THRESHOLD', '0.85')),
            'low_threshold': float(os.environ.get('DEDUP_LOW_THRESHOLD', '0.70'))
        }
    
    def __init__(self, base_path: Optional[str] = None, llm_client: Optional[Any] = None, 
                 embedding_backend: Optional[Any] = None, storage_dir: Optional[str] = None,
                 memory_provider: Optional[Any] = None):
        """åˆå§‹åŒ–æŒä¹…ä¸Šä¸‹æ–‡è¿½è¸ªå™¨
        
        å­˜å‚¨ç»“æ„ï¼š{base_path}/{user_id}/{character_id}/contexts.json
        ä¸ MultiTenantStorage å’Œ ForeshadowingTracker ä¿æŒä¸€è‡´çš„è·¯å¾„ç»“æ„ã€‚
        
        Args:
            base_path: æ•°æ®æ ¹ç›®å½•ï¼ˆæ–°å‚æ•°ï¼Œæ¨èä½¿ç”¨ï¼‰
            llm_client: LLM å®¢æˆ·ç«¯ï¼ˆç”¨äºæ™ºèƒ½æå–å’Œå‹ç¼©ï¼‰
            embedding_backend: Embedding åç«¯ï¼ˆç”¨äºæ™ºèƒ½å»é‡ï¼‰ï¼Œå¦‚æœä¸º None ä¼šå°è¯•è‡ªåŠ¨è·å–
            storage_dir: æ—§å‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰ï¼Œå¦‚æœæä¾›åˆ™è‡ªåŠ¨è¿ç§»åˆ°æ–°ç»“æ„
            memory_provider: è®°å¿†æä¾›å›è°ƒå‡½æ•°ï¼Œæ ¼å¼ä¸º (user_id, limit) -> List[Dict]
        """
        # å‘åå…¼å®¹ï¼šå¦‚æœæä¾›äº† storage_dir è€Œé base_path
        if storage_dir and not base_path:
            # æ—§çš„ storage_dir æ ¼å¼æ˜¯ {data_root}/data/contexts
            # æ–°çš„ base_path æ ¼å¼æ˜¯ {data_root}/data
            if storage_dir.endswith('contexts'):
                base_path = os.path.dirname(storage_dir)
            else:
                base_path = storage_dir
        
        self.base_path = base_path
        self.llm_client = llm_client
        # æŒ‰ {user_id}/{character_id} åˆ†éš”çš„ä¸Šä¸‹æ–‡å­˜å‚¨
        self.contexts: Dict[str, List[PersistentContext]] = {}  # cache_key -> contexts
        
        # Embedding åç«¯ï¼ˆç”¨äºæ™ºèƒ½å»é‡ï¼‰
        self.embedding_backend = embedding_backend
        
        # è®°å¿†æä¾›å›è°ƒï¼ˆç”¨äºè·å–å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
        self._memory_provider = memory_provider
        
        # è§¦å‘æœºåˆ¶ï¼ˆç±»ä¼¼ ForeshadowingAnalyzerï¼‰
        # æ¯ N è½®å¯¹è¯è§¦å‘ä¸€æ¬¡æ¡ä»¶æå–ï¼Œé¿å…é‡å¤åˆ†æç›¸åŒå†…å®¹
        # é…ç½®é¡¹é€šè¿‡ @property æ”¯æŒçƒ­æ›´æ–°ï¼Œä¸å†åœ¨ __init__ ä¸­ç¼“å­˜
        self._turn_counters: Dict[str, int] = {}  # cache_key -> å½“å‰è½®æ¬¡è®¡æ•°
        self._last_analyzed_turn: Dict[str, int] = {}  # cache_key -> ä¸Šæ¬¡åˆ†ææ—¶çš„æ€»è½®æ¬¡
        
        if base_path:
            os.makedirs(base_path, exist_ok=True)
        # æ”¹ä¸ºæŒ‰éœ€åŠ è½½ï¼ˆlazy loadingï¼‰ï¼Œé¿å…ç¼“å­˜é”®ä¸ä¸€è‡´é—®é¢˜
        # self._load_all()  # å·²åºŸå¼ƒï¼Œæ”¹ç”¨ _ensure_loaded()
        
        # æ¡ä»¶æ£€æµ‹æ¨¡å¼ï¼ˆç”¨äºè‡ªåŠ¨ä»å¯¹è¯ä¸­æå–æ¡ä»¶ï¼‰
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å¼ï¼Œé¿å…è¯¯åŒ¹é…
        self.detection_patterns = {
            ContextType.USER_IDENTITY: [
                r'æˆ‘æ˜¯(?:ä¸€ä¸ª|ä¸€å|ä¸€ä½)?([^ï¼Œã€‚,.\n]{2,20}?)(?:ï¼Œ|ã€‚|,|\.|$)',  # æˆ‘æ˜¯XXX
                r'æˆ‘ç›®å‰æ˜¯(?:ä¸€ä¸ª|ä¸€å)?([^ï¼Œã€‚,.\n]{2,20}?)(?:ï¼Œ|ã€‚|,|\.|$)',   # æˆ‘ç›®å‰æ˜¯XXX
            ],
            ContextType.USER_GOAL: [
                r'æˆ‘æƒ³(?:è¦)?(?:å­¦ä¹ |åš|å¼€å‘|åˆ›å»º|å®ç°|å®Œæˆ)([^ï¼Œã€‚,.\n]{2,30}?)(?:ï¼Œ|ã€‚|,|\.|$)',  # æˆ‘æƒ³åšXXX
                r'æˆ‘çš„ç›®æ ‡æ˜¯([^ï¼Œã€‚,.\n]{2,30}?)(?:ï¼Œ|ã€‚|,|\.|$)',  # æˆ‘çš„ç›®æ ‡æ˜¯XXX
                r'æˆ‘(?:æ­£åœ¨|æ‰“ç®—|å‡†å¤‡)(?:å­¦ä¹ |åš|å¼€å‘|åˆ›å»º)([^ï¼Œã€‚,.\n]{2,30}?)(?:ï¼Œ|ã€‚|,|\.|$)',
            ],
            ContextType.ENVIRONMENT: [
                r'(?:æˆ‘)?åœ¨(Windows|Linux|Mac|Ubuntu|CentOS|MacOS)(?:ä¸Š|ç³»ç»Ÿ)?(?:å¼€å‘|éƒ¨ç½²|è¿è¡Œ)',  # åœ¨Windowsä¸Šå¼€å‘
                r'(?:å¼€å‘|éƒ¨ç½²|è¿è¡Œ)(?:ç¯å¢ƒ|å¹³å°)(?:æ˜¯|ç”¨çš„æ˜¯)(Windows|Linux|Mac|Ubuntu)',
                r'(Windows|Linux|Mac|Ubuntu)(?:ä¸Š)?å¼€å‘.{0,10}?(Windows|Linux|Mac|Ubuntu)(?:ä¸Š)?éƒ¨ç½²',  # Windowså¼€å‘Ubuntuéƒ¨ç½²
            ],
            ContextType.PROJECT: [
                r'(?:æˆ‘|æˆ‘ä»¬)(?:æ­£åœ¨|åœ¨)?(?:å¼€å‘|åš|å†™)(?:ä¸€ä¸ª|è¿™ä¸ª)?([^ï¼Œã€‚,.\n]{2,20}?)(?:é¡¹ç›®|ç³»ç»Ÿ|ç¨‹åº|åº”ç”¨)',
                r'è¿™ä¸ªé¡¹ç›®(?:æ˜¯|å«)([^ï¼Œã€‚,.\n]{2,20}?)(?:ï¼Œ|ã€‚|,|\.|$)',
            ],
        }
        
        # æœ€å°åŒ¹é…é•¿åº¦ï¼ˆé¿å…åŒ¹é…å¤ªçŸ­çš„æ— æ„ä¹‰ç‰‡æ®µï¼‰
        self.min_content_length = 5
        
        # LLM æå–æç¤º
        self.extraction_prompt = """åˆ†æä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œæå–å‡ºåº”è¯¥ä½œä¸º"æŒä¹…å‰ææ¡ä»¶"çš„ä¿¡æ¯ã€‚

ã€é‡è¦æç¤ºã€‘
1. åªæå–å¯¹è¯ä¸­**æ˜ç¡®æåˆ°æˆ–ç¡®ç«‹**çš„ä¿¡æ¯
2. å¦‚æœå¯¹è¯å†…å®¹å¾ˆçŸ­æˆ–æ²¡æœ‰æ˜ç¡®çš„èƒŒæ™¯ä¿¡æ¯ï¼Œè¯·è¿”å›ç©ºæ•°ç»„ []
3. å¯¹äºè§’è‰²æ‰®æ¼”/æ•…äº‹åœºæ™¯ï¼Œè¦æå–è§’è‰²çŠ¶æ€ã€æŠ€èƒ½ã€ç‰©å“ã€å…³ç³»ç­‰å˜åŒ–

æŒä¹…å‰ææ¡ä»¶æ˜¯æŒ‡ï¼šä¸€æ—¦ç¡®ç«‹å°±åº”è¯¥åœ¨åç»­æ‰€æœ‰å¯¹è¯ä¸­é»˜è®¤æˆç«‹çš„èƒŒæ™¯ä¿¡æ¯ã€‚

ã€è§’è‰²æ‰®æ¼”/æ•…äº‹åœºæ™¯é€‚ç”¨çš„æ¡ä»¶ç±»å‹ã€‘
- character_trait: è§’è‰²æ€§æ ¼ã€èº«ä»½ã€å¤–è²Œç­‰å›ºå®šç‰¹å¾
- skill_ability: è§’è‰²ä¹ å¾—çš„æŠ€èƒ½æˆ–èƒ½åŠ›ï¼ˆå¦‚ï¼šå…¥ä¾µç¥è¯†ã€å‰‘æœ¯ï¼‰
- item_prop: è§’è‰²è·å¾—æˆ–æºå¸¦çš„ç‰©å“é“å…·
- relationship: è§’è‰²ä¹‹é—´çš„å…³ç³»å˜åŒ–ï¼ˆå¦‚ï¼šæˆä¸ºå¸ˆå…„å¦¹ã€ç»“ä»‡ï¼‰
- world_setting: ä¸–ç•Œè§‚è®¾å®šï¼ˆå¦‚ï¼šä¿®ä»™ä¸–ç•Œã€é­”æ³•ä½“ç³»ï¼‰
- emotional_state: è§’è‰²å½“å‰æƒ…ç»ªçŠ¶æ€

ã€æ—¥å¸¸å¯¹è¯é€‚ç”¨çš„æ¡ä»¶ç±»å‹ã€‘
- user_identity: ç”¨æˆ·èº«ä»½ï¼ˆå¦‚ï¼šå¤§å­¦æ¯•ä¸šç”Ÿã€ç¨‹åºå‘˜ï¼‰
- user_goal: ç”¨æˆ·ç›®æ ‡ï¼ˆå¦‚ï¼šæƒ³åˆ›ä¸šã€å­¦ä¹ Pythonï¼‰
- environment: æŠ€æœ¯ç¯å¢ƒï¼ˆå¦‚ï¼šWindowså¼€å‘ã€Ubuntuéƒ¨ç½²ï¼‰
- project: é¡¹ç›®ä¿¡æ¯

ã€æå–ç¤ºä¾‹ã€‘
- "æˆ‘ç”¨å…¥ä¾µç¥è¯†çš„åŠæ³•æ‰«æè®°å¿†" â†’ skill_ability: "ä¸»è§’æŒæ¡å…¥ä¾µç¥è¯†èƒ½åŠ›"
- "æäºŒç‹—æ˜¯é‚»æ‘çš„äºŒæµå­" â†’ character_trait: "æäºŒç‹—æ˜¯é‚»æ‘çš„äºŒæµå­"
- "å¸ˆå§é€ç»™äº†æˆ‘ä¸€æŠŠæ³•å‰‘" â†’ item_prop: "ä¸»è§’è·å¾—å¸ˆå§èµ é€çš„æ³•å‰‘"
- "æˆ‘æ˜¯åœ¨ä¿®ä»™ç•Œ" â†’ world_setting: "æ•…äº‹å‘ç”Ÿåœ¨ä¿®ä»™ç•Œ"

å¯¹è¯å†…å®¹ï¼š
{content}

è¯·ä»¥JSONæ ¼å¼è¿”å›æå–çš„æ¡ä»¶ï¼ˆå¦‚æœæ²¡æœ‰æ˜ç¡®çš„æ¡ä»¶åˆ™è¿”å›ç©ºæ•°ç»„ []ï¼‰ï¼š
[
  {{"type": "æ¡ä»¶ç±»å‹", "content": "æ¡ä»¶å†…å®¹ï¼ˆç®€æ´æ¦‚æ‹¬ï¼‰", "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"]}}
]

åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„æ¡ä»¶ï¼Œè¿”å› []ã€‚"""
    
    def _sanitize_path_component(self, name: str) -> str:
        """æ¸…ç†è·¯å¾„ç»„ä»¶ä¸­çš„éæ³•å­—ç¬¦"""
        return "".join(c if c.isalnum() or c in ('_', '-') else '_' for c in name)
    
    def _get_cache_key(self, user_id: str, character_id: str) -> str:
        """è·å–å†…éƒ¨ç¼“å­˜é”®"""
        return f"{user_id}/{character_id}"
    
    def _get_storage_path(self, user_id: str, character_id: str) -> str:
        """è·å–å­˜å‚¨è·¯å¾„
        
        æ–°ç»“æ„ï¼š{base_path}/{user_id}/{character_id}/contexts.json
        """
        safe_user_id = self._sanitize_path_component(user_id)
        safe_char_id = self._sanitize_path_component(character_id)
        return os.path.join(self.base_path, safe_user_id, safe_char_id, 'contexts.json')
    
    def _load_all(self):
        """[å·²åºŸå¼ƒ] é¢„åŠ è½½æ‰€æœ‰æ•°æ® - ä¸å†ä½¿ç”¨
        
        é—®é¢˜ï¼šé¢„åŠ è½½æ—¶ä½¿ç”¨æ–‡ä»¶å¤¹åï¼ˆå·²æ¸…ç†ï¼‰ä½œä¸º user_idï¼Œ
        ä½†å‰ç«¯è¯·æ±‚æ—¶ä½¿ç”¨åŸå§‹ IDï¼Œå¯¼è‡´ç¼“å­˜é”®ä¸åŒ¹é…ã€‚
        æ”¹ç”¨ _ensure_loaded() æŒ‰éœ€åŠ è½½ã€‚
        """
        pass  # ä¸å†é¢„åŠ è½½
    
    def _ensure_loaded(self, user_id: str, character_id: str = "default"):
        """ç¡®ä¿ç”¨æˆ·/è§’è‰²çš„æ•°æ®å·²åŠ è½½ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰
        
        ç±»ä¼¼ ForeshadowingTracker çš„æŒ‰éœ€åŠ è½½æ–¹å¼ï¼Œ
        ä½¿ç”¨åŸå§‹ user_id ä½œä¸ºç¼“å­˜é”®ï¼Œä¿è¯ä¸€è‡´æ€§ã€‚
        """
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key in self.contexts:
            return  # å·²åŠ è½½
        
        # å°è¯•ä»æ–‡ä»¶åŠ è½½
        self._load_user(user_id, character_id)
    
    def _load_user(self, user_id: str, character_id: str = "default"):
        """åŠ è½½ç”¨æˆ·/è§’è‰²çš„ä¸Šä¸‹æ–‡"""
        cache_key = self._get_cache_key(user_id, character_id)
        filepath = self._get_storage_path(user_id, character_id)
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.contexts[cache_key] = [
                        PersistentContext.from_dict(item) for item in data
                    ]
            except Exception as e:
                _safe_print(f"[Recall] åŠ è½½ä¸Šä¸‹æ–‡æ•°æ®å¤±è´¥ ({user_id}/{character_id}): {e}")
                self.contexts[cache_key] = []
    
    def _save_user(self, user_id: str, character_id: str = "default"):
        """ä¿å­˜ç”¨æˆ·/è§’è‰²çš„ä¸Šä¸‹æ–‡"""
        cache_key = self._get_cache_key(user_id, character_id)
        filepath = self._get_storage_path(user_id, character_id)
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        contexts = self.contexts.get(cache_key, [])
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump([c.to_dict() for c in contexts], f, ensure_ascii=False, indent=2)
    
    def set_memory_provider(self, provider):
        """è®¾ç½®è®°å¿†æä¾›å›è°ƒå‡½æ•°
        
        Args:
            provider: å›è°ƒå‡½æ•°ï¼Œæ ¼å¼ä¸º (user_id, limit) -> List[Dict]
                     è¿”å›æ ¼å¼ï¼š[{'content': '...', 'metadata': {'role': 'user/assistant', ...}}]
        """
        self._memory_provider = provider
        _safe_print(f"[ContextTracker] ğŸ”— å·²è®¾ç½® memory_provider")
    
    def on_turn(self, user_id: str = "default", character_id: str = "default") -> Dict[str, Any]:
        """é€šçŸ¥ä¸€è½®å¯¹è¯å®Œæˆï¼Œæ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘æ¡ä»¶æå–
        
        ç±»ä¼¼ ForeshadowingAnalyzer.on_turn()ï¼Œæ¯ N è½®è§¦å‘ä¸€æ¬¡åˆ†æã€‚
        è¿™é¿å…äº†æ¯è½®éƒ½é‡å¤åˆ†æç›¸åŒå¯¹è¯å†å²çš„é—®é¢˜ã€‚
        
        Args:
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            
        Returns:
            Dict: åŒ…å«è§¦å‘çŠ¶æ€å’Œæå–ç»“æœ
        """
        cache_key = f"{user_id}/{character_id}"
        
        # å¢åŠ è½®æ¬¡è®¡æ•°
        if cache_key not in self._turn_counters:
            self._turn_counters[cache_key] = 0
        self._turn_counters[cache_key] += 1
        current_count = self._turn_counters[cache_key]
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘åˆ†æ
        if current_count >= self.trigger_interval:
            _safe_print(f"[ContextTracker] [SYNC] è§¦å‘æ¡ä»¶æå–: user={user_id}, char={character_id}")
            _safe_print(f"[ContextTracker]    è½®æ¬¡={current_count}, é—´éš”={self.trigger_interval}")
            
            # é‡ç½®è®¡æ•°
            self._turn_counters[cache_key] = 0
            
            # æ‰§è¡Œæå–ï¼ˆä½¿ç”¨å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
            if self.llm_client and self._memory_provider:
                extracted = self._extract_from_conversation(user_id, character_id)
                return {
                    'triggered': True,
                    'extracted_count': len(extracted),
                    'extracted': [{'id': ctx.id, 'content': ctx.content, 'type': ctx.context_type.value} for ctx in extracted]
                }
            else:
                _safe_print(f"[ContextTracker]    [SKIP] LLM æˆ– memory_provider æœªé…ç½®ï¼Œè·³è¿‡")
                return {'triggered': False, 'reason': 'LLM or memory_provider not configured'}
        
        return {
            'triggered': False,
            'turns_until_next': self.trigger_interval - current_count
        }
    
    def _extract_from_conversation(self, user_id: str, character_id: str = "default") -> List['PersistentContext']:
        """ä»å¯¹è¯å†å²ä¸­æå–æ¡ä»¶
        
        è·å–æœ€è¿‘çš„å¯¹è¯å†å²ï¼Œç„¶åä½¿ç”¨ LLM æå–æŒä¹…æ¡ä»¶ã€‚
        ä¸ extract_from_text ä¸åŒï¼Œè¿™ä¸ªæ–¹æ³•ä¸“é—¨ç”¨äºæ‰¹é‡åˆ†æå¯¹è¯å†å²ã€‚
        
        ä½¿ç”¨ CONTEXT_MAX_CONTEXT_TURNS é…ç½®æ§åˆ¶è·å–èŒƒå›´ï¼Œä¸ä¼ç¬”åˆ†æå™¨ç»Ÿä¸€ã€‚
        
        Args:
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            
        Returns:
            List[PersistentContext]: æå–çš„æ¡ä»¶åˆ—è¡¨
        """
        # ä½¿ç”¨ max_context_turns * 2 ä½œä¸ºè·å–èŒƒå›´ï¼ˆä¸ä¼ç¬”åˆ†æå™¨ä¿æŒä¸€è‡´ï¼‰
        conversation_context = self._get_conversation_context(user_id, character_id, max_turns=self.max_context_turns * 2)
        
        if not conversation_context:
            _safe_print(f"[ContextTracker] [SKIP] æ— å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡æå–")
            return []
        
        _safe_print(f"[ContextTracker] [SEARCH] ä»å¯¹è¯å†å²æå–æ¡ä»¶")
        _safe_print(f"[ContextTracker]    å¯¹è¯é•¿åº¦: {len(conversation_context)} å­—ç¬¦")
        
        # ä½¿ç”¨ LLM æå–
        return self._extract_with_llm("", user_id, character_id, conversation_context)
    
    def _get_conversation_context(self, user_id: str, character_id: str = "default", 
                                   max_turns: int = 10) -> str:
        """è·å–æœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡
        
        ä»å·²ä¿å­˜çš„è®°å¿†ä¸­è·å–æœ€è¿‘çš„å¯¹è¯ï¼Œç”¨äºæ¡ä»¶æå–æ—¶æä¾›æ›´å¤šä¸Šä¸‹æ–‡ã€‚
        
        Args:
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID  
            max_turns: æœ€å¤§è½®æ•°
            
        Returns:
            æ ¼å¼åŒ–çš„å¯¹è¯å†…å®¹å­—ç¬¦ä¸²
        """
        if not self._memory_provider:
            return ""
        
        try:
            # è·å–æœ€è¿‘çš„è®°å¿†
            memories = self._memory_provider(user_id, max_turns * 2)
            
            if not memories:
                return ""
            
            # è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼
            conversations = []
            for mem in memories:
                metadata = mem.get('metadata', {})
                role = metadata.get('role', 'user')
                content = mem.get('content', '')
                timestamp = metadata.get('timestamp', 0)
                
                if content:
                    conversations.append({
                        'role': role,
                        'content': content,
                        'timestamp': timestamp
                    })
            
            # æŒ‰æ—¶é—´æˆ³æ’åº
            conversations.sort(key=lambda x: x.get('timestamp', 0))
            
            # æ ¼å¼åŒ–ä¸ºæ–‡æœ¬
            lines = []
            for conv in conversations:
                role_label = "ç”¨æˆ·" if conv['role'] == 'user' else "AI"
                lines.append(f"{role_label}: {conv['content']}")
            
            return "\n".join(lines)
            
        except Exception as e:
            _safe_print(f"[ContextTracker] âš ï¸ è·å–å¯¹è¯ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return ""

    # =========================
    # å½’æ¡£æœºåˆ¶ï¼ˆä½ç½®ä¿¡åº¦æ¡ä»¶è‡ªåŠ¨å½’æ¡£ï¼‰
    # =========================
    
    MAX_ARCHIVE_FILE_SIZE = 10 * 1024 * 1024  # 10MB å½’æ¡£æ–‡ä»¶å¤§å°ä¸Šé™
    
    def _get_archive_path(self, user_id: str, character_id: str) -> str:
        """è·å–å½’æ¡£æ–‡ä»¶è·¯å¾„
        
        è·¯å¾„æ ¼å¼ï¼š{base_path}/{user_id}/{character_id}/archive/contexts.jsonl
        """
        safe_user_id = self._sanitize_path_component(user_id)
        safe_char_id = self._sanitize_path_component(character_id)
        archive_dir = os.path.join(self.base_path, safe_user_id, safe_char_id, 'archive')
        os.makedirs(archive_dir, exist_ok=True)
        return os.path.join(archive_dir, 'contexts.jsonl')
    
    def _archive_context(self, ctx: 'PersistentContext', user_id: str, character_id: str,
                         reason: str = 'low_confidence') -> bool:
        """å°†å•ä¸ªæ¡ä»¶å½’æ¡£åˆ° JSONL æ–‡ä»¶
        
        å½’æ¡£æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«å®Œæ•´æ¡ä»¶æ•°æ®å’Œå½’æ¡£æ—¶é—´
        
        Args:
            ctx: è¦å½’æ¡£çš„æ¡ä»¶
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            reason: å½’æ¡£åŸå›  ('low_confidence', 'type_overflow', 'total_overflow')
        
        Returns:
            bool: å½’æ¡£æ˜¯å¦æˆåŠŸ
        """
        import logging
        logger = logging.getLogger(__name__)
        
        try:
            archive_path = self._get_archive_path(user_id, character_id)
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¦‚æœè¶…è¿‡ä¸Šé™åˆ™è½®è½¬
            if os.path.exists(archive_path):
                file_size = os.path.getsize(archive_path)
                if file_size >= self.MAX_ARCHIVE_FILE_SIZE:
                    # è½®è½¬æ–‡ä»¶ï¼šcontexts.jsonl -> contexts_001.jsonl
                    base_name = archive_path.rsplit('.', 1)[0]
                    suffix = 1
                    while os.path.exists(f"{base_name}_{suffix:03d}.jsonl"):
                        suffix += 1
                    os.rename(archive_path, f"{base_name}_{suffix:03d}.jsonl")
                    logger.info(f"[ContextTracker] å½’æ¡£æ–‡ä»¶è½®è½¬: contexts.jsonl -> contexts_{suffix:03d}.jsonl")
            
            # å‡†å¤‡å½’æ¡£æ•°æ®
            archive_data = ctx.to_dict()
            archive_data['archived_at'] = time.time()
            archive_data['archive_reason'] = reason
            
            # è¿½åŠ å†™å…¥ JSONL
            with open(archive_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(archive_data, ensure_ascii=False) + '\n')
            
            logger.debug(f"[ContextTracker] å·²å½’æ¡£æ¡ä»¶: {ctx.id} (ç½®ä¿¡åº¦={ctx.confidence:.2f}, åŸå› ={reason})")
            return True
            
        except Exception as e:
            logger.error(f"[ContextTracker] å½’æ¡£æ¡ä»¶å¤±è´¥: {e}")
            return False
    
    def get_context_by_id(self, context_id: str, user_id: str = "default", 
                          character_id: str = "default") -> Optional['PersistentContext']:
        """æ ¹æ®IDè·å–æ¡ä»¶ï¼ˆåŒ…æ‹¬å·²å½’æ¡£çš„ï¼‰
        
        ä¼˜å…ˆä»æ´»è·ƒæ¡ä»¶ä¸­æŸ¥æ‰¾ï¼Œæ‰¾ä¸åˆ°å†å»å½’æ¡£ä¸­æ‰¾ï¼ˆåŒ…æ‹¬åˆ†å·æ–‡ä»¶ï¼‰
        """
        # ç¡®ä¿æ•°æ®å·²åŠ è½½
        self._ensure_loaded(user_id, character_id)
        
        # å…ˆä»æ´»è·ƒæ¡ä»¶ä¸­æ‰¾
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key in self.contexts:
            for ctx in self.contexts[cache_key]:
                if ctx.id == context_id:
                    return ctx
        
        # å†ä»å½’æ¡£ä¸­æ‰¾ï¼ˆæœç´¢æ‰€æœ‰å½’æ¡£æ–‡ä»¶ï¼ŒåŒ…æ‹¬åˆ†å·ï¼‰
        if self.base_path:
            safe_user_id = self._sanitize_path_component(user_id)
            safe_char_id = self._sanitize_path_component(character_id)
            archive_dir = os.path.join(self.base_path, safe_user_id, safe_char_id, 'archive')
            
            if os.path.exists(archive_dir):
                for filename in os.listdir(archive_dir):
                    if filename.startswith('contexts') and filename.endswith('.jsonl'):
                        archive_path = os.path.join(archive_dir, filename)
                        try:
                            with open(archive_path, 'r', encoding='utf-8') as f:
                                for line in f:
                                    if line.strip():
                                        data = json.loads(line)
                                        if data.get('id') == context_id:
                                            return PersistentContext.from_dict(data)
                        except Exception:
                            pass
        
        return None

    def get_archived_contexts(
        self,
        user_id: str = "default",
        character_id: str = "default",
        page: int = 1,
        page_size: int = 20,
        search: Optional[str] = None,
        context_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """è·å–å½’æ¡£çš„æŒä¹…æ¡ä»¶åˆ—è¡¨ï¼ˆåˆ†é¡µã€æœç´¢ã€ç­›é€‰ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            page: é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
            page_size: æ¯é¡µæ•°é‡
            search: æœç´¢å…³é”®è¯ï¼ˆæœç´¢å†…å®¹ï¼‰
            context_type: ç±»å‹ç­›é€‰
            
        Returns:
            Dict: {
                'items': List[Dict],  # å½“å‰é¡µçš„æ¡ä»¶åˆ—è¡¨
                'total': int,         # æ€»æ•°é‡
                'page': int,          # å½“å‰é¡µ
                'page_size': int,     # æ¯é¡µæ•°é‡
                'total_pages': int    # æ€»é¡µæ•°
            }
        """
        all_archived = []
        
        if self.base_path:
            safe_user_id = self._sanitize_path_component(user_id)
            safe_char_id = self._sanitize_path_component(character_id)
            archive_dir = os.path.join(self.base_path, safe_user_id, safe_char_id, 'archive')
            
            if os.path.exists(archive_dir):
                # è¯»å–æ‰€æœ‰å½’æ¡£æ–‡ä»¶
                for filename in sorted(os.listdir(archive_dir), reverse=True):
                    if filename.startswith('contexts') and filename.endswith('.jsonl'):
                        archive_path = os.path.join(archive_dir, filename)
                        try:
                            with open(archive_path, 'r', encoding='utf-8') as f:
                                for line in f:
                                    if line.strip():
                                        data = json.loads(line)
                                        all_archived.append(data)
                        except Exception:
                            pass
        
        # æŒ‰å½’æ¡£æ—¶é—´å€’åºæ’åˆ—
        all_archived.sort(key=lambda x: x.get('archived_at', 0), reverse=True)
        
        # ç­›é€‰ï¼šç±»å‹
        if context_type:
            all_archived = [c for c in all_archived if c.get('context_type') == context_type]
        
        # ç­›é€‰ï¼šæœç´¢
        if search:
            search_lower = search.lower()
            all_archived = [c for c in all_archived if search_lower in c.get('content', '').lower()]
        
        # åˆ†é¡µ
        total = len(all_archived)
        total_pages = max(1, (total + page_size - 1) // page_size)
        page = max(1, min(page, total_pages))
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        items = all_archived[start_idx:end_idx]
        
        return {
            'items': items,
            'total': total,
            'page': page,
            'page_size': page_size,
            'total_pages': total_pages
        }

    def restore_from_archive(
        self,
        context_id: str,
        user_id: str = "default",
        character_id: str = "default"
    ) -> Optional['PersistentContext']:
        """ä»å½’æ¡£æ¢å¤æ¡ä»¶åˆ°æ´»è·ƒåˆ—è¡¨
        
        Args:
            context_id: æ¡ä»¶ID
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            
        Returns:
            Optional[PersistentContext]: æ¢å¤çš„æ¡ä»¶ï¼Œæœªæ‰¾åˆ°è¿”å› None
        """
        import logging
        logger = logging.getLogger(__name__)
        
        if not self.base_path:
            return None
        
        safe_user_id = self._sanitize_path_component(user_id)
        safe_char_id = self._sanitize_path_component(character_id)
        archive_dir = os.path.join(self.base_path, safe_user_id, safe_char_id, 'archive')
        
        if not os.path.exists(archive_dir):
            return None
        
        # åœ¨æ‰€æœ‰å½’æ¡£æ–‡ä»¶ä¸­æŸ¥æ‰¾å¹¶ç§»é™¤
        found_data = None
        for filename in os.listdir(archive_dir):
            if not (filename.startswith('contexts') and filename.endswith('.jsonl')):
                continue
            
            archive_path = os.path.join(archive_dir, filename)
            lines_to_keep = []
            
            try:
                with open(archive_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            data = json.loads(line)
                            if data.get('id') == context_id and found_data is None:
                                found_data = data
                            else:
                                lines_to_keep.append(line)
                
                if found_data:
                    # é‡å†™å½’æ¡£æ–‡ä»¶ï¼ˆç§»é™¤å·²æ¢å¤çš„æ¡ä»¶ï¼‰
                    with open(archive_path, 'w', encoding='utf-8') as f:
                        f.writelines(lines_to_keep)
                    break
            except Exception as e:
                logger.error(f"[ContextTracker] è¯»å–å½’æ¡£æ–‡ä»¶å¤±è´¥: {e}")
        
        if not found_data:
            return None
        
        # ç§»é™¤å½’æ¡£å­—æ®µï¼Œåˆ›å»º PersistentContext å¯¹è±¡
        found_data.pop('archived_at', None)
        found_data.pop('archive_reason', None)
        ctx = PersistentContext.from_dict(found_data)
        ctx.is_active = True
        
        # æ·»åŠ åˆ°æ´»è·ƒåˆ—è¡¨
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            self.contexts[cache_key] = []
        self.contexts[cache_key].append(ctx)
        self._save_user(user_id, character_id)
        
        logger.info(f"[ContextTracker] å·²ä»å½’æ¡£æ¢å¤æ¡ä»¶: {context_id}")
        return ctx

    def delete_archived(
        self,
        context_id: str,
        user_id: str = "default",
        character_id: str = "default"
    ) -> bool:
        """å½»åº•åˆ é™¤å½’æ¡£ä¸­çš„æ¡ä»¶
        
        Args:
            context_id: æ¡ä»¶ID
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        import logging
        logger = logging.getLogger(__name__)
        
        if not self.base_path:
            return False
        
        safe_user_id = self._sanitize_path_component(user_id)
        safe_char_id = self._sanitize_path_component(character_id)
        archive_dir = os.path.join(self.base_path, safe_user_id, safe_char_id, 'archive')
        
        if not os.path.exists(archive_dir):
            return False
        
        deleted = False
        for filename in os.listdir(archive_dir):
            if not (filename.startswith('contexts') and filename.endswith('.jsonl')):
                continue
            
            archive_path = os.path.join(archive_dir, filename)
            lines_to_keep = []
            
            try:
                with open(archive_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            data = json.loads(line)
                            if data.get('id') == context_id:
                                deleted = True
                            else:
                                lines_to_keep.append(line)
                
                if deleted:
                    with open(archive_path, 'w', encoding='utf-8') as f:
                        f.writelines(lines_to_keep)
                    logger.info(f"[ContextTracker] å·²å½»åº•åˆ é™¤å½’æ¡£æ¡ä»¶: {context_id}")
                    break
            except Exception as e:
                logger.error(f"[ContextTracker] åˆ é™¤å½’æ¡£æ¡ä»¶å¤±è´¥: {e}")
        
        return deleted

    def clear_archived(
        self,
        user_id: str = "default",
        character_id: str = "default"
    ) -> int:
        """æ¸…ç©ºæ‰€æœ‰å½’æ¡£æ¡ä»¶
        
        Args:
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            
        Returns:
            int: åˆ é™¤çš„æ¡ä»¶æ•°é‡
        """
        import logging
        logger = logging.getLogger(__name__)
        
        if not self.base_path:
            return 0
        
        safe_user_id = self._sanitize_path_component(user_id)
        safe_char_id = self._sanitize_path_component(character_id)
        archive_dir = os.path.join(self.base_path, safe_user_id, safe_char_id, 'archive')
        
        if not os.path.exists(archive_dir):
            return 0
        
        count = 0
        for filename in os.listdir(archive_dir):
            if filename.startswith('contexts') and filename.endswith('.jsonl'):
                archive_path = os.path.join(archive_dir, filename)
                try:
                    # ç»Ÿè®¡è¡Œæ•°
                    with open(archive_path, 'r', encoding='utf-8') as f:
                        count += sum(1 for line in f if line.strip())
                    # åˆ é™¤æ–‡ä»¶
                    os.remove(archive_path)
                except Exception as e:
                    logger.error(f"[ContextTracker] æ¸…ç©ºå½’æ¡£æ–‡ä»¶å¤±è´¥: {e}")
        
        logger.info(f"[ContextTracker] å·²æ¸…ç©ºå½’æ¡£æ¡ä»¶: {count} ä¸ª")
        return count

    def clear_user(
        self,
        user_id: str = "default",
        character_id: str = "default"
    ) -> bool:
        """æ¸…ç©ºæŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰ä¸Šä¸‹æ–‡æ•°æ®
        
        Args:
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²IDï¼ˆå¦‚æœä¸ºç©ºåˆ™æ¸…ç©ºè¯¥ç”¨æˆ·æ‰€æœ‰è§’è‰²ï¼‰
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        import logging
        import shutil
        logger = logging.getLogger(__name__)
        
        try:
            # æ¸…ç©ºå†…å­˜ä¸­çš„æ•°æ®
            cache_key = f"{user_id}/{character_id}"
            if cache_key in self._user_data:
                del self._user_data[cache_key]
            
            # æ¸…ç©ºç£ç›˜æ•°æ®
            if self.base_path:
                safe_user_id = self._sanitize_path_component(user_id)
                safe_char_id = self._sanitize_path_component(character_id)
                user_path = os.path.join(self.base_path, safe_user_id, safe_char_id)
                
                if os.path.exists(user_path):
                    shutil.rmtree(user_path)
                    logger.info(f"[ContextTracker] å·²æ¸…ç©ºç”¨æˆ·æ•°æ®: {user_id}/{character_id}")
            
            return True
        except Exception as e:
            logger.error(f"[ContextTracker] æ¸…ç©ºç”¨æˆ·æ•°æ®å¤±è´¥: {e}")
            return False
    
    def clear(self) -> bool:
        """æ¸…ç©ºæ‰€æœ‰ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆå…¨å±€ï¼‰
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        import logging
        import shutil
        logger = logging.getLogger(__name__)
        
        try:
            # æ¸…ç©ºå†…å­˜ä¸­çš„æ‰€æœ‰æ•°æ®
            self._user_data.clear()
            
            # æ¸…ç©ºç£ç›˜ä¸Šçš„æ‰€æœ‰æ•°æ®
            if self.base_path and os.path.exists(self.base_path):
                for item in os.listdir(self.base_path):
                    item_path = os.path.join(self.base_path, item)
                    if os.path.isdir(item_path):
                        shutil.rmtree(item_path)
                logger.info("[ContextTracker] å·²æ¸…ç©ºæ‰€æœ‰ä¸Šä¸‹æ–‡æ•°æ®")
            
            return True
        except Exception as e:
            logger.error(f"[ContextTracker] æ¸…ç©ºæ‰€æœ‰æ•°æ®å¤±è´¥: {e}")
            return False

    def archive_context(
        self,
        context_id: str,
        user_id: str = "default",
        character_id: str = "default"
    ) -> bool:
        """æ‰‹åŠ¨å°†æ´»è·ƒæ¡ä»¶å½’æ¡£
        
        Args:
            context_id: æ¡ä»¶ID
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå½’æ¡£
        """
        import logging
        logger = logging.getLogger(__name__)
        
        # ç¡®ä¿æ•°æ®å·²åŠ è½½
        self._ensure_loaded(user_id, character_id)
        
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return False
        
        # æŸ¥æ‰¾å¹¶ç§»é™¤
        ctx_to_archive = None
        for i, ctx in enumerate(self.contexts[cache_key]):
            if ctx.id == context_id:
                ctx_to_archive = self.contexts[cache_key].pop(i)
                break
        
        if not ctx_to_archive:
            return False
        
        # å½’æ¡£
        success = self._archive_context(ctx_to_archive, user_id, character_id, reason='manual')
        if success:
            self._save_user(user_id, character_id)
            logger.info(f"[ContextTracker] å·²æ‰‹åŠ¨å½’æ¡£æ¡ä»¶: {context_id}")
        
        return success

    def update_context(
        self,
        context_id: str,
        user_id: str = "default",
        character_id: str = "default",
        content: Optional[str] = None,
        context_type: Optional[str] = None,
        confidence: Optional[float] = None,
        keywords: Optional[List[str]] = None
    ) -> Optional['PersistentContext']:
        """æ›´æ–°æŒä¹…æ¡ä»¶çš„å­—æ®µ
        
        Args:
            context_id: æ¡ä»¶ID
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            content: æ–°å†…å®¹ï¼ˆå¯é€‰ï¼‰
            context_type: æ–°ç±»å‹ï¼ˆå¯é€‰ï¼‰
            confidence: æ–°ç½®ä¿¡åº¦ï¼ˆå¯é€‰ï¼‰
            keywords: æ–°å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Optional[PersistentContext]: æ›´æ–°åçš„æ¡ä»¶ï¼Œæœªæ‰¾åˆ°è¿”å› None
        """
        import logging
        logger = logging.getLogger(__name__)
        
        # ç¡®ä¿æ•°æ®å·²åŠ è½½
        self._ensure_loaded(user_id, character_id)
        
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return None
        
        for ctx in self.contexts[cache_key]:
            if ctx.id == context_id:
                if content is not None:
                    ctx.content = content
                    # æ›´æ–° embedding
                    new_embedding = self._get_embedding(content)
                    if new_embedding:
                        ctx.embedding = new_embedding
                if context_type is not None:
                    try:
                        ctx.context_type = ContextType(context_type)
                    except ValueError:
                        ctx.context_type = ContextType.CUSTOM
                if confidence is not None:
                    ctx.confidence = max(0.0, min(1.0, confidence))
                if keywords is not None:
                    ctx.keywords = keywords
                
                self._save_user(user_id, character_id)
                logger.info(f"[ContextTracker] å·²æ›´æ–°æ¡ä»¶: {context_id}")
                return ctx
        
        return None

    def _get_embedding(self, text: str) -> Optional[List[float]]:
        """è·å–æ–‡æœ¬çš„Embeddingå‘é‡
        
        ä½¿ç”¨embedding_backendè·å–å‘é‡ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if not self.embedding_backend:
            return None
        
        try:
            # embedding_backend.embed() è¿”å› List[List[float]]
            embeddings = self.embedding_backend.embed([text])
            if embeddings and len(embeddings) > 0:
                return embeddings[0]
        except Exception as e:
            import logging
            logging.getLogger(__name__).debug(f"è·å–Embeddingå¤±è´¥: {e}")
        
        return None
    
    def _compute_embedding_similarity(self, emb1: List[float], emb2: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸ªEmbeddingå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        ä½¿ç”¨numpyé«˜æ•ˆè®¡ç®—ï¼ŒèŒƒå›´ [-1, 1]ï¼Œé€šå¸¸æ­£å¸¸æ–‡æœ¬ç›¸ä¼¼åº¦åœ¨ [0, 1]
        """
        if emb1 is None or emb2 is None:
            return 0.0
        if len(emb1) == 0 or len(emb2) == 0:
            return 0.0
        
        vec1 = np.array(emb1)
        vec2 = np.array(emb2)
        
        # ä½™å¼¦ç›¸ä¼¼åº¦ = dot(a, b) / (norm(a) * norm(b))
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(dot_product / (norm1 * norm2))
    
    def _compute_word_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„è¯é‡å ç›¸ä¼¼åº¦ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
        
        åŸºäºJaccardç›¸ä¼¼åº¦ï¼Œæ”¯æŒä¸­è‹±æ–‡
        """
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        # å¯¹äºä¸­æ–‡ï¼ŒæŒ‰å­—ç¬¦
        if len(words1) <= 2:  # å¯èƒ½æ˜¯ä¸­æ–‡
            words1 = set(text1.lower())
            words2 = set(text2.lower())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        return len(intersection) / len(union) if union else 0.0
    
    def _compute_similarity(self, text1: str, text2: str, 
                           emb1: Optional[List[float]] = None, 
                           emb2: Optional[List[float]] = None) -> Tuple[float, str]:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦
        
        ä¸‰çº§ç­–ç•¥ï¼š
        1. å¦‚æœä¸¤è¾¹éƒ½æœ‰Embeddingï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæœ€å‡†ç¡®ï¼‰
        2. å¦‚æœåªæœ‰ä¸€è¾¹æœ‰Embeddingï¼Œå°è¯•è·å–å¦ä¸€è¾¹çš„
        3. åå¤‡ï¼šä½¿ç”¨è¯é‡å ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
        
        Args:
            text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
            text2: ç¬¬äºŒä¸ªæ–‡æœ¬
            emb1: text1çš„Embeddingå‘é‡ï¼ˆå¯é€‰ï¼‰
            emb2: text2çš„Embeddingå‘é‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Tuple[float, str]: (ç›¸ä¼¼åº¦, ä½¿ç”¨çš„æ–¹æ³• "embedding"/"word")
        """
        dedup_config = self._get_dedup_config()
        
        # å¦‚æœå¯ç”¨äº†Embeddingå»é‡
        if dedup_config['enabled']:
            # ä¼˜åŒ–ï¼šåªæœ‰å½“æ–°å†…å®¹çš„ embedding å·²ç»è®¡ç®—å¥½æ—¶ï¼Œæ‰ä½¿ç”¨ embedding æ¯”è¾ƒ
            # ä¸è¦ä¸ºæ—§æ•°æ®ï¼ˆæ²¡æœ‰å­˜å‚¨ embeddingï¼‰è°ƒç”¨ APIï¼Œé¿å… N æ¬¡è°ƒç”¨
            # emb2 æ˜¯æ–°å†…å®¹çš„ embeddingï¼ˆè°ƒç”¨æ–¹åº”è¯¥é¢„å…ˆè®¡ç®—å¥½ï¼‰
            # emb1 æ˜¯ç°æœ‰æ¡ä»¶çš„ embeddingï¼ˆåº”è¯¥ä»å­˜å‚¨ä¸­è¯»å–ï¼Œå¦‚æœæ²¡æœ‰å°±è·³è¿‡ï¼‰
            
            # å¦‚æœä¸¤è¾¹éƒ½æœ‰Embeddingï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            if emb1 is not None and emb2 is not None:
                sim = self._compute_embedding_similarity(emb1, emb2)
                return (sim, "embedding")
        
        # åå¤‡ï¼šè¯é‡å 
        sim = self._compute_word_similarity(text1, text2)
        return (sim, "word")
    
    def _find_similar(self, content: str, user_id: str, character_id: str,
                     context_type: ContextType,
                     new_embedding: Optional[List[float]] = None) -> Tuple[Optional[PersistentContext], float, str]:
        """æŸ¥æ‰¾è¯­ä¹‰ç›¸ä¼¼çš„å·²æœ‰æ¡ä»¶
        
        ä½¿ç”¨ä¸‰çº§ç­–ç•¥ï¼š
        1. å®Œå…¨ç›¸åŒï¼šç›´æ¥è¿”å›
        2. Embeddingç›¸ä¼¼åº¦ >= HIGH_THRESHOLDï¼šé«˜åº¦ç›¸ä¼¼ï¼Œå¯è‡ªåŠ¨åˆå¹¶
        3. Embeddingç›¸ä¼¼åº¦ < LOW_THRESHOLDï¼šæ˜æ˜¾ä¸åŒï¼Œè·³è¿‡
        4. ä¸­é—´åŒºåŸŸï¼šè¿”å›ä½†æ ‡è®°ï¼Œå¯èƒ½éœ€è¦LLMåˆ¤æ–­
        
        Args:
            content: æ–°æ¡ä»¶å†…å®¹
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            context_type: æ¡ä»¶ç±»å‹
            new_embedding: æ–°æ¡ä»¶çš„Embeddingå‘é‡ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™é¿å…é‡å¤è®¡ç®—ï¼‰
            
        Returns:
            Tuple[Optional[PersistentContext], float, str]: 
                (ç›¸ä¼¼æ¡ä»¶æˆ–None, ç›¸ä¼¼åº¦, æ–¹æ³• "exact"/"embedding"/"word")
        """
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return (None, 0.0, "none")
        
        dedup_config = self._get_dedup_config()
        best_match: Optional[PersistentContext] = None
        best_sim = 0.0
        best_method = "none"
        
        for existing in self.contexts[cache_key]:
            if existing.context_type != context_type:
                continue
            if not existing.is_active:
                continue
            
            # 1. å®Œå…¨ç›¸åŒï¼ˆå¿«é€Ÿè·¯å¾„ï¼‰
            if existing.content.lower().strip() == content.lower().strip():
                return (existing, 1.0, "exact")
            
            # 2. è®¡ç®—ç›¸ä¼¼åº¦
            sim, method = self._compute_similarity(
                existing.content, content,
                emb1=existing.embedding,  # ä½¿ç”¨å·²å­˜å‚¨çš„Embedding
                emb2=new_embedding
            )
            
            # ä½¿ç”¨åŠ¨æ€é˜ˆå€¼ï¼ˆEmbeddingæ–¹æ³•ç”¨é…ç½®çš„é˜ˆå€¼ï¼Œè¯é‡å ç”¨å›ºå®šé˜ˆå€¼ï¼‰
            if method == "embedding":
                threshold = dedup_config['high_threshold']
            else:
                threshold = self.SIMILARITY_THRESHOLD
            
            # è®°å½•æœ€ä½³åŒ¹é…
            if sim > best_sim:
                best_sim = sim
                best_match = existing
                best_method = method
        
        # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°åˆå¹¶é˜ˆå€¼
        if best_match:
            if best_method == "embedding":
                if best_sim >= dedup_config['high_threshold']:
                    return (best_match, best_sim, best_method)
                elif best_sim < dedup_config['low_threshold']:
                    return (None, best_sim, best_method)  # æ˜æ˜¾ä¸åŒ
                else:
                    # ä¸­é—´åŒºåŸŸï¼Œè¿”å›åŒ¹é…ä½†éœ€è¦è¿›ä¸€æ­¥åˆ¤æ–­
                    return (best_match, best_sim, best_method + "_uncertain")
            else:
                # è¯é‡å æ–¹æ³•ï¼Œä½¿ç”¨å›ºå®šé˜ˆå€¼
                if best_sim >= self.SIMILARITY_THRESHOLD:
                    return (best_match, best_sim, best_method)
        
        return (None, best_sim, best_method)
    
    def _enforce_limits(self, user_id: str, character_id: str = "default"):
        """å¼ºåˆ¶æ‰§è¡Œæ•°é‡é™åˆ¶
        
        å¤„ç†æµç¨‹ï¼š
        1. åº”ç”¨ç½®ä¿¡åº¦è¡°å‡
        2. å½’æ¡£å¹¶åˆ é™¤ä½ç½®ä¿¡åº¦æ¡ä»¶
        3. æŒ‰ç±»å‹é™åˆ¶æ•°é‡ï¼ˆæº¢å‡ºçš„å½’æ¡£ï¼‰
        4. æŒ‰æ€»æ•°é™åˆ¶æ•°é‡ï¼ˆæº¢å‡ºçš„å½’æ¡£ï¼‰
        """
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return
        
        contexts = self.contexts[cache_key]
        
        # 1. åº”ç”¨ç½®ä¿¡åº¦è¡°å‡ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
        now = time.time()
        decay_threshold = now - (self.DECAY_DAYS * 24 * 3600)
        
        for ctx in contexts:
            if ctx.is_active and ctx.last_used < decay_threshold:
                # ä¹˜æ³•è¡°å‡ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰ï¼šæ¯æ¬¡è¡°å‡ DECAY_RATE æ¯”ä¾‹
                # ä¾‹å¦‚ DECAY_RATE=0.05 æ—¶ï¼Œæ¯æ¬¡è¡°å‡åä¿ç•™ 95% çš„ç½®ä¿¡åº¦
                # ä½äº MIN_CONFIDENCE æ—¶ä¼šè¢«ä¸‹é¢çš„é€»è¾‘å½’æ¡£
                ctx.confidence = max(0, ctx.confidence * (1 - self.DECAY_RATE))
        
        # 2. å½’æ¡£å¹¶åˆ é™¤ä½ç½®ä¿¡åº¦æ¡ä»¶
        to_archive = []
        for ctx in contexts:
            if ctx.confidence <= self.MIN_CONFIDENCE + self.FLOAT_EPSILON:  # ä½¿ç”¨ç±»å¸¸é‡å®¹å·®
                ctx.is_active = False
                to_archive.append(ctx)
        
        # å½’æ¡£ä½ç½®ä¿¡åº¦æ¡ä»¶
        for ctx in to_archive:
            self._archive_context(ctx, user_id, character_id, reason='low_confidence')
        
        # åªä¿ç•™æ´»è·ƒçš„
        contexts = [c for c in contexts if c.is_active]
        
        # 3. æ¯ç±»å‹æ•°é‡é™åˆ¶
        by_type: Dict[ContextType, List[PersistentContext]] = {}
        for ctx in contexts:
            if ctx.context_type not in by_type:
                by_type[ctx.context_type] = []
            by_type[ctx.context_type].append(ctx)
        
        # æ¯ç±»å‹åªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„ MAX_PER_TYPE ä¸ªï¼Œè¶…å‡ºçš„å½’æ¡£
        kept = []
        for ctx_type, type_contexts in by_type.items():
            # æŒ‰ç½®ä¿¡åº¦æ’åº
            type_contexts.sort(key=lambda c: -c.confidence)
            kept.extend(type_contexts[:self.MAX_PER_TYPE])
            # è¶…å‡ºçš„å½’æ¡£
            for ctx in type_contexts[self.MAX_PER_TYPE:]:
                self._archive_context(ctx, user_id, character_id, reason='type_overflow')
        
        # 4. æ€»æ•°é™åˆ¶
        if len(kept) > self.MAX_TOTAL:
            kept.sort(key=lambda c: -c.confidence)
            # è¶…å‡ºçš„å½’æ¡£
            for ctx in kept[self.MAX_TOTAL:]:
                self._archive_context(ctx, user_id, character_id, reason='total_overflow')
            kept = kept[:self.MAX_TOTAL]
        
        self.contexts[cache_key] = kept
        
        # ä¿å­˜æ›´æ”¹ï¼ˆç¡®ä¿å½’æ¡£åçš„çŠ¶æ€æŒä¹…åŒ–ï¼‰
        self._save_user(user_id, character_id)
    
    def add(self, content: str, context_type: ContextType, user_id: str = "default",
            character_id: str = "default",
            source_turn: str = None, keywords: List[str] = None,
            related_entities: List[str] = None) -> PersistentContext:
        """æ·»åŠ æŒä¹…ä¸Šä¸‹æ–‡
        
        æ™ºèƒ½å¤„ç†ï¼ˆä¸‰çº§å»é‡ç­–ç•¥ï¼‰ï¼š
        1. è®¡ç®—æ–°å†…å®¹çš„Embeddingï¼ˆå¦‚æœå¯ç”¨ï¼‰
        2. æŸ¥æ‰¾è¯­ä¹‰ç›¸ä¼¼çš„å·²æœ‰æ¡ä»¶
        3. é«˜ç›¸ä¼¼åº¦ï¼šè‡ªåŠ¨åˆå¹¶ï¼ˆå¢åŠ ç½®ä¿¡åº¦ï¼‰
        4. ä¸­ç­‰ç›¸ä¼¼åº¦ï¼šè°¨æ…åˆå¹¶ï¼ˆå¦‚æœå†…å®¹æ›´è¯¦ç»†åˆ™æ›´æ–°ï¼‰
        5. ä½ç›¸ä¼¼åº¦æˆ–æ— åŒ¹é…ï¼šåˆ›å»ºæ–°æ¡ä»¶
        6. è¶…å‡ºæ•°é‡é™åˆ¶æ—¶ï¼Œæ·˜æ±°ç½®ä¿¡åº¦æœ€ä½çš„
        
        æ³¨æ„ï¼šå¦‚æœè¯¥ç±»å‹å·²è¾¾åˆ°ä¸Šé™ä¸”æ–°æ¡ä»¶ç½®ä¿¡åº¦ä¸å¤Ÿé«˜ï¼Œ
        æ–°æ¡ä»¶å¯èƒ½ä¼šè¢«ç«‹å³æ·˜æ±°ï¼ˆä½†ä»ä¼šè¿”å›è¯¥å¯¹è±¡ï¼Œis_active=Falseï¼‰
        """
        import uuid
        
        # ç¡®ä¿æ•°æ®å·²åŠ è½½
        self._ensure_loaded(user_id, character_id)
        
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            self.contexts[cache_key] = []
        
        # 1. é¢„è®¡ç®—æ–°å†…å®¹çš„Embeddingï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        new_embedding = self._get_embedding(content)
        
        # 2. æ£€æŸ¥æ˜¯å¦æœ‰è¯­ä¹‰ç›¸ä¼¼çš„å·²æœ‰æ¡ä»¶
        similar, sim_score, sim_method = self._find_similar(
            content, user_id, character_id, context_type, new_embedding=new_embedding
        )
        
        if similar:
            # å‘ç°ç›¸ä¼¼æ¡ä»¶ï¼Œè¿›è¡Œåˆå¹¶è€Œä¸æ˜¯åˆ›å»ºæ–°æ¡ä»¶
            content_preview = content[:35].replace('\n', ' ')
            similar_preview = similar.content[:35].replace('\n', ' ')
            _safe_print(f"[ContextTracker] ğŸ”„ å»é‡åˆå¹¶:")
            _safe_print(f"[ContextTracker]    æ–°: {content_preview}...")
            _safe_print(f"[ContextTracker]    æ—§: {similar_preview}...")
            _safe_print(f"[ContextTracker]    æ–¹æ³•={sim_method}, ç›¸ä¼¼åº¦={sim_score:.3f}")
            
            # åˆå¹¶ç­–ç•¥
            if sim_method == "exact":
                # å®Œå…¨ç›¸åŒï¼Œåªæ›´æ–°ä½¿ç”¨ä¿¡æ¯
                similar.use_count += 1
                similar.last_used = time.time()
                _safe_print(f"[ContextTracker]    âœ… å®Œå…¨ç›¸åŒï¼Œæ›´æ–°ä½¿ç”¨è®¡æ•°: {similar.use_count}")
            elif sim_method.endswith("_uncertain"):
                # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼Œè°¨æ…åˆå¹¶
                similar.confidence = min(1.0, similar.confidence + 0.05)  # è¾ƒå°å¢é‡
                similar.use_count += 1
                similar.last_used = time.time()
                # å¦‚æœæ–°å†…å®¹æ˜æ˜¾æ›´é•¿/æ›´è¯¦ç»†ï¼Œç”¨æ–°å†…å®¹æ›¿æ¢
                if len(content) > len(similar.content) * 1.2:
                    similar.content = content
                    # æ›´æ–°Embedding
                    if new_embedding:
                        similar.embedding = new_embedding
            else:
                # é«˜åº¦ç›¸ä¼¼ï¼Œæ­£å¸¸åˆå¹¶
                similar.confidence = min(1.0, similar.confidence + 0.1)
                similar.use_count += 1
                similar.last_used = time.time()
                # å¦‚æœæ–°å†…å®¹æ›´é•¿/æ›´è¯¦ç»†ï¼Œç”¨æ–°å†…å®¹æ›¿æ¢
                if len(content) > len(similar.content):
                    similar.content = content
                    # æ›´æ–°Embedding
                    if new_embedding:
                        similar.embedding = new_embedding
            
            self._save_user(user_id, character_id)
            return similar
        
        # 3. åˆ›å»ºæ–°æ¡ä»¶
        ctx = PersistentContext(
            id=f"ctx_{uuid.uuid4().hex[:12]}",
            content=content,
            context_type=context_type,
            user_id=user_id,
            source_turn=source_turn,
            keywords=keywords or [],
            related_entities=related_entities or []
        )
        
        # å­˜å‚¨Embeddingï¼ˆå¦‚æœæœ‰ï¼‰
        if new_embedding:
            ctx.embedding = new_embedding
        
        self.contexts[cache_key].append(ctx)
        
        # 4. å¼ºåˆ¶æ‰§è¡Œæ•°é‡é™åˆ¶
        self._enforce_limits(user_id, character_id)
        
        # æ£€æŸ¥æ–°æ¡ä»¶æ˜¯å¦è¢«æ·˜æ±°äº†
        if ctx not in self.contexts[cache_key]:
            # æ¡ä»¶è¢«æ·˜æ±°äº†ï¼ˆå› ä¸ºç½®ä¿¡åº¦ä¸å¤Ÿé«˜ï¼‰ï¼Œæ ‡è®°ä¸ºä¸æ´»è·ƒ
            ctx.is_active = False
            _safe_print(f"[ContextTracker] æ–°æ¡ä»¶å› æ•°é‡é™åˆ¶è¢«æ·˜æ±°: {content[:50]}...")
        else:
            _safe_print(f"[ContextTracker] åˆ›å»ºæ–°æ¡ä»¶: type={context_type.value}, content={content[:50]}...")
        
        self._save_user(user_id, character_id)
        return ctx
    
    def get_active(self, user_id: str = "default", character_id: str = "default") -> List[PersistentContext]:
        """è·å–æ‰€æœ‰æ´»è·ƒçš„æŒä¹…ä¸Šä¸‹æ–‡"""
        # ç¡®ä¿æ•°æ®å·²åŠ è½½
        self._ensure_loaded(user_id, character_id)
        
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return []
        
        now = time.time()
        active = []
        
        for ctx in self.contexts[cache_key]:
            if not ctx.is_active:
                continue
            if ctx.expires_at and ctx.expires_at < now:
                ctx.is_active = False
                continue
            active.append(ctx)
        
        # æŒ‰ç±»å‹å’Œç½®ä¿¡åº¦æ’åº
        active.sort(key=lambda c: (-c.confidence, c.context_type.value))
        return active
    
    def get_by_type(self, context_type: ContextType, user_id: str = "default",
                    character_id: str = "default") -> List[PersistentContext]:
        """æŒ‰ç±»å‹è·å–ä¸Šä¸‹æ–‡"""
        return [c for c in self.get_active(user_id, character_id) if c.context_type == context_type]
    
    def deactivate(self, context_id: str, user_id: str = "default",
                   character_id: str = "default") -> bool:
        """åœç”¨æŸä¸ªä¸Šä¸‹æ–‡"""
        # ç¡®ä¿æ•°æ®å·²åŠ è½½
        self._ensure_loaded(user_id, character_id)
        
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return False
        
        for ctx in self.contexts[cache_key]:
            if ctx.id == context_id:
                ctx.is_active = False
                self._save_user(user_id, character_id)
                return True
        return False
    
    def mark_used(self, context_id: str, user_id: str = "default", character_id: str = "default",
                  save: bool = True):
        """æ ‡è®°ä¸Šä¸‹æ–‡è¢«ä½¿ç”¨
        
        Args:
            context_id: æ¡ä»¶ID
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            save: æ˜¯å¦ç«‹å³ä¿å­˜ï¼ˆæ‰¹é‡æ“ä½œæ—¶è®¾ä¸ºFalseï¼Œæœ€åç»Ÿä¸€ä¿å­˜ï¼‰
        """
        # ç¡®ä¿æ•°æ®å·²åŠ è½½
        self._ensure_loaded(user_id, character_id)
        
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts:
            return
        
        for ctx in self.contexts[cache_key]:
            if ctx.id == context_id:
                ctx.last_used = time.time()
                ctx.use_count += 1
                if save:
                    self._save_user(user_id, character_id)
                return
    
    def mark_used_batch(self, context_ids: List[str], user_id: str = "default",
                        character_id: str = "default"):
        """æ‰¹é‡æ ‡è®°ä¸Šä¸‹æ–‡è¢«ä½¿ç”¨ï¼ˆåªä¿å­˜ä¸€æ¬¡ï¼‰"""
        # ç¡®ä¿æ•°æ®å·²åŠ è½½
        self._ensure_loaded(user_id, character_id)
        
        cache_key = self._get_cache_key(user_id, character_id)
        if cache_key not in self.contexts or not context_ids:
            return
        
        now = time.time()
        id_set = set(context_ids)  # è½¬æˆ set æé«˜æŸ¥æ‰¾æ•ˆç‡
        for ctx in self.contexts[cache_key]:
            if ctx.id in id_set:
                ctx.last_used = now
                ctx.use_count += 1
        
        self._save_user(user_id, character_id)
    
    def extract_from_text(self, text: str, user_id: str = "default",
                          character_id: str = "default", 
                          use_conversation_context: bool = True) -> List[PersistentContext]:
        """ä»æ–‡æœ¬ä¸­è‡ªåŠ¨æå–æŒä¹…ä¸Šä¸‹æ–‡
        
        ä¼˜å…ˆä½¿ç”¨ LLMï¼Œå¦‚æœæ²¡æœ‰ LLM åˆ™ä½¿ç”¨è§„åˆ™ã€‚
        å¦‚æœå¯ç”¨äº† memory_provider ä¸” use_conversation_context=Trueï¼Œ
        ä¼šè‡ªåŠ¨è·å–æœ€è¿‘çš„å¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¸®åŠ© LLM æ›´å¥½åœ°ç†è§£å’Œæå–æ¡ä»¶ã€‚
        
        Args:
            text: å½“å‰æ¶ˆæ¯æ–‡æœ¬
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            use_conversation_context: æ˜¯å¦è·å–å¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆé»˜è®¤Trueï¼‰
        """
        text_preview = text[:60].replace('\n', ' ') if len(text) > 60 else text.replace('\n', ' ')
        _safe_print(f"[ContextTracker] ğŸ” å¼€å§‹æå–: user={user_id}, char={character_id}")
        _safe_print(f"[ContextTracker]    æ–‡æœ¬({len(text)}å­—): {text_preview}{'...' if len(text) > 60 else ''}")
        
        # è·å–å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœé…ç½®äº† memory_providerï¼‰
        conversation_context = ""
        if use_conversation_context and self.llm_client and self._memory_provider:
            conversation_context = self._get_conversation_context(user_id, character_id, max_turns=10)
            if conversation_context:
                _safe_print(f"[ContextTracker]    ğŸ“œ è·å–åˆ°å¯¹è¯ä¸Šä¸‹æ–‡: {len(conversation_context)} å­—ç¬¦")
        
        _safe_print(f"[ContextTracker]    æ¨¡å¼: {'LLM' if self.llm_client else 'è§„åˆ™'}")
        
        if self.llm_client:
            # ä¼ å…¥å¯¹è¯ä¸Šä¸‹æ–‡ä»¥å¸®åŠ© LLM æ›´å¥½åœ°ç†è§£
            result = self._extract_with_llm(text, user_id, character_id, conversation_context)
        else:
            result = self._extract_with_rules(text, user_id, character_id)
        
        if result:
            _safe_print(f"[ContextTracker] âœ… æå–å®Œæˆ: æ–°å¢ {len(result)} æ¡æ¡ä»¶")
            for ctx in result:
                _safe_print(f"[ContextTracker]    ğŸŒ± [{ctx.context_type.value}] {ctx.content[:50]}{'...' if len(ctx.content) > 50 else ''}")
        else:
            _safe_print(f"[ContextTracker] â­ï¸ æå–å®Œæˆ: æœªå‘ç°æ–°æ¡ä»¶")
        
        return result
    
    def _extract_with_rules(self, text: str, user_id: str,
                            character_id: str = "default") -> List[PersistentContext]:
        """ä½¿ç”¨è§„åˆ™æå–"""
        extracted = []
        seen_contents = set()  # é¿å…é‡å¤
        
        for context_type, patterns in self.detection_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    # å°è¯•è·å–æ•è·ç»„çš„å†…å®¹ï¼Œå¦åˆ™ç”¨æ•´ä¸ªåŒ¹é…
                    content = match.group(0).strip()
                    
                    # æ¸…ç†å†…å®¹ï¼šå»é™¤é¦–å°¾æ ‡ç‚¹
                    content = content.strip('ï¼Œã€‚,. ')
                    
                    # è·³è¿‡å¤ªçŸ­æˆ–é‡å¤çš„å†…å®¹
                    if len(content) < self.min_content_length:
                        continue
                    
                    content_key = content.lower()
                    if content_key in seen_contents:
                        continue
                    seen_contents.add(content_key)
                    
                    # å¯¹äºç¯å¢ƒç±»å‹ï¼Œç‰¹æ®Šå¤„ç†ï¼šå®Œæ•´è®°å½•å¼€å‘å’Œéƒ¨ç½²ç¯å¢ƒ
                    if context_type == ContextType.ENVIRONMENT:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯"Windowså¼€å‘Ubuntuéƒ¨ç½²"è¿™ç§æ¨¡å¼
                        env_match = re.search(
                            r'(Windows|Linux|Mac|Ubuntu)(?:ä¸Š)?å¼€å‘.{0,10}?(Windows|Linux|Mac|Ubuntu)(?:ä¸Š)?éƒ¨ç½²',
                            text, re.IGNORECASE
                        )
                        if env_match:
                            content = f"å¼€å‘ç¯å¢ƒ: {env_match.group(1)}, éƒ¨ç½²ç¯å¢ƒ: {env_match.group(2)}"
                    
                    ctx = self.add(
                        content=content,
                        context_type=context_type,
                        user_id=user_id,
                        character_id=character_id
                    )
                    extracted.append(ctx)
        
        return extracted
    
    def _extract_with_llm(self, text: str, user_id: str,
                          character_id: str = "default",
                          conversation_context: str = "") -> List[PersistentContext]:
        """ä½¿ç”¨ LLM æå–
        
        Args:
            text: å½“å‰æ¶ˆæ¯æ–‡æœ¬
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            conversation_context: å¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼Œå¸®åŠ© LLM æ›´å¥½ç†è§£ï¼‰
        """
        try:
            _safe_print(f"[ContextTracker] ğŸ¤– è°ƒç”¨ LLM æå–æ¡ä»¶...")
            
            # æ„å»ºæå–å†…å®¹ï¼šå¦‚æœæœ‰å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œåˆ™åˆå¹¶
            if conversation_context:
                # ä½¿ç”¨å¯¹è¯ä¸Šä¸‹æ–‡ + å½“å‰æ¶ˆæ¯
                extract_content = f"""ã€æœ€è¿‘å¯¹è¯å†å²ã€‘
{conversation_context}

ã€å½“å‰æ¶ˆæ¯ã€‘
{text}"""
                _safe_print(f"[ContextTracker]    ğŸ“ æå–å†…å®¹: å¯¹è¯å†å² + å½“å‰æ¶ˆæ¯ = {len(extract_content)} å­—ç¬¦")
            else:
                extract_content = text
            
            prompt = self.extraction_prompt.format(content=extract_content)
            response = self.llm_client.complete(prompt, max_tokens=800)
            _safe_print(f"[ContextTracker]    LLM å“åº”: {len(response)} å­—ç¬¦")
            
            # è§£æ JSON
            import json
            # å°è¯•æ‰¾åˆ° JSON æ•°ç»„
            json_match = re.search(r'\[[\s\S]*\]', response)
            if json_match:
                items = json.loads(json_match.group(0))
                _safe_print(f"[ContextTracker]    è§£æåˆ° {len(items)} æ¡å€™é€‰æ¡ä»¶")
                
                extracted = []
                for item in items:
                    try:
                        ctx = self.add(
                            content=item['content'],
                            context_type=ContextType(item['type']),
                            user_id=user_id,
                            character_id=character_id,
                            keywords=item.get('keywords', [])
                        )
                        extracted.append(ctx)
                    except (KeyError, ValueError) as e:
                        _safe_print(f"[ContextTracker]    âš ï¸ è·³è¿‡æ— æ•ˆæ¡ä»¶: {e}")
                        continue
                
                return extracted
            else:
                _safe_print(f"[ContextTracker]    âš ï¸ LLM å“åº”ä¸­æœªæ‰¾åˆ° JSON æ•°ç»„")
        except Exception as e:
            _safe_print(f"[ContextTracker] âŒ LLMæå–å¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™: {e}")
        
        # å›é€€åˆ°è§„åˆ™æå–
        return self._extract_with_rules(text, user_id, character_id)
    
    def format_for_prompt(self, user_id: str = "default", character_id: str = "default") -> str:
        """æ ¼å¼åŒ–ä¸ºæç¤ºè¯æ³¨å…¥
        
        æ³¨æ„ï¼šæœ¬æ–¹æ³•ä¸å†è‡ªåŠ¨è°ƒç”¨ _enforce_limits()ï¼Œé¿å…é‡å¤è¡°å‡æ£€æŸ¥ã€‚
        è°ƒç”¨æ–¹ï¼ˆå¦‚ engine.build_contextï¼‰åº”è´Ÿè´£åœ¨é€‚å½“æ—¶æœºè°ƒç”¨è¡°å‡æ£€æŸ¥ã€‚
        """
        active = self.get_active(user_id, character_id)
        if not active:
            return ""
        
        lines = ["<persistent_context>", "ã€æŒä¹…å‰ææ¡ä»¶ã€‘ä»¥ä¸‹æ˜¯å·²ç¡®ç«‹çš„èƒŒæ™¯è®¾å®šï¼Œè¯·åœ¨æ‰€æœ‰å›å¤ä¸­é»˜è®¤éµå®ˆï¼š"]
        
        # æŒ‰ç±»å‹åˆ†ç»„
        by_type: Dict[ContextType, List[PersistentContext]] = {}
        for ctx in active:
            if ctx.context_type not in by_type:
                by_type[ctx.context_type] = []
            by_type[ctx.context_type].append(ctx)
        
        type_names = {
            ContextType.USER_IDENTITY: "ç”¨æˆ·èº«ä»½",
            ContextType.USER_GOAL: "ç”¨æˆ·ç›®æ ‡",
            ContextType.USER_PREFERENCE: "ç”¨æˆ·åå¥½",
            ContextType.ENVIRONMENT: "æŠ€æœ¯ç¯å¢ƒ",
            ContextType.PROJECT: "é¡¹ç›®ä¿¡æ¯",
            ContextType.TIME_CONSTRAINT: "æ—¶é—´çº¦æŸ",
            ContextType.CHARACTER_TRAIT: "è§’è‰²ç‰¹å¾",
            ContextType.WORLD_SETTING: "ä¸–ç•Œè§‚",
            ContextType.RELATIONSHIP: "å…³ç³»è®¾å®š",
            ContextType.EMOTIONAL_STATE: "æƒ…ç»ªçŠ¶æ€",
            ContextType.SKILL_ABILITY: "æŠ€èƒ½èƒ½åŠ›",
            ContextType.ITEM_PROP: "ç‰©å“é“å…·",
            ContextType.ASSUMPTION: "å‡è®¾å‰æ",
            ContextType.CONSTRAINT: "çº¦æŸæ¡ä»¶",
            ContextType.CUSTOM: "å…¶ä»–",
        }
        
        for ctx_type, contexts in by_type.items():
            type_name = type_names.get(ctx_type, ctx_type.value)
            lines.append(f"\n[{type_name}]")
            for ctx in contexts:
                lines.append(f"â€¢ {ctx.content}")
        
        lines.append("</persistent_context>")
        return "\n".join(lines)
    
    def get_relevant(self, query: str, user_id: str = "default", character_id: str = "default",
                     top_k: int = 5) -> List[PersistentContext]:
        """è·å–ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡"""
        active = self.get_active(user_id, character_id)
        if not active:
            return []
        
        query_lower = query.lower()
        scored = []
        
        for ctx in active:
            score = 0
            
            # å…³é”®è¯åŒ¹é…
            for kw in ctx.keywords:
                if kw.lower() in query_lower:
                    score += 0.3
            
            # å†…å®¹åŒ¹é…
            if any(word in ctx.content.lower() for word in query_lower.split()):
                score += 0.2
            
            # å®ä½“åŒ¹é…
            for entity in ctx.related_entities:
                if entity.lower() in query_lower:
                    score += 0.4
            
            # åŸºç¡€åˆ†ï¼ˆæ‰€æœ‰æ´»è·ƒæ¡ä»¶éƒ½æœ‰ä¸€å®šç›¸å…³æ€§ï¼‰
            score += 0.1
            
            scored.append((ctx, score))
        
        # æ’åºå¹¶è¿”å›
        scored.sort(key=lambda x: -x[1])
        return [ctx for ctx, score in scored[:top_k]]
    
    def consolidate_contexts(self, user_id: str = "default", character_id: str = "default",
                             force: bool = False) -> int:
        """æ™ºèƒ½å‹ç¼©åˆå¹¶ç›¸ä¼¼çš„æŒä¹…æ¡ä»¶
        
        å½“æ¡ä»¶æ•°é‡è¶…è¿‡é˜ˆå€¼æˆ–å¼ºåˆ¶æ‰§è¡Œæ—¶ï¼Œä½¿ç”¨LLMå°†ç›¸ä¼¼æ¡ä»¶åˆå¹¶
        
        Returns:
            å‡å°‘çš„æ¡ä»¶æ•°é‡
        """
        # ç¡®ä¿æ•°æ®å·²åŠ è½½
        self._ensure_loaded(user_id, character_id)
        
        cache_key = self._get_cache_key(user_id, character_id)
        
        if cache_key not in self.contexts:
            return 0
        
        contexts = [c for c in self.contexts[cache_key] if c.is_active]
        original_count = len(contexts)
        
        # åªæœ‰æ¡ä»¶æ•°é‡è¶…è¿‡é˜ˆå€¼æˆ–å¼ºåˆ¶æ‰§è¡Œæ—¶æ‰å‹ç¼©
        if not force and original_count < self.MAX_TOTAL * 0.8:
            return 0
        
        # æŒ‰ç±»å‹åˆ†ç»„
        by_type: Dict[ContextType, List[PersistentContext]] = {}
        for ctx in contexts:
            if ctx.context_type not in by_type:
                by_type[ctx.context_type] = []
            by_type[ctx.context_type].append(ctx)
        
        # å¯¹æ¯ä¸ªç±»å‹è¿›è¡Œå‹ç¼©
        for ctx_type, type_contexts in by_type.items():
            if len(type_contexts) <= 2:  # 2ä¸ªä»¥ä¸‹ä¸å‹ç¼©
                continue
            
            if self.llm_client:
                # ä½¿ç”¨ LLM å‹ç¼©
                self._consolidate_with_llm(type_contexts, ctx_type, user_id, character_id)
            else:
                # ç®€å•å‹ç¼©ï¼šä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„å‡ ä¸ª
                type_contexts.sort(key=lambda c: -c.confidence)
                for ctx in type_contexts[self.MAX_PER_TYPE:]:
                    ctx.is_active = False
        
        self._save_user(user_id, character_id)
        
        # è®¡ç®—å‡å°‘çš„æ•°é‡
        new_count = len([c for c in self.contexts[cache_key] if c.is_active])
        return original_count - new_count
    
    def _consolidate_with_llm(self, contexts: List[PersistentContext], 
                              ctx_type: ContextType, user_id: str,
                              character_id: str = "default"):
        """ä½¿ç”¨ LLM å‹ç¼©åˆå¹¶åŒç±»å‹çš„æ¡ä»¶"""
        try:
            # æ„å»ºæ¡ä»¶åˆ—è¡¨
            conditions = "\n".join([f"- {ctx.content}" for ctx in contexts])
            
            prompt = f"""ä½ éœ€è¦å°†ä»¥ä¸‹åŒç±»å‹çš„æ¡ä»¶åˆå¹¶å‹ç¼©ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼Œå»é™¤å†—ä½™ã€‚

æ¡ä»¶ç±»å‹: {ctx_type.value}
å½“å‰æ¡ä»¶ï¼š
{conditions}

è¯·å°†è¿™äº›æ¡ä»¶åˆå¹¶æˆä¸è¶…è¿‡{self.MAX_PER_TYPE}æ¡æ ¸å¿ƒæ¡ä»¶ã€‚
è¾“å‡ºæ ¼å¼ï¼ˆJSONæ•°ç»„ï¼‰:
[
  "åˆå¹¶åçš„æ¡ä»¶1",
  "åˆå¹¶åçš„æ¡ä»¶2"
]

åªè¾“å‡ºJSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
            
            response = self.llm_client.complete(prompt, max_tokens=300)
            
            # è§£æç»“æœ
            json_match = re.search(r'\[[\s\S]*\]', response)
            if json_match:
                merged = json.loads(json_match.group(0))
                
                # è¿‡æ»¤æœ‰æ•ˆå†…å®¹
                valid_contents = [c.strip() for c in merged[:self.MAX_PER_TYPE] if c and c.strip()]
                
                # å¦‚æœ LLM è¿”å›ç©ºç»“æœï¼Œä½¿ç”¨å›é€€ç­–ç•¥
                if not valid_contents:
                    raise ValueError("LLM è¿”å›ç©ºç»“æœ")
                
                # åœç”¨æ—§æ¡ä»¶ï¼ˆåªåœ¨ç¡®è®¤æœ‰æ–°æ¡ä»¶åå†åœç”¨ï¼‰
                for ctx in contexts:
                    ctx.is_active = False
                
                # æ·»åŠ åˆå¹¶åçš„æ¡ä»¶
                for content in valid_contents:
                    self.add(
                        content=content,
                        context_type=ctx_type,
                        user_id=user_id,
                        character_id=character_id
                    )
            else:
                raise ValueError("LLM å“åº”æ ¼å¼æ— æ•ˆ")
                        
        except Exception as e:
            _safe_print(f"[ContextTracker] LLMå‹ç¼©å¤±è´¥: {e}")
            # å›é€€ï¼šç®€å•ä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„
            contexts.sort(key=lambda c: -c.confidence)
            for ctx in contexts[self.MAX_PER_TYPE:]:
                ctx.is_active = False
    
    # get_by_id æ˜¯ get_context_by_id çš„åˆ«åï¼Œä¿æŒAPIä¸€è‡´æ€§
    def get_by_id(self, context_id: str, user_id: str = "default", 
                  character_id: str = "default") -> Optional[PersistentContext]:
        """get_context_by_id çš„åˆ«åï¼Œä¿æŒä¸ ForeshadowingTracker çš„ API ä¸€è‡´"""
        return self.get_context_by_id(context_id, user_id, character_id)
    
    def get_stats(self, user_id: str = "default", character_id: str = "default") -> Dict[str, Any]:
        """è·å–æŒä¹…æ¡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯"""
        # ç¡®ä¿æ•°æ®å·²åŠ è½½
        self._ensure_loaded(user_id, character_id)
        
        cache_key = self._get_cache_key(user_id, character_id)
        
        if cache_key not in self.contexts:
            return {
                "total": 0,
                "active": 0,
                "by_type": {},
                "avg_confidence": 0.0,
                "oldest_days": 0
            }
        
        contexts = self.contexts[cache_key]
        active = [c for c in contexts if c.is_active]
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        by_type = {}
        for ctx in active:
            type_name = ctx.context_type.value
            if type_name not in by_type:
                by_type[type_name] = 0
            by_type[type_name] += 1
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_conf = sum(c.confidence for c in active) / len(active) if active else 0.0
        
        # è®¡ç®—æœ€è€çš„æ¡ä»¶
        now = time.time()
        oldest_days = 0
        if active:
            oldest = min(c.created_at for c in active)
            oldest_days = (now - oldest) / (24 * 3600)
        
        return {
            "total": len(contexts),
            "active": len(active),
            "by_type": by_type,
            "avg_confidence": round(avg_conf, 2),
            "oldest_days": round(oldest_days, 1),
            "limits": {
                "max_per_type": self.MAX_PER_TYPE,
                "max_total": self.MAX_TOTAL
            }
        }
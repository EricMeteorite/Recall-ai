"""æ™ºèƒ½æŠ½å–å™¨ - Recall 4.0 Phase 2

è®¾è®¡ç†å¿µï¼š
1. ä¸‰æ¨¡å¼è‡ªé€‚åº”ï¼šLocalï¼ˆçº¯æœ¬åœ°ï¼‰ã€Hybridï¼ˆæ··åˆï¼‰ã€LLMï¼ˆçº¯ LLMï¼‰
2. å¤æ‚åº¦è¯„ä¼°ï¼šæ ¹æ®æ–‡æœ¬å¤æ‚åº¦è‡ªåŠ¨é€‰æ‹©ç­–ç•¥
3. æˆæœ¬æ§åˆ¶ï¼šé›†æˆé¢„ç®—ç®¡ç†ï¼Œé¿å…è¶…æ”¯
4. å‘åå…¼å®¹ï¼šä¸ç ´åç°æœ‰ EntityExtractor åŠŸèƒ½
"""

from __future__ import annotations

import os
import re
import time
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Any, Optional, Tuple, Union
from enum import Enum

from .entity_extractor import EntityExtractor, ExtractedEntity
from ..utils.llm_client import LLMClient
from ..utils.budget_manager import BudgetManager, BudgetConfig


# Windows GBK ç¼–ç å…¼å®¹çš„å®‰å…¨æ‰“å°å‡½æ•°
def _safe_print(msg: str) -> None:
    """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œæ›¿æ¢ emoji ä¸º ASCII ç­‰ä»·ç‰©ä»¥é¿å… Windows GBK ç¼–ç é”™è¯¯"""
    emoji_map = {
        'ğŸ“¥': '[IN]', 'ğŸ“¤': '[OUT]', 'ğŸ”': '[SEARCH]', 'âœ…': '[OK]', 'âŒ': '[FAIL]',
        'âš ï¸': '[WARN]', 'ğŸ’¾': '[SAVE]', 'ğŸ—ƒï¸': '[DB]', 'ğŸ§¹': '[CLEAN]', 'ğŸ“Š': '[STATS]',
        'ğŸ”„': '[SYNC]', 'ğŸ“¦': '[PKG]', 'ğŸš€': '[START]', 'ğŸ¯': '[TARGET]', 'ğŸ’¡': '[HINT]',
        'ğŸ”§': '[FIX]', 'ğŸ“': '[NOTE]', 'ğŸ‰': '[DONE]', 'â±ï¸': '[TIME]', 'ğŸŒ': '[NET]',
        'ğŸ§ ': '[BRAIN]', 'ğŸ’¬': '[CHAT]', 'ğŸ·ï¸': '[TAG]', 'ğŸ“': '[DIR]', 'ğŸ”’': '[LOCK]',
        'ğŸŒ±': '[PLANT]', 'ğŸ—‘ï¸': '[DEL]', 'ğŸ’«': '[MAGIC]', 'ğŸ­': '[MASK]', 'ğŸ“–': '[BOOK]',
        'âš¡': '[FAST]', 'ğŸ”¥': '[HOT]', 'ğŸ’': '[GEM]', 'ğŸŒŸ': '[STAR]', 'ğŸ¨': '[ART]'
    }
    for emoji, ascii_equiv in emoji_map.items():
        msg = msg.replace(emoji, ascii_equiv)
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode('ascii', errors='replace').decode('ascii'))


class ExtractionMode(str, Enum):
    """æŠ½å–æ¨¡å¼"""
    RULES = "rules"           # çº¯è§„åˆ™ï¼šspaCy + jieba + è§„åˆ™
    ADAPTIVE = "adaptive"     # è‡ªé€‚åº”ï¼šè§„åˆ™åˆç­› + LLM ç²¾ç‚¼ï¼ˆé»˜è®¤ï¼‰
    LLM = "llm"               # çº¯ LLMï¼šæœ€é«˜è´¨é‡
    
    @classmethod
    def _missing_(cls, value):
        """å‘åå…¼å®¹ï¼šæ˜ å°„æ—§å€¼åˆ°æ–°å€¼"""
        legacy_map = {
            'local': cls.RULES,
            'hybrid': cls.ADAPTIVE,
            'llm_full': cls.LLM,
        }
        if isinstance(value, str):
            return legacy_map.get(value.lower())
        return None


# å‘åå…¼å®¹åˆ«åï¼ˆæ”¯æŒ ExtractionMode.LOCAL ç­‰æ—§ç”¨æ³•ï¼‰
ExtractionMode.LOCAL = ExtractionMode.RULES
ExtractionMode.HYBRID = ExtractionMode.ADAPTIVE
ExtractionMode.LLM_FULL = ExtractionMode.LLM


@dataclass
class ExtractedRelation:
    """æŠ½å–çš„å…³ç³»"""
    subject: str
    predicate: str
    object: str
    confidence: float = 0.5
    source_text: str = ""
    temporal_info: Optional[str] = None  # æ—¶æ€ä¿¡æ¯ï¼ˆå¦‚"ä»2023å¹´å¼€å§‹"ï¼‰


@dataclass
class ExtractionResult:
    """æŠ½å–ç»“æœ"""
    entities: List[ExtractedEntity]
    relations: List[ExtractedRelation]
    temporal_markers: List[Dict[str, Any]]  # æ—¶æ€æ ‡è®°
    keywords: List[str]
    
    # å…ƒä¿¡æ¯
    mode_used: ExtractionMode = ExtractionMode.RULES
    complexity_score: float = 0.0
    llm_used: bool = False
    cost: float = 0.0
    latency_ms: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'entities': [
                {
                    'name': e.name,
                    'entity_type': e.entity_type,
                    'confidence': e.confidence,
                    'source_text': e.source_text
                }
                for e in self.entities
            ],
            'relations': [asdict(r) for r in self.relations],
            'temporal_markers': self.temporal_markers,
            'keywords': self.keywords,
            'mode_used': self.mode_used.value,
            'complexity_score': self.complexity_score,
            'llm_used': self.llm_used,
            'cost': self.cost,
            'latency_ms': self.latency_ms
        }


@dataclass
class SmartExtractorConfig:
    """æ™ºèƒ½æŠ½å–å™¨é…ç½®"""
    mode: ExtractionMode = ExtractionMode.ADAPTIVE
    complexity_threshold: float = 0.6       # å¤æ‚åº¦é˜ˆå€¼ï¼ˆè¶…è¿‡æ­¤å€¼ä½¿ç”¨ LLMï¼‰
    max_text_length: int = 10000            # æœ€å¤§å¤„ç†æ–‡æœ¬é•¿åº¦
    enable_relation_extraction: bool = True # æ˜¯å¦æŠ½å–å…³ç³»
    enable_temporal_detection: bool = True  # æ˜¯å¦æ£€æµ‹æ—¶æ€
    
    @classmethod
    def rules_only(cls) -> 'SmartExtractorConfig':
        """çº¯è§„åˆ™æ¨¡å¼é…ç½®"""
        return cls(mode=ExtractionMode.RULES)
    
    @classmethod
    def adaptive(cls, threshold: float = 0.6) -> 'SmartExtractorConfig':
        """è‡ªé€‚åº”æ¨¡å¼é…ç½®"""
        return cls(mode=ExtractionMode.ADAPTIVE, complexity_threshold=threshold)
    
    @classmethod
    def llm(cls) -> 'SmartExtractorConfig':
        """çº¯ LLM æ¨¡å¼é…ç½®"""
        return cls(mode=ExtractionMode.LLM)
    
    # å‘åå…¼å®¹åˆ«å
    @classmethod
    def local_only(cls) -> 'SmartExtractorConfig':
        """[å·²å¼ƒç”¨] è¯·ä½¿ç”¨ rules_only()"""
        return cls.rules_only()
    
    @classmethod
    def hybrid(cls, threshold: float = 0.6) -> 'SmartExtractorConfig':
        """[å·²å¼ƒç”¨] è¯·ä½¿ç”¨ adaptive()"""
        return cls.adaptive(threshold)
    
    @classmethod
    def llm_full(cls) -> 'SmartExtractorConfig':
        """[å·²å¼ƒç”¨] è¯·ä½¿ç”¨ llm()"""
        return cls.llm()


class SmartExtractor:
    """æ™ºèƒ½æŠ½å–å™¨ - ä¸‰æ¨¡å¼è‡ªé€‚åº”
    
    åŠŸèƒ½ï¼š
    1. å®ä½“æŠ½å–ï¼ˆLocal/LLMï¼‰
    2. å…³ç³»æŠ½å–ï¼ˆLocal/LLMï¼‰
    3. æ—¶æ€æ£€æµ‹ï¼ˆè§„åˆ™ + LLMï¼‰
    4. å¤æ‚åº¦è¯„ä¼°
    5. æˆæœ¬æ§åˆ¶
    
    ä½¿ç”¨æ–¹å¼ï¼š
        extractor = SmartExtractor(llm_client=llm_client)
        result = extractor.extract("Alice joined OpenAI in 2023")
    """
    
    # æ—¶æ€å…³é”®è¯ï¼ˆç”¨äºè§„åˆ™æ£€æµ‹ï¼‰
    TEMPORAL_KEYWORDS_ZH = {
        'ä»', 'è‡ªä»', 'å¼€å§‹', 'ç»“æŸ', 'ç›´åˆ°', 'ä¹‹å‰', 'ä¹‹å', 'æœŸé—´',
        'å¹´', 'æœˆ', 'æ—¥', 'ä»Šå¤©', 'æ˜¨å¤©', 'æ˜å¤©', 'ç°åœ¨', 'ä»¥å‰', 'ä»¥å',
        'ä¸€ç›´', 'å·²ç»', 'æ›¾ç»', 'å°†è¦', 'æ­£åœ¨'
    }
    
    TEMPORAL_KEYWORDS_EN = {
        'from', 'since', 'until', 'before', 'after', 'during', 'when',
        'started', 'ended', 'began', 'finished', 'now', 'currently',
        'previously', 'formerly', 'recently', 'already', 'still'
    }
    
    # å…³ç³»åŠ¨è¯æ¨¡å¼ï¼ˆç”¨äºè§„åˆ™æŠ½å–ï¼‰
    RELATION_PATTERNS = [
        # ä¸­æ–‡
        (r'(.{2,10})æ˜¯(.{2,20})çš„(.{2,10})', 'is_a'),
        (r'(.{2,10})åœ¨(.{2,20})å·¥ä½œ', 'works_at'),
        (r'(.{2,10})ä½åœ¨(.{2,20})', 'lives_in'),
        (r'(.{2,10})å–œæ¬¢(.{2,20})', 'likes'),
        (r'(.{2,10})å’Œ(.{2,10})æ˜¯(.{2,10})', 'relationship'),
        # è‹±æ–‡
        (r'(\w+)\s+is\s+(?:a|an|the)\s+(\w+)', 'is_a'),
        (r'(\w+)\s+works?\s+(?:at|for)\s+(\w+)', 'works_at'),
        (r'(\w+)\s+lives?\s+in\s+(\w+)', 'lives_in'),
        (r'(\w+)\s+(?:likes?|loves?)\s+(\w+)', 'likes'),
    ]
    
    # LLM æŠ½å– Prompt
    EXTRACTION_PROMPT = '''è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–å®ä½“ã€å…³ç³»å’Œæ—¶æ€ä¿¡æ¯ã€‚

æ–‡æœ¬ï¼š
{text}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
  "entities": [
    {{"name": "å®ä½“å", "type": "PERSON/ORG/LOCATION/ITEM/CONCEPT", "confidence": 0.9}}
  ],
  "relations": [
    {{"subject": "ä¸»ä½“", "predicate": "å…³ç³»ç±»å‹", "object": "å®¢ä½“", "temporal": "æ—¶æ€ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰"}}
  ],
  "temporal_markers": [
    {{"text": "åŸæ–‡ç‰‡æ®µ", "type": "START/END/DURATION/POINT", "normalized": "æ ‡å‡†åŒ–æ—¶é—´"}}
  ]
}}

æ³¨æ„ï¼š
1. åªæŠ½å–æ˜ç¡®æåˆ°çš„å®ä½“å’Œå…³ç³»
2. æ—¶æ€ä¿¡æ¯åŒ…æ‹¬ï¼šå¼€å§‹æ—¶é—´ã€ç»“æŸæ—¶é—´ã€æŒç»­æ—¶é—´ã€æ—¶é—´ç‚¹
3. ç½®ä¿¡åº¦åæ˜ æŠ½å–çš„ç¡®å®šæ€§
4. å¦‚æœæ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå¯¹åº”æ•°ç»„ä¸ºç©º'''

    def __init__(
        self,
        config: Optional[SmartExtractorConfig] = None,
        local_extractor: Optional[EntityExtractor] = None,
        llm_client: Optional[LLMClient] = None,
        budget_manager: Optional[BudgetManager] = None
    ):
        """åˆå§‹åŒ–æ™ºèƒ½æŠ½å–å™¨
        
        Args:
            config: æŠ½å–å™¨é…ç½®
            local_extractor: æœ¬åœ°å®ä½“æŠ½å–å™¨ï¼ˆå¤ç”¨ç°æœ‰ï¼‰
            llm_client: LLM å®¢æˆ·ç«¯
            budget_manager: é¢„ç®—ç®¡ç†å™¨
        """
        self.config = config or SmartExtractorConfig()
        self.local_extractor = local_extractor or EntityExtractor()
        self.llm_client = llm_client
        self.budget_manager = budget_manager
        
        # ç¼–è¯‘å…³ç³»æ¨¡å¼
        self._relation_patterns = [
            (re.compile(pattern, re.IGNORECASE), rel_type)
            for pattern, rel_type in self.RELATION_PATTERNS
        ]
    
    def extract(
        self,
        text: str,
        context: Optional[Dict[str, Any]] = None,
        force_mode: Optional[ExtractionMode] = None
    ) -> ExtractionResult:
        """æ‰§è¡Œæ™ºèƒ½æŠ½å–
        
        Args:
            text: è¦æŠ½å–çš„æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            force_mode: å¼ºåˆ¶ä½¿ç”¨çš„æ¨¡å¼ï¼ˆå¿½ç•¥è‡ªé€‚åº”ï¼‰
            
        Returns:
            ExtractionResult: æŠ½å–ç»“æœ
        """
        start_time = time.time()
        
        # æ–‡æœ¬é¢„å¤„ç†
        text = self._preprocess(text)
        if not text:
            return ExtractionResult(
                entities=[], relations=[], temporal_markers=[], keywords=[],
                mode_used=ExtractionMode.RULES, latency_ms=0
            )
        
        # ç¡®å®šä½¿ç”¨çš„æ¨¡å¼
        mode = force_mode or self.config.mode
        complexity = 0.0
        
        # 1. å§‹ç»ˆæ‰§è¡Œæœ¬åœ°æŠ½å–ï¼ˆå…è´¹ã€å¿«é€Ÿï¼‰
        local_result = self._local_extract(text)
        
        # å¦‚æœæ˜¯çº¯æœ¬åœ°æ¨¡å¼ï¼Œç›´æ¥è¿”å›
        if mode == ExtractionMode.RULES:
            local_result.mode_used = ExtractionMode.RULES
            local_result.latency_ms = (time.time() - start_time) * 1000
            return local_result
        
        # 2. è¯„ä¼°å¤æ‚åº¦
        complexity = self._assess_complexity(text, local_result)
        local_result.complexity_score = complexity
        
        # 3. å†³ç­–æ˜¯å¦éœ€è¦ LLM
        need_llm = (
            mode == ExtractionMode.LLM or
            (mode == ExtractionMode.ADAPTIVE and complexity >= self.config.complexity_threshold)
        )
        
        # 4. æ£€æŸ¥é¢„ç®—
        if need_llm and self.budget_manager:
            # ä¼°ç®— LLM è°ƒç”¨æˆæœ¬
            estimated_tokens = len(text) // 4 + 200  # ç²—ç•¥ä¼°ç®—
            estimated_cost = self.budget_manager.estimate_cost(
                tokens_in=estimated_tokens,
                tokens_out=200
            )
            if not self.budget_manager.can_afford(estimated_cost, operation="extraction"):
                # é¢„ç®—ä¸è¶³ï¼Œé™çº§åˆ°æœ¬åœ°æ¨¡å¼
                need_llm = False
                _safe_print("[SmartExtractor] é¢„ç®—ä¸è¶³ï¼Œé™çº§åˆ°æœ¬åœ°æ¨¡å¼")
        
        # 5. æ‰§è¡Œ LLM æŠ½å–ï¼ˆå¦‚æœéœ€è¦ä¸”å¯ç”¨ï¼‰
        if need_llm and self.llm_client:
            llm_result = self._llm_extract(text, local_result, context)
            if llm_result:
                # åˆå¹¶ç»“æœ
                merged = self._merge_results(local_result, llm_result)
                merged.mode_used = mode
                merged.llm_used = True
                merged.latency_ms = (time.time() - start_time) * 1000
                return merged
        
        # è¿”å›æœ¬åœ°ç»“æœ
        local_result.mode_used = mode
        local_result.latency_ms = (time.time() - start_time) * 1000
        return local_result
    
    def _preprocess(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        if not text:
            return ""
        
        # æˆªæ–­è¿‡é•¿æ–‡æœ¬
        if len(text) > self.config.max_text_length:
            text = text[:self.config.max_text_length]
        
        # æ¸…ç†å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _local_extract(self, text: str) -> ExtractionResult:
        """æœ¬åœ°æŠ½å–ï¼ˆä½¿ç”¨ç°æœ‰ EntityExtractor + è§„åˆ™ï¼‰"""
        # å®ä½“æŠ½å–
        entities = self.local_extractor.extract(text)
        
        # å…³é”®è¯æŠ½å–
        keywords = self.local_extractor.extract_keywords(text)
        
        # å…³ç³»æŠ½å–ï¼ˆè§„åˆ™åŒ¹é…ï¼‰
        relations = []
        if self.config.enable_relation_extraction:
            relations = self._extract_relations_by_rules(text, entities)
        
        # æ—¶æ€æ£€æµ‹ï¼ˆè§„åˆ™åŒ¹é…ï¼‰
        temporal_markers = []
        if self.config.enable_temporal_detection:
            temporal_markers = self._detect_temporal_markers(text)
        
        return ExtractionResult(
            entities=entities,
            relations=relations,
            temporal_markers=temporal_markers,
            keywords=keywords
        )
    
    def _extract_relations_by_rules(
        self,
        text: str,
        entities: List[ExtractedEntity]
    ) -> List[ExtractedRelation]:
        """ä½¿ç”¨è§„åˆ™æŠ½å–å…³ç³»"""
        relations = []
        entity_names = {e.name.lower() for e in entities}
        
        for pattern, rel_type in self._relation_patterns:
            for match in pattern.finditer(text):
                groups = match.groups()
                if len(groups) >= 2:
                    subject = groups[0].strip()
                    obj = groups[1].strip() if len(groups) > 1 else ""
                    
                    # éªŒè¯ä¸»ä½“å’Œå®¢ä½“æ˜¯å¦ä¸ºå·²çŸ¥å®ä½“
                    # è¿™é‡Œæ”¾å®½æ¡ä»¶ï¼Œåªè¦æœ‰ä¸€ä¸ªæ˜¯å·²çŸ¥å®ä½“å°±è®°å½•
                    if subject.lower() in entity_names or obj.lower() in entity_names:
                        relations.append(ExtractedRelation(
                            subject=subject,
                            predicate=rel_type,
                            object=obj,
                            confidence=0.6,
                            source_text=match.group(0)
                        ))
        
        return relations
    
    def _detect_temporal_markers(self, text: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹æ—¶æ€æ ‡è®°"""
        markers = []
        
        # ä¸­æ–‡æ—¥æœŸæ¨¡å¼
        zh_date_patterns = [
            (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥?', 'POINT'),
            (r'(\d{4})å¹´(\d{1,2})æœˆ', 'POINT'),
            (r'(\d{4})å¹´', 'POINT'),
            (r'(ä»|è‡ª)(.{2,20})(å¼€å§‹|èµ·)', 'START'),
            (r'(åˆ°|è‡³|ç›´åˆ°)(.{2,20})(ç»“æŸ|ä¸ºæ­¢)?', 'END'),
            (r'(.{2,10})(æœŸé—´|ä¹‹é—´)', 'DURATION'),
        ]
        
        # è‹±æ–‡æ—¥æœŸæ¨¡å¼
        en_date_patterns = [
            (r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},?\s+\d{4}', 'POINT'),
            (r'\d{1,2}/\d{1,2}/\d{4}', 'POINT'),
            (r'\d{4}-\d{2}-\d{2}', 'POINT'),
            (r'(since|from)\s+(.{2,20})', 'START'),
            (r'(until|to|till)\s+(.{2,20})', 'END'),
            (r'(during|between)\s+(.{2,20})', 'DURATION'),
        ]
        
        all_patterns = zh_date_patterns + en_date_patterns
        
        for pattern, marker_type in all_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                markers.append({
                    'text': match.group(0),
                    'type': marker_type,
                    'start': match.start(),
                    'end': match.end()
                })
        
        # æ£€æµ‹æ—¶æ€å…³é”®è¯
        for keyword in self.TEMPORAL_KEYWORDS_ZH | self.TEMPORAL_KEYWORDS_EN:
            if keyword in text.lower():
                # æ‰¾åˆ°åŒ…å«å…³é”®è¯çš„ä¸Šä¸‹æ–‡
                idx = text.lower().find(keyword)
                context_start = max(0, idx - 10)
                context_end = min(len(text), idx + len(keyword) + 20)
                markers.append({
                    'text': text[context_start:context_end],
                    'type': 'KEYWORD',
                    'keyword': keyword,
                    'start': idx,
                    'end': idx + len(keyword)
                })
        
        # å»é‡
        seen = set()
        unique_markers = []
        for m in markers:
            key = (m['text'], m['type'])
            if key not in seen:
                seen.add(key)
                unique_markers.append(m)
        
        return unique_markers
    
    def _assess_complexity(
        self,
        text: str,
        local_result: ExtractionResult
    ) -> float:
        """è¯„ä¼°æ–‡æœ¬å¤æ‚åº¦
        
        è¿”å› 0-1 çš„åˆ†æ•°ï¼Œè¶Šé«˜è¶Šå¤æ‚ï¼Œè¶Šéœ€è¦ LLM
        """
        score = 0.0
        
        # 1. é•¿åº¦å› ç´ 
        if len(text) > 500:
            score += 0.15
        if len(text) > 1000:
            score += 0.15
        
        # 2. å®ä½“å¯†åº¦
        entity_count = len(local_result.entities)
        entity_density = entity_count / max(len(text) / 100, 1)
        if entity_density > 0.5:
            score += 0.2
        
        # 3. å…³ç³»å¤æ‚åº¦
        if entity_count > 3:
            score += 0.15
        if entity_count > 5:
            score += 0.1
        
        # 4. æ—¶æ€æ ‡è®°
        if local_result.temporal_markers:
            score += 0.15
        
        # 5. æœ¬åœ°æŠ½å–ç½®ä¿¡åº¦
        if local_result.entities:
            avg_confidence = sum(e.confidence for e in local_result.entities) / len(local_result.entities)
            if avg_confidence < 0.6:
                score += 0.2
        
        # 6. å¥å­ç»“æ„å¤æ‚åº¦ï¼ˆç®€å•å¯å‘å¼ï¼‰
        sentence_count = len(re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text))
        if sentence_count > 5:
            score += 0.1
        
        return min(1.0, score)
    
    def _llm_extract(
        self,
        text: str,
        local_result: ExtractionResult,
        context: Optional[Dict[str, Any]] = None
    ) -> Optional[ExtractionResult]:
        """ä½¿ç”¨ LLM æŠ½å–"""
        if not self.llm_client:
            return None
        
        try:
            prompt = self.EXTRACTION_PROMPT.format(text=text)
            
            response = self.llm_client.complete(
                prompt=prompt,
                max_tokens=1000,
                temperature=0.1
            )
            
            # è§£æ JSON å“åº”
            import json
            
            # å°è¯•æå– JSON éƒ¨åˆ†
            json_str = response
            if '```json' in response:
                json_str = response.split('```json')[1].split('```')[0]
            elif '```' in response:
                json_str = response.split('```')[1].split('```')[0]
            
            data = json.loads(json_str.strip())
            
            # è½¬æ¢ä¸º ExtractionResult
            entities = []
            for e in data.get('entities', []):
                entities.append(ExtractedEntity(
                    name=e.get('name', ''),
                    entity_type=e.get('type', 'UNKNOWN'),
                    confidence=e.get('confidence', 0.8),
                    source_text=text[:100]
                ))
            
            relations = []
            for r in data.get('relations', []):
                relations.append(ExtractedRelation(
                    subject=r.get('subject', ''),
                    predicate=r.get('predicate', ''),
                    object=r.get('object', ''),
                    confidence=0.8,
                    temporal_info=r.get('temporal')
                ))
            
            # è®°å½•é¢„ç®—ä½¿ç”¨
            if self.budget_manager:
                self.budget_manager.record_usage(
                    operation="extraction",
                    tokens_in=len(text) // 4,
                    tokens_out=len(response) // 4,
                    model=self.llm_client.model
                )
            
            return ExtractionResult(
                entities=entities,
                relations=relations,
                temporal_markers=data.get('temporal_markers', []),
                keywords=local_result.keywords,  # å…³é”®è¯ç”¨æœ¬åœ°ç»“æœ
                llm_used=True
            )
        
        except Exception as e:
            _safe_print(f"[SmartExtractor] LLM æŠ½å–å¤±è´¥: {e}")
            return None
    
    def _merge_results(
        self,
        local_result: ExtractionResult,
        llm_result: ExtractionResult
    ) -> ExtractionResult:
        """åˆå¹¶æœ¬åœ°å’Œ LLM æŠ½å–ç»“æœ"""
        # å®ä½“å»é‡åˆå¹¶ï¼ˆä»¥ LLM ç»“æœä¸ºä¸»ï¼Œè¡¥å……æœ¬åœ°ç»“æœï¼‰
        merged_entities = list(llm_result.entities)
        llm_entity_names = {e.name.lower() for e in llm_result.entities}
        
        for e in local_result.entities:
            if e.name.lower() not in llm_entity_names:
                # é™ä½æœ¬åœ°ç»“æœçš„ç½®ä¿¡åº¦
                e.confidence *= 0.8
                merged_entities.append(e)
        
        # å…³ç³»åˆå¹¶
        merged_relations = list(llm_result.relations)
        llm_rel_keys = {(r.subject.lower(), r.predicate, r.object.lower()) for r in llm_result.relations}
        
        for r in local_result.relations:
            key = (r.subject.lower(), r.predicate, r.object.lower())
            if key not in llm_rel_keys:
                r.confidence *= 0.7
                merged_relations.append(r)
        
        # æ—¶æ€æ ‡è®°åˆå¹¶
        merged_temporal = list(llm_result.temporal_markers)
        seen_texts = {m.get('text', '') for m in merged_temporal}
        for m in local_result.temporal_markers:
            if m.get('text', '') not in seen_texts:
                merged_temporal.append(m)
        
        # å…³é”®è¯åˆå¹¶
        merged_keywords = list(set(local_result.keywords + llm_result.keywords))
        
        return ExtractionResult(
            entities=merged_entities,
            relations=merged_relations,
            temporal_markers=merged_temporal,
            keywords=merged_keywords,
            cost=llm_result.cost,
            llm_used=True
        )
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æŠ½å–å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'mode': self.config.mode.value,
            'complexity_threshold': self.config.complexity_threshold,
            'llm_available': self.llm_client is not None,
        }
        
        if self.budget_manager:
            stats['budget'] = self.budget_manager.get_stats()
        
        return stats

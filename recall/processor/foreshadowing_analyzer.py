"""ä¼ç¬”åˆ†æå™¨ - LLM è¾…åŠ©ä¼ç¬”æ£€æµ‹ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰

è®¾è®¡ç†å¿µï¼š
1. é»˜è®¤ MANUAL æ¨¡å¼ - çº¯æ‰‹åŠ¨ç®¡ç†ï¼Œæ— éœ€ä»»ä½•é…ç½®
2. LLM æ¨¡å¼ - å¯é€‰çš„æ™ºèƒ½åˆ†æè¾…åŠ©ï¼ˆéœ€é…ç½® API keyï¼‰
3. æ‰‹åŠ¨æ“ä½œå§‹ç»ˆå¯ç”¨ - LLM åªæ˜¯è¾…åŠ©ï¼Œä¸æ˜¯æ›¿ä»£

ä¸ ForeshadowingTracker çš„å…³ç³»ï¼š
- ForeshadowingTracker: åŸºç¡€åŠŸèƒ½ï¼Œè´Ÿè´£ä¼ç¬”çš„å­˜å‚¨å’Œç®¡ç†
- ForeshadowingAnalyzer: å¯é€‰å¢å¼ºï¼Œè´Ÿè´£ LLM æ™ºèƒ½åˆ†æ

ä½¿ç”¨æ–¹å¼ï¼š
    # æ–¹å¼1ï¼šé»˜è®¤æ‰‹åŠ¨æ¨¡å¼ï¼ˆæ— éœ€ä»»ä½•é…ç½®ï¼‰
    analyzer = ForeshadowingAnalyzer(tracker=tracker)
    
    # æ–¹å¼2ï¼šLLM è¾…åŠ©æ¨¡å¼
    analyzer = ForeshadowingAnalyzer(
        tracker=tracker,
        config=ForeshadowingAnalyzerConfig.llm_based(
            api_key="sk-xxx",
            trigger_interval=10
        )
    )
"""

import json
import os
import time
import threading
from enum import Enum
from typing import Optional, List, Dict, Any, TYPE_CHECKING, Callable
from dataclasses import dataclass, field

if TYPE_CHECKING:
    from .foreshadowing import ForeshadowingTracker

class AnalyzerBackend(Enum):
    """åˆ†æå™¨åç«¯ç±»å‹"""
    MANUAL = "manual"  # æ‰‹åŠ¨æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰- ä¸åšä»»ä½•è‡ªåŠ¨åˆ†æ
    LLM = "llm"        # LLM æ™ºèƒ½åˆ†æï¼ˆéœ€é…ç½® APIï¼‰


@dataclass
class ForeshadowingAnalyzerConfig:
    """ä¼ç¬”åˆ†æå™¨é…ç½®
    
    é…ç½®è¯´æ˜ï¼š
    - backend: åç«¯æ¨¡å¼ï¼Œé»˜è®¤ MANUAL
    - trigger_interval: æ¯Nè½®è§¦å‘ä¸€æ¬¡åˆ†æï¼ˆLLMæ¨¡å¼ï¼‰ï¼Œæœ€å°1
    - llm_model: LLM æ¨¡å‹åç§°
    - llm_api_key: LLM API Key
    - llm_base_url: è‡ªå®šä¹‰ API åœ°å€ï¼ˆå¯é€‰ï¼‰
    - auto_plant: è‡ªåŠ¨åŸ‹ä¸‹æ£€æµ‹åˆ°çš„ä¼ç¬”
    - auto_resolve: è‡ªåŠ¨æ ‡è®°è§£å†³ï¼ˆå»ºè®® Falseï¼‰
    - max_context_turns: å‘é€ç»™ LLM çš„æœ€å¤§è½®æ¬¡æ•°
    """
    backend: AnalyzerBackend = AnalyzerBackend.MANUAL
    
    # è§¦å‘æ¡ä»¶ï¼ˆLLM æ¨¡å¼ï¼‰
    trigger_interval: int = 10      # æ¯Nè½®è§¦å‘ä¸€æ¬¡åˆ†æï¼ˆæœ€å°1=æ¯è½®éƒ½è§¦å‘ï¼‰
    
    # LLM é…ç½®
    llm_model: str = "gpt-4o-mini"  # é»˜è®¤ç”¨ä¾¿å®œçš„æ¨¡å‹
    llm_api_key: Optional[str] = None
    llm_base_url: Optional[str] = None  # æ”¯æŒè‡ªå®šä¹‰ API åœ°å€
    
    # è¡Œä¸ºé…ç½®
    auto_plant: bool = True         # è‡ªåŠ¨åŸ‹ä¸‹æ£€æµ‹åˆ°çš„ä¼ç¬”
    auto_resolve: bool = False      # è‡ªåŠ¨æ ‡è®°è§£å†³ï¼ˆå»ºè®® Falseï¼Œè®©ç”¨æˆ·ç¡®è®¤ï¼‰
    include_resolved_check: bool = True  # åŒæ—¶æ£€æŸ¥å·²æœ‰ä¼ç¬”æ˜¯å¦è¢«è§£å†³
    
    # é«˜çº§é…ç½®
    max_context_turns: int = field(default_factory=lambda: int(os.environ.get('CONTEXT_MAX_CONTEXT_TURNS', '20')))     # å‘é€ç»™ LLM çš„æœ€å¤§è½®æ¬¡æ•°ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œä¸æŒä¹…æ¡ä»¶ç»Ÿä¸€ï¼‰
    language: str = "zh"            # æç¤ºè¯è¯­è¨€ï¼ˆzh/enï¼‰
    
    def __post_init__(self):
        """éªŒè¯é…ç½®"""
        if self.trigger_interval < 1:
            self.trigger_interval = 1
        if self.max_context_turns < 1:
            self.max_context_turns = 1
    
    @classmethod
    def manual(cls) -> 'ForeshadowingAnalyzerConfig':
        """æ‰‹åŠ¨æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰- ç”¨æˆ·è‡ªå·±ç®¡ç†ä¼ç¬”"""
        return cls(backend=AnalyzerBackend.MANUAL)
    
    @classmethod
    def llm_based(
        cls, 
        api_key: str, 
        model: str = "gpt-4o-mini",
        trigger_interval: int = 10,
        base_url: Optional[str] = None,
        auto_plant: bool = True,
        auto_resolve: bool = False
    ) -> 'ForeshadowingAnalyzerConfig':
        """LLM è¾…åŠ©æ¨¡å¼ - æ™ºèƒ½åˆ†æ"""
        return cls(
            backend=AnalyzerBackend.LLM,
            llm_api_key=api_key,
            llm_model=model,
            llm_base_url=base_url,
            trigger_interval=trigger_interval,
            auto_plant=auto_plant,
            auto_resolve=auto_resolve
        )
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äºåºåˆ—åŒ–ï¼‰"""
        return {
            'backend': self.backend.value,
            'trigger_interval': self.trigger_interval,
            'llm_model': self.llm_model,
            'llm_base_url': self.llm_base_url,
            'auto_plant': self.auto_plant,
            'auto_resolve': self.auto_resolve,
            'include_resolved_check': self.include_resolved_check,
            'max_context_turns': self.max_context_turns,
            'language': self.language
            # æ³¨æ„ï¼šä¸åºåˆ—åŒ– api_key
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any], api_key: Optional[str] = None) -> 'ForeshadowingAnalyzerConfig':
        """ä»å­—å…¸åˆ›å»º"""
        backend_str = data.get('backend', 'manual')
        backend = AnalyzerBackend(backend_str) if backend_str in ('manual', 'llm') else AnalyzerBackend.MANUAL
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–é»˜è®¤å€¼ï¼Œä¿æŒä¸€è‡´æ€§
        default_max_context_turns = int(os.environ.get('CONTEXT_MAX_CONTEXT_TURNS', '20'))
        
        return cls(
            backend=backend,
            trigger_interval=data.get('trigger_interval', 10),
            llm_model=data.get('llm_model', 'gpt-4o-mini'),
            llm_api_key=api_key,
            llm_base_url=data.get('llm_base_url'),
            auto_plant=data.get('auto_plant', True),
            auto_resolve=data.get('auto_resolve', False),
            include_resolved_check=data.get('include_resolved_check', True),
            max_context_turns=data.get('max_context_turns', default_max_context_turns),
            language=data.get('language', 'zh')
        )


@dataclass
class AnalysisResult:
    """åˆ†æç»“æœ"""
    triggered: bool = False                      # æ˜¯å¦è§¦å‘äº†åˆ†æ
    new_foreshadowings: List[Dict[str, Any]] = field(default_factory=list)  # æ–°æ£€æµ‹åˆ°çš„ä¼ç¬”
    potentially_resolved: List[Dict[str, Any]] = field(default_factory=list)  # å¯èƒ½å·²è§£å†³çš„ä¼ç¬”
    error: Optional[str] = None                  # é”™è¯¯ä¿¡æ¯
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'triggered': self.triggered,
            'new_foreshadowings': self.new_foreshadowings,
            'potentially_resolved': self.potentially_resolved,
            'error': self.error
        }


class ForeshadowingAnalyzer:
    """ä¼ç¬”åˆ†æå™¨ - æ‰‹åŠ¨æ¨¡å¼ / LLM æ™ºèƒ½è¾…åŠ©
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. on_new_turn(): æ¯è½®å¯¹è¯åè°ƒç”¨ï¼ŒLLMæ¨¡å¼ä¼šåœ¨è¾¾åˆ°è§¦å‘æ¡ä»¶æ—¶è‡ªåŠ¨åˆ†æ
    2. trigger_analysis(): æ‰‹åŠ¨è§¦å‘åˆ†æï¼ˆä»»ä½•æ¨¡å¼ä¸‹éƒ½å¯ç”¨ï¼‰
    
    LLM åˆ†æåŠŸèƒ½ï¼š
    - æ£€æµ‹æ–°çš„ä¼ç¬”
    - è¯†åˆ«å¯èƒ½å·²è§£å†³çš„ä¼ç¬”
    - å¯é…ç½®è‡ªåŠ¨åŸ‹ä¸‹/è§£å†³
    """
    
    # ä¸­æ–‡æç¤ºè¯æ¨¡æ¿
    ANALYSIS_PROMPT_ZH = '''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å™äº‹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œè¯†åˆ«å…¶ä¸­çš„ä¼ç¬”ï¼ˆforeshadowingï¼‰ã€‚

## ä»€ä¹ˆæ˜¯ä¼ç¬”ï¼Ÿ
ä¼ç¬”æ˜¯æ•…äº‹ä¸­åŸ‹ä¸‹çš„çº¿ç´¢ï¼Œæš—ç¤ºæœªæ¥ä¼šå‘ç”Ÿçš„äº‹æƒ…ï¼ŒåŒ…æ‹¬ï¼š
- ç¥ç§˜çš„æš—ç¤ºæˆ–é¢„è¨€
- æœªè§£é‡Šçš„äº‹ä»¶æˆ–ç°è±¡
- è§’è‰²æåˆ°çš„"æœ‰ä¸€å¤©ä¼š..."ã€"æ€»æœ‰ä¸€å¤©..."
- éšè—çš„ç§˜å¯†æˆ–è°œå›¢
- ä¸ç¥¥çš„å¾å…†
- æœªå®Œæˆçš„æ‰¿è¯ºæˆ–çº¦å®š

## å½“å‰æ´»è·ƒçš„ä¼ç¬”ï¼ˆå¦‚æœæœ‰ï¼‰ï¼š
{active_foreshadowings}

## æœ€è¿‘çš„å¯¹è¯å†…å®¹ï¼š
{conversation}

## è¯·è¾“å‡º JSON æ ¼å¼ï¼š
```json
{{
  "new_foreshadowings": [
    {{
      "content": "ä¼ç¬”å†…å®¹æè¿°ï¼ˆç®€æ´æ¦‚æ‹¬ï¼‰",
      "importance": 0.8,
      "evidence": "åŸæ–‡ä¾æ®ï¼ˆå¼•ç”¨å¯¹è¯ä¸­çš„å…³é”®å¥å­ï¼‰",
      "related_entities": ["è§’è‰²A", "ç‰©å“B"]
    }}
  ],
  "potentially_resolved": [
    {{
      "foreshadowing_id": "fsh_xxx",
      "evidence": "è§£å†³çš„ä¾æ®ï¼ˆå¼•ç”¨å¯¹è¯ï¼‰",
      "confidence": 0.9
    }}
  ]
}}
```

é‡è¦æç¤ºï¼š
1. åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹
2. å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä¼ç¬”ï¼Œè¿”å›ç©ºæ•°ç»„
3. åªè¯†åˆ«çœŸæ­£çš„å™äº‹ä¼ç¬”ï¼Œä¸è¦æŠŠæ™®é€šå¯¹è¯å½“ä¼ç¬”
4. importance èŒƒå›´ 0-1ï¼Œè¶Šé‡è¦è¶Šé«˜
5. confidence èŒƒå›´ 0-1ï¼Œè¶Šç¡®å®šè¶Šé«˜'''

    # è‹±æ–‡æç¤ºè¯æ¨¡æ¿
    ANALYSIS_PROMPT_EN = '''You are a professional narrative analyst. Please analyze the following conversation and identify foreshadowing elements.

## What is foreshadowing?
Foreshadowing is a narrative device that hints at future events:
- Mysterious hints or prophecies
- Unexplained events or phenomena
- Characters saying "someday..." or "one day..."
- Hidden secrets or mysteries
- Ominous signs
- Unfulfilled promises or agreements

## Currently active foreshadowings (if any):
{active_foreshadowings}

## Recent conversation:
{conversation}

## Output in JSON format:
```json
{{
  "new_foreshadowings": [
    {{
      "content": "Brief description of the foreshadowing",
      "importance": 0.8,
      "evidence": "Quote from the conversation",
      "related_entities": ["Character A", "Item B"]
    }}
  ],
  "potentially_resolved": [
    {{
      "foreshadowing_id": "fsh_xxx",
      "evidence": "Quote showing resolution",
      "confidence": 0.9
    }}
  ]
}}
```

Important:
1. Output JSON only, no other text
2. Return empty arrays if no foreshadowing detected
3. Only identify genuine narrative foreshadowing
4. importance range: 0-1, higher = more important
5. confidence range: 0-1, higher = more certain'''

    def __init__(
        self, 
        tracker: 'ForeshadowingTracker',
        config: Optional[ForeshadowingAnalyzerConfig] = None,
        storage_dir: Optional[str] = None,
        memory_provider: Optional[Callable[[str, int], List[Dict[str, Any]]]] = None
    ):
        """åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            tracker: ForeshadowingTracker å®ä¾‹ï¼ˆå¿…éœ€ï¼‰
            config: åˆ†æå™¨é…ç½®ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºæ‰‹åŠ¨æ¨¡å¼ï¼‰
            storage_dir: æŒä¹…åŒ–ç›®å½•ï¼ˆå¯é€‰ï¼Œç”¨äºä¿å­˜åˆ†æçŠ¶æ€ï¼‰
            memory_provider: è·å–è®°å¿†çš„å›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼‰ï¼Œç­¾å: (user_id, limit) -> List[Dict]
                           ç”¨äºä»å·²ä¿å­˜çš„è®°å¿†æ„å»ºå¯¹è¯ï¼Œæé«˜å¯é æ€§
        """
        if tracker is None:
            raise ValueError("tracker å‚æ•°ä¸èƒ½ä¸º None")
        
        self.tracker = tracker
        self.config = config or ForeshadowingAnalyzerConfig.manual()
        self._storage_dir = storage_dir
        self._memory_provider = memory_provider
        
        # å¯¹è¯ç¼“å†²åŒºï¼ˆæŒ‰ç”¨æˆ·åˆ†éš”ï¼‰
        self._buffers: Dict[str, List[Dict[str, Any]]] = {}
        # è½®æ¬¡è®¡æ•°å™¨ï¼ˆæŒ‰ç”¨æˆ·åˆ†éš”ï¼‰
        self._turn_counters: Dict[str, int] = {}
        
        # ã€æ–°å¢ã€‘åˆ†æé”ï¼ˆæŒ‰ç”¨æˆ·åˆ†éš”ï¼Œé˜²æ­¢å¹¶å‘åˆ†æå†²çªï¼‰
        self._analysis_locks: Dict[str, threading.Lock] = {}
        self._locks_lock = threading.Lock()  # ä¿æŠ¤ _analysis_locks å­—å…¸æœ¬èº«
        
        # ã€æ–°å¢ã€‘æœ€ååˆ†æä½ç½®è®°å½•ï¼ˆæŒ‰ç”¨æˆ·åˆ†éš”ï¼‰
        # æ ¼å¼: {user_id: {'last_memory_id': str, 'last_timestamp': float}}
        self._analysis_markers: Dict[str, Dict[str, Any]] = {}
        
        # ã€æ–°å¢ã€‘ä»æŒä¹…åŒ–æ–‡ä»¶åŠ è½½åˆ†æçŠ¶æ€
        if storage_dir:
            os.makedirs(storage_dir, exist_ok=True)
            self._load_analysis_markers()
        
        # LLM å®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰
        self._llm_client = None
        
        # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ï¼ˆå¦‚æœæ˜¯ LLM æ¨¡å¼ï¼‰
        if self.config.backend == AnalyzerBackend.LLM:
            self._init_llm_client()
    
    def _get_user_lock(self, user_id: str) -> threading.Lock:
        """è·å–æŒ‡å®šç”¨æˆ·çš„åˆ†æé”ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self._locks_lock:
            if user_id not in self._analysis_locks:
                self._analysis_locks[user_id] = threading.Lock()
            return self._analysis_locks[user_id]
    
    def _get_markers_file_path(self) -> Optional[str]:
        """è·å–åˆ†ææ ‡è®°æ–‡ä»¶è·¯å¾„"""
        if not self._storage_dir:
            return None
        return os.path.join(self._storage_dir, 'analysis_markers.json')
    
    def _load_analysis_markers(self):
        """ä»æ–‡ä»¶åŠ è½½åˆ†ææ ‡è®°ï¼ˆæœåŠ¡å™¨é‡å¯æ—¶æ¢å¤çŠ¶æ€ï¼‰"""
        markers_file = self._get_markers_file_path()
        if not markers_file or not os.path.exists(markers_file):
            return
        
        try:
            with open(markers_file, 'r', encoding='utf-8') as f:
                self._analysis_markers = json.load(f)
            print(f"[Recall] å·²åŠ è½½ä¼ç¬”åˆ†æçŠ¶æ€ï¼ˆ{len(self._analysis_markers)} ä¸ªç”¨æˆ·ï¼‰")
        except Exception as e:
            print(f"[Recall] åŠ è½½åˆ†ææ ‡è®°å¤±è´¥: {e}")
            self._analysis_markers = {}
    
    def _save_analysis_markers(self):
        """ä¿å­˜åˆ†ææ ‡è®°åˆ°æ–‡ä»¶"""
        markers_file = self._get_markers_file_path()
        if not markers_file:
            return
        
        try:
            with open(markers_file, 'w', encoding='utf-8') as f:
                json.dump(self._analysis_markers, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[Recall] ä¿å­˜åˆ†ææ ‡è®°å¤±è´¥: {e}")
    
    def _update_analysis_marker(self, user_id: str, memory_id: Optional[str] = None):
        """æ›´æ–°æŒ‡å®šç”¨æˆ·çš„åˆ†ææ ‡è®°"""
        self._analysis_markers[user_id] = {
            'last_memory_id': memory_id,
            'last_timestamp': time.time()
        }
        self._save_analysis_markers()

    def _init_llm_client(self):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        if not self.config.llm_api_key:
            print("[Recall] è­¦å‘Šï¼šLLM æ¨¡å¼éœ€è¦é…ç½® API key")
            return
        
        try:
            from ..utils.llm_client import LLMClient
            self._llm_client = LLMClient(
                model=self.config.llm_model,
                api_key=self.config.llm_api_key,
                api_base=self.config.llm_base_url
            )
        except ImportError as e:
            print(f"[Recall] è­¦å‘Šï¼šLLM å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self._llm_client = None
    
    @property
    def is_llm_enabled(self) -> bool:
        """æ£€æŸ¥ LLM åŠŸèƒ½æ˜¯å¦å¯ç”¨"""
        return (
            self.config.backend == AnalyzerBackend.LLM and 
            self._llm_client is not None
        )
    
    def on_new_turn(
        self, 
        content: str,
        role: str = "user",
        user_id: str = "default",
        character_id: str = "default"
    ) -> AnalysisResult:
        """å¤„ç†æ–°çš„ä¸€è½®å¯¹è¯
        
        åœ¨æ¯è½®å¯¹è¯åè°ƒç”¨æ­¤æ–¹æ³•ã€‚LLM æ¨¡å¼ä¸‹ä¼šç´¯ç§¯å¯¹è¯å¹¶åœ¨è¾¾åˆ°è§¦å‘æ¡ä»¶æ—¶åˆ†æã€‚
        
        Args:
            content: å¯¹è¯å†…å®¹
            role: è§’è‰²ï¼ˆuser/assistantï¼‰
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²IDï¼ˆç”¨äºå¤šè§’è‰²åœºæ™¯ï¼‰
            
        Returns:
            AnalysisResult: åˆ†æç»“æœï¼ˆå¦‚æœè§¦å‘äº†åˆ†æï¼‰
        """
        # ä½¿ç”¨å¤åˆé”®åŒºåˆ†ä¸åŒç”¨æˆ·/è§’è‰²
        cache_key = f"{user_id}/{character_id}"
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        if cache_key not in self._buffers:
            self._buffers[cache_key] = []
        
        self._buffers[cache_key].append({
            'role': role,
            'content': content,
            'timestamp': time.time()
        })
        
        # é™åˆ¶ç¼“å†²åŒºå¤§å°
        max_size = self.config.max_context_turns * 2  # è€ƒè™‘ user + assistant
        if len(self._buffers[cache_key]) > max_size:
            self._buffers[cache_key] = self._buffers[cache_key][-max_size:]
        
        # å¢åŠ è½®æ¬¡è®¡æ•°
        if cache_key not in self._turn_counters:
            self._turn_counters[cache_key] = 0
        self._turn_counters[cache_key] += 1
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘åˆ†æï¼ˆä»… LLM æ¨¡å¼ï¼‰
        if self.config.backend == AnalyzerBackend.LLM:
            current_count = self._turn_counters[cache_key]
            if current_count >= self.config.trigger_interval:
                print(f"[ForeshadowAnalyzer] ğŸ”„ è§¦å‘åˆ†æ: user={cache_key}")
                print(f"[ForeshadowAnalyzer]    è½®æ¬¡={current_count}, é—´éš”={self.config.trigger_interval}")
                self._turn_counters[cache_key] = 0
                return self._trigger_llm_analysis(user_id, character_id)
        
        # æ‰‹åŠ¨æ¨¡å¼ä¸åšä»»ä½•åˆ†æ
        return AnalysisResult(triggered=False)
    
    def trigger_analysis(self, user_id: str = "default", character_id: str = "default") -> AnalysisResult:
        """æ‰‹åŠ¨è§¦å‘åˆ†æ
        
        å¯ä»¥åœ¨ä»»ä½•æ¨¡å¼ä¸‹è°ƒç”¨ã€‚å¦‚æœæ˜¯ LLM æ¨¡å¼ä¸”æœ‰é…ç½®ï¼Œä¼šä½¿ç”¨ LLM åˆ†æã€‚
        å¦åˆ™è¿”å›ç©ºç»“æœã€‚
        
        Args:
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²IDï¼ˆç”¨äºå¤šè§’è‰²åœºæ™¯ï¼‰
            
        Returns:
            AnalysisResult: åˆ†æç»“æœ
        """
        if self.is_llm_enabled:
            return self._trigger_llm_analysis(user_id, character_id)
        
        return AnalysisResult(
            triggered=False,
            error="LLM åˆ†ææœªå¯ç”¨ï¼ˆéœ€é…ç½® API keyï¼‰"
        )
    
    def _get_conversation_from_memories(self, user_id: str, character_id: str = "default") -> List[Dict[str, Any]]:
        """ä»å·²ä¿å­˜çš„è®°å¿†ä¸­è·å–å¯¹è¯å†…å®¹
        
        è¿™æ˜¯å¯¹ buffer çš„è¡¥å……/æ›¿ä»£ï¼Œç¡®ä¿å³ä½¿ buffer ä¸¢å¤±ä¹Ÿèƒ½ä»è®°å¿†æ¢å¤ã€‚
        ä¼˜å…ˆä½¿ç”¨ memory_providerï¼ˆå¦‚æœé…ç½®ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ bufferã€‚
        
        Args:
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
            
        Returns:
            List[Dict]: å¯¹è¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸ buffer ç›¸åŒ
        """
        cache_key = f"{user_id}/{character_id}"
        
        if not self._memory_provider:
            # æ²¡æœ‰é…ç½® memory_providerï¼Œä½¿ç”¨åŸæœ‰çš„ buffer
            return self._buffers.get(cache_key, [])
        
        try:
            # è·å–æœ€è¿‘çš„è®°å¿†ï¼ˆé™åˆ¶æ•°é‡ï¼‰
            limit = self.config.max_context_turns * 2
            memories = self._memory_provider(user_id, limit)
            
            if not memories:
                # è®°å¿†ä¸ºç©ºï¼Œå›é€€åˆ° buffer
                return self._buffers.get(cache_key, [])
            
            # è½¬æ¢è®°å¿†æ ¼å¼ä¸ºå¯¹è¯æ ¼å¼
            conversations = []
            for mem in memories:
                metadata = mem.get('metadata', {})
                role = metadata.get('role', 'user')
                content = mem.get('content', '')
                timestamp = metadata.get('timestamp', 0)
                
                if content:
                    conversations.append({
                        'role': role,
                        'content': content,
                        'timestamp': timestamp,
                        'memory_id': metadata.get('id')  # ä¿ç•™è®°å¿†IDç”¨äºæ ‡è®°
                    })
            
            # æŒ‰æ—¶é—´æˆ³æ’åºï¼ˆç¡®ä¿é¡ºåºæ­£ç¡®ï¼‰
            conversations.sort(key=lambda x: x.get('timestamp', 0))
            
            return conversations
            
        except Exception as e:
            print(f"[Recall] ä»è®°å¿†è·å–å¯¹è¯å¤±è´¥: {e}ï¼Œå›é€€åˆ° buffer")
            return self._buffers.get(cache_key, [])
    
    def _trigger_llm_analysis(self, user_id: str, character_id: str = "default") -> AnalysisResult:
        """è§¦å‘ LLM åˆ†æï¼ˆå¸¦é”ä¿æŠ¤ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            character_id: è§’è‰²ID
        """
        if not self.is_llm_enabled:
            return AnalysisResult(
                triggered=False,
                error="LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
            )
        
        cache_key = f"{user_id}/{character_id}"
        
        # ã€æ–°å¢ã€‘è·å–ç”¨æˆ·/è§’è‰²é”ï¼Œé˜²æ­¢åŒä¸€ç”¨æˆ·/è§’è‰²çš„å¹¶å‘åˆ†æ
        user_lock = self._get_user_lock(cache_key)
        
        # å°è¯•è·å–é”ï¼Œå¦‚æœå·²ç»æœ‰åˆ†æåœ¨è¿›è¡Œåˆ™è·³è¿‡
        if not user_lock.acquire(blocking=False):
            print(f"[ForeshadowAnalyzer] â­ï¸ è·³è¿‡: ç”¨æˆ· {cache_key} çš„åˆ†ææ­£åœ¨è¿›è¡Œä¸­")
            return AnalysisResult(
                triggered=False,
                error="åˆ†ææ­£åœ¨è¿›è¡Œä¸­"
            )
        
        try:
            # ã€æ”¹è¿›ã€‘ä¼˜å…ˆä»å·²ä¿å­˜çš„è®°å¿†è·å–å¯¹è¯
            conversations = self._get_conversation_from_memories(user_id, character_id)
            
            if not conversations:
                return AnalysisResult(
                    triggered=True,
                    error="å¯¹è¯ç¼“å†²åŒºä¸ºç©º"
                )
            
            # é™åˆ¶å¯¹è¯æ•°é‡
            conversations = conversations[-self.config.max_context_turns * 2:]
            
            # æ„å»ºå¯¹è¯æ–‡æœ¬
            conversation_text = self._format_conversation(conversations)
            
            # è·å–å½“å‰æ´»è·ƒçš„ä¼ç¬”
            active = self.tracker.get_active(user_id, character_id)
            active_text = self._format_active_foreshadowings(active)
            
            # é€‰æ‹©æç¤ºè¯
            prompt_template = (
                self.ANALYSIS_PROMPT_ZH 
                if self.config.language == 'zh' 
                else self.ANALYSIS_PROMPT_EN
            )
            
            # æ„å»ºæç¤ºè¯
            prompt = prompt_template.format(
                active_foreshadowings=active_text or "ï¼ˆæš‚æ— ï¼‰" if self.config.language == 'zh' else "(None)",
                conversation=conversation_text
            )
            
            # è°ƒç”¨ LLM
            print(f"[ForeshadowAnalyzer] ğŸ¤– è°ƒç”¨ LLM åˆ†æ...")
            print(f"[ForeshadowAnalyzer]    ç”¨æˆ·={cache_key}, å¯¹è¯æ•°={len(conversations)}, æ´»è·ƒä¼ç¬”={len(active)}")
            response = self._llm_client.chat(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§
                max_tokens=1000
            )
            print(f"[ForeshadowAnalyzer]    LLM å“åº”: {len(response.content)} å­—ç¬¦")
            
            # è§£æç»“æœ
            result = self._parse_llm_response(response.content)
            result.triggered = True
            
            # å¤„ç†ç»“æœï¼šè‡ªåŠ¨åŸ‹ä¸‹ä¼ç¬”
            if self.config.auto_plant and result.new_foreshadowings:
                print(f"[ForeshadowAnalyzer] ğŸŒ± è‡ªåŠ¨åŸ‹ä¸‹ {len(result.new_foreshadowings)} ä¸ªæ–°ä¼ç¬”")
                for fsh_data in result.new_foreshadowings:
                    try:
                        content = fsh_data.get('content', '')
                        print(f"[ForeshadowAnalyzer]    åŸ‹ä¸‹: {content[:50]}..." if len(content) > 50 else f"[ForeshadowAnalyzer]    åŸ‹ä¸‹: {content}")
                        self.tracker.plant(
                            content=content,
                            user_id=user_id,
                            character_id=character_id,
                            importance=fsh_data.get('importance', 0.5),
                            related_entities=fsh_data.get('related_entities', [])
                        )
                    except Exception as e:
                        print(f"[ForeshadowAnalyzer] âŒ åŸ‹ä¸‹å¤±è´¥: {e}")
            
            # å¤„ç†ç»“æœï¼šè‡ªåŠ¨è§£å†³ä¼ç¬”
            if self.config.auto_resolve and result.potentially_resolved:
                print(f"[ForeshadowAnalyzer] âœ… æ£€æµ‹åˆ° {len(result.potentially_resolved)} ä¸ªå¯èƒ½è§£å†³çš„ä¼ç¬”")
                for resolved_data in result.potentially_resolved:
                    fsh_id = resolved_data.get('foreshadowing_id')
                    confidence = resolved_data.get('confidence', 0)
                    evidence = resolved_data.get('evidence', '')
                    
                    print(f"[ForeshadowAnalyzer]    æ£€æµ‹: id={fsh_id}, ç½®ä¿¡åº¦={confidence:.2f}")
                    
                    # åªæœ‰ç½®ä¿¡åº¦é«˜äº0.8æ‰è‡ªåŠ¨è§£å†³
                    if fsh_id and confidence >= 0.8:
                        try:
                            print(f"[ForeshadowAnalyzer]    è‡ªåŠ¨è§£å†³: {evidence[:50]}..." if len(evidence) > 50 else f"[ForeshadowAnalyzer]    è‡ªåŠ¨è§£å†³: {evidence}")
                            self.tracker.resolve(
                                foreshadowing_id=fsh_id,
                                resolution=f"[è‡ªåŠ¨æ£€æµ‹] {evidence}",
                                user_id=user_id,
                                character_id=character_id
                            )
                        except Exception as e:
                            print(f"[ForeshadowAnalyzer] âŒ è‡ªåŠ¨è§£å†³å¤±è´¥: {e}")
                    else:
                        print(f"[ForeshadowAnalyzer]    è·³è¿‡: ç½®ä¿¡åº¦ä¸è¶³ ({confidence:.2f} < 0.8)")
            
            # ã€æ”¹è¿›ã€‘æ›´æ–°åˆ†ææ ‡è®°ï¼ˆè®°å½•æœ€ååˆ†æçš„è®°å¿†IDï¼‰
            cache_key = f"{user_id}/{character_id}"
            last_memory_id = None
            if conversations and 'memory_id' in conversations[-1]:
                last_memory_id = conversations[-1]['memory_id']
            self._update_analysis_marker(cache_key, last_memory_id)
            
            # æ¸…ç©ºå·²åˆ†æçš„ç¼“å†²åŒº
            self._buffers[cache_key] = []
            
            print(f"[ForeshadowAnalyzer] âœ… åˆ†æå®Œæˆ: æ–°ä¼ç¬”={len(result.new_foreshadowings)}, å¯èƒ½è§£å†³={len(result.potentially_resolved)}")
            
            return result
            
        except Exception as e:
            print(f"[ForeshadowAnalyzer] âŒ LLM åˆ†æå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return AnalysisResult(
                triggered=True,
                error=f"LLM åˆ†æå¤±è´¥: {str(e)}"
            )
        finally:
            # ã€é‡è¦ã€‘ç¡®ä¿é‡Šæ”¾é”
            user_lock.release()
    
    def _format_conversation(self, turns: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†…å®¹"""
        lines = []
        for t in turns:
            role_name = "ç”¨æˆ·" if t['role'] == 'user' else "AI"
            if self.config.language != 'zh':
                role_name = "User" if t['role'] == 'user' else "AI"
            lines.append(f"[{role_name}]: {t['content']}")
        return "\n\n".join(lines)
    
    def _format_active_foreshadowings(self, foreshadowings) -> str:
        """æ ¼å¼åŒ–æ´»è·ƒä¼ç¬”åˆ—è¡¨"""
        if not foreshadowings:
            return ""
        
        lines = []
        for f in foreshadowings:
            # å…¼å®¹ Foreshadowing å¯¹è±¡å’Œå­—å…¸
            if hasattr(f, 'id'):
                fsh_id = f.id
                content = f.content
                importance = f.importance
            else:
                fsh_id = f.get('id', '')
                content = f.get('content', '')
                importance = f.get('importance', 0.5)
            
            lines.append(f"- [{fsh_id}] {content} (é‡è¦æ€§: {importance})")
        return "\n".join(lines)
    
    def _parse_llm_response(self, response: str) -> AnalysisResult:
        """è§£æ LLM è¿”å›çš„ JSON"""
        result = AnalysisResult()
        
        try:
            # å°è¯•æå– JSONï¼ˆå¤„ç†å¯èƒ½çš„ markdown ä»£ç å—ï¼‰
            json_str = response
            
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0]
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0]
            
            data = json.loads(json_str.strip())
            
            result.new_foreshadowings = data.get('new_foreshadowings', [])
            result.potentially_resolved = data.get('potentially_resolved', [])
            
        except json.JSONDecodeError as e:
            result.error = f"JSON è§£æå¤±è´¥: {e}"
        except Exception as e:
            result.error = f"è§£æå¤±è´¥: {e}"
        
        return result
    
    def get_buffer_size(self, user_id: str = "default", character_id: str = "default") -> int:
        """è·å–å½“å‰ç¼“å†²åŒºå¤§å°"""
        cache_key = f"{user_id}/{character_id}"
        return len(self._buffers.get(cache_key, []))
    
    def get_turns_until_analysis(self, user_id: str = "default", character_id: str = "default") -> int:
        """è·å–è·ç¦»ä¸‹æ¬¡åˆ†æè¿˜éœ€è¦å¤šå°‘è½®"""
        if self.config.backend != AnalyzerBackend.LLM:
            return -1  # æ‰‹åŠ¨æ¨¡å¼ä¸ä¼šè‡ªåŠ¨åˆ†æ
        
        cache_key = f"{user_id}/{character_id}"
        current = self._turn_counters.get(cache_key, 0)
        return self.config.trigger_interval - current
    
    def clear_buffer(self, user_id: str = "default", character_id: str = "default"):
        """æ¸…ç©ºå¯¹è¯ç¼“å†²åŒº"""
        cache_key = f"{user_id}/{character_id}"
        if cache_key in self._buffers:
            self._buffers[cache_key] = []
        if cache_key in self._turn_counters:
            self._turn_counters[cache_key] = 0
    
    def update_config(
        self,
        trigger_interval: Optional[int] = None,
        auto_plant: Optional[bool] = None,
        auto_resolve: Optional[bool] = None,
        max_context_turns: Optional[int] = None
    ):
        """æ›´æ–°é…ç½®ï¼ˆä¸é‡æ–°åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ï¼‰"""
        if trigger_interval is not None:
            self.config.trigger_interval = max(1, trigger_interval)
        if auto_plant is not None:
            self.config.auto_plant = auto_plant
        if auto_resolve is not None:
            self.config.auto_resolve = auto_resolve
        if max_context_turns is not None:
            self.config.max_context_turns = max(1, max_context_turns)
    
    def enable_llm_mode(
        self,
        api_key: str,
        model: str = "gpt-4o-mini",
        base_url: Optional[str] = None
    ):
        """åŠ¨æ€å¯ç”¨ LLM æ¨¡å¼ï¼ˆæ— éœ€é‡å¯æœåŠ¡ï¼‰
        
        Args:
            api_key: LLM API Key
            model: æ¨¡å‹åç§°
            base_url: API Base URLï¼ˆå¯é€‰ï¼‰
        """
        # æ›´æ–°é…ç½®
        self.config.backend = AnalyzerBackend.LLM
        self.config.llm_api_key = api_key
        self.config.llm_model = model
        self.config.llm_base_url = base_url
        
        # é‡æ–°åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        self._init_llm_client()
        
        if self._llm_client:
            print(f"[Recall] ä¼ç¬”åˆ†æå™¨å·²åˆ‡æ¢åˆ° LLM æ¨¡å¼ (model={model})")
        else:
            print("[Recall] è­¦å‘Šï¼šLLM å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
    
    def disable_llm_mode(self):
        """åŠ¨æ€ç¦ç”¨ LLM æ¨¡å¼ï¼Œåˆ‡æ¢å›æ‰‹åŠ¨æ¨¡å¼ï¼ˆæ— éœ€é‡å¯æœåŠ¡ï¼‰"""
        self.config.backend = AnalyzerBackend.MANUAL
        self._llm_client = None
        print("[Recall] ä¼ç¬”åˆ†æå™¨å·²åˆ‡æ¢åˆ°æ‰‹åŠ¨æ¨¡å¼")

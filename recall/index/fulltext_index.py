"""ÂÖ®ÊñáÁ¥¢Âºï - BM25 ÂÆûÁé∞

ËÆæËÆ°ÁêÜÂøµÔºö
1. Á∫Ø Python ÂÆûÁé∞ BM25 ÁÆóÊ≥ïÔºåÈõ∂Â§ñÈÉ®‰æùËµñ
2. ÊîØÊåÅ‰∏≠Ëã±ÊñáÊ∑∑ÂêàÊñáÊú¨
3. ‰∏éÁé∞Êúâ InvertedIndex ‰∫íË°•ÔºàInvertedIndex ÊòØÁ≤æÁ°ÆÂåπÈÖçÔºåËøôÊòØÁõ∏ÂÖ≥ÊÄßÊéíÂ∫èÔºâ
4. Â¢ûÈáèÊõ¥Êñ∞ÔºåÊó†ÈúÄÈáçÂª∫Êï¥‰∏™Á¥¢Âºï
"""

from __future__ import annotations

import os
import json
import math
import re
from dataclasses import dataclass, field
from typing import Dict, List, Set, Optional, Tuple, Any
from collections import defaultdict, Counter


# Windows GBK ÁºñÁ†ÅÂÖºÂÆπÁöÑÂÆâÂÖ®ÊâìÂç∞ÂáΩÊï∞
def _safe_print(msg: str) -> None:
    """ÂÆâÂÖ®ÊâìÂç∞ÂáΩÊï∞ÔºåÊõøÊç¢ emoji ‰∏∫ ASCII Á≠â‰ª∑Áâ©‰ª•ÈÅøÂÖç Windows GBK ÁºñÁ†ÅÈîôËØØ"""
    emoji_map = {
        'üì•': '[IN]', 'üì§': '[OUT]', 'üîç': '[SEARCH]', '‚úÖ': '[OK]', '‚ùå': '[FAIL]',
        '‚ö†Ô∏è': '[WARN]', 'üíæ': '[SAVE]', 'üóÉÔ∏è': '[DB]', 'üßπ': '[CLEAN]', 'üìä': '[STATS]',
        'üîÑ': '[SYNC]', 'üì¶': '[PKG]', 'üöÄ': '[START]', 'üéØ': '[TARGET]', 'üí°': '[HINT]',
        'üîß': '[FIX]', 'üìù': '[NOTE]', 'üéâ': '[DONE]', '‚è±Ô∏è': '[TIME]', 'üåê': '[NET]',
        'üß†': '[BRAIN]', 'üí¨': '[CHAT]', 'üè∑Ô∏è': '[TAG]', 'üìÅ': '[DIR]', 'üîí': '[LOCK]',
        'üå±': '[PLANT]', 'üóëÔ∏è': '[DEL]', 'üí´': '[MAGIC]', 'üé≠': '[MASK]', 'üìñ': '[BOOK]',
        '‚ö°': '[FAST]', 'üî•': '[HOT]', 'üíé': '[GEM]', 'üåü': '[STAR]', 'üé®': '[ART]'
    }
    for emoji, ascii_equiv in emoji_map.items():
        msg = msg.replace(emoji, ascii_equiv)
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode('ascii', errors='replace').decode('ascii'))


@dataclass
class BM25Config:
    """BM25 ÂèÇÊï∞ÈÖçÁΩÆ"""
    k1: float = 1.5     # ËØçÈ¢ëÈ•±ÂíåÂèÇÊï∞Ôºà1.2-2.0Ôºâ
    b: float = 0.75     # ÊñáÊ°£ÈïøÂ∫¶ÂΩí‰∏ÄÂåñÂèÇÊï∞Ôºà0-1Ôºâ
    delta: float = 0.0  # BM25+ ÁöÑ delta ÂèÇÊï∞Ôºà0 Ë°®Á§∫ÁªèÂÖ∏ BM25Ôºâ


class FullTextIndex:
    """ÂÖ®ÊñáÁ¥¢Âºï - BM25 ÂÆûÁé∞
    
    ÁâπÁÇπÔºö
    1. ÊîØÊåÅ‰∏≠Ëã±ÊñáÊ∑∑ÂêàÂàÜËØç
    2. Â¢ûÈáèÊõ¥Êñ∞
    3. ÂèØÈÖçÁΩÆÁöÑ BM25 ÂèÇÊï∞
    4. ÊîØÊåÅÂ≠óÊÆµÊùÉÈáçÔºàÂèØÈÄâÔºâ
    
    ‰ΩøÁî®ÊñπÂºèÔºö
        index = FullTextIndex(data_path)
        index.add('doc1', 'ËøôÊòØ‰∏ÄÊÆµÊµãËØïÊñáÊú¨')
        results = index.search('ÊµãËØï', top_k=10)
    """
    
    def __init__(
        self,
        data_path: str,
        config: Optional[BM25Config] = None
    ):
        """ÂàùÂßãÂåñÂÖ®ÊñáÁ¥¢Âºï
        
        Args:
            data_path: Êï∞ÊçÆÂ≠òÂÇ®Ë∑ØÂæÑ
            config: BM25 ÂèÇÊï∞ÈÖçÁΩÆ
        """
        self.data_path = data_path
        self.index_dir = os.path.join(data_path, 'indexes')
        self.index_file = os.path.join(self.index_dir, 'fulltext_index.json')
        
        self.config = config or BM25Config()
        
        # Ê†∏ÂøÉÊï∞ÊçÆÁªìÊûÑ
        self.doc_count = 0                                    # ÊñáÊ°£ÊÄªÊï∞
        self.avg_doc_length = 0.0                            # Âπ≥ÂùáÊñáÊ°£ÈïøÂ∫¶
        self.total_doc_length = 0                            # ÊÄªÊñáÊ°£ÈïøÂ∫¶
        
        # ÂÄíÊéíÁ¥¢Âºï: term -> {doc_id -> term_frequency}
        self.inverted_index: Dict[str, Dict[str, int]] = defaultdict(dict)
        
        # ÊñáÊ°£‰ø°ÊÅØ: doc_id -> {length, terms}
        self.doc_info: Dict[str, Dict[str, Any]] = {}
        
        # ÊñáÊ°£È¢ëÁéá: term -> ÂåÖÂê´ËØ•ËØçÁöÑÊñáÊ°£Êï∞
        self.doc_freq: Dict[str, int] = defaultdict(int)
        
        # ÂÅúÁî®ËØç
        self.stopwords: Set[str] = self._default_stopwords()
        
        # ËÑèÊ†áËÆ∞
        self._dirty = False
        
        # Âä†ËΩΩ
        self._load()
    
    def _default_stopwords(self) -> Set[str]:
        """ÈªòËÆ§ÂÅúÁî®ËØç"""
        return {
            # ‰∏≠ÊñáÂÅúÁî®ËØç
            'ÁöÑ', '‰∫Ü', 'Âú®', 'ÊòØ', 'Êàë', 'Êúâ', 'Âíå', 'Â∞±', '‰∏ç', '‰∫∫', 'ÈÉΩ', '‰∏Ä', '‰∏Ä‰∏™',
            '‰∏ä', '‰πü', 'Âæà', 'Âà∞', 'ËØ¥', 'Ë¶Å', 'Âéª', '‰Ω†', '‰ºö', 'ÁùÄ', 'Ê≤°Êúâ', 'Áúã', 'Â•Ω',
            'Ëá™Â∑±', 'Ëøô', 'ÈÇ£', 'Âïä', 'Âêß', 'Âêó', 'Âë¢', 'Âì¶', 'ÂóØ', 'Âï¶',
            # Ëã±ÊñáÂÅúÁî®ËØç
            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',
            'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those',
            'i', 'you', 'he', 'she', 'it', 'we', 'they', 'what', 'which', 'who',
            'when', 'where', 'why', 'how', 'all', 'each', 'every', 'both', 'few',
            'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',
            'own', 'same', 'so', 'than', 'too', 'very', 'just', 'and', 'but', 'or',
            'as', 'if', 'then', 'else', 'for', 'of', 'at', 'by', 'from', 'to', 'in',
            'on', 'with', 'about', 'against', 'between', 'into', 'through', 'during',
            'before', 'after', 'above', 'below', 'up', 'down', 'out', 'off', 'over',
        }
    
    def _load(self):
        """Âä†ËΩΩÁ¥¢Âºï"""
        if not os.path.exists(self.index_file):
            return
        
        try:
            with open(self.index_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.doc_count = data.get('doc_count', 0)
            self.avg_doc_length = data.get('avg_doc_length', 0.0)
            self.total_doc_length = data.get('total_doc_length', 0)
            self.inverted_index = defaultdict(dict, data.get('inverted_index', {}))
            self.doc_info = data.get('doc_info', {})
            self.doc_freq = defaultdict(int, data.get('doc_freq', {}))
            
            # ÊÅ¢Â§çÈÖçÁΩÆ
            if 'config' in data:
                cfg = data['config']
                self.config = BM25Config(
                    k1=cfg.get('k1', 1.5),
                    b=cfg.get('b', 0.75),
                    delta=cfg.get('delta', 0.0)
                )
            
        except Exception as e:
            _safe_print(f"[FullTextIndex] Âä†ËΩΩÁ¥¢ÂºïÂ§±Ë¥•: {e}")
    
    def _save(self):
        """‰øùÂ≠òÁ¥¢Âºï"""
        if not self._dirty:
            return
        
        os.makedirs(self.index_dir, exist_ok=True)
        
        data = {
            'version': '4.0',
            'doc_count': self.doc_count,
            'avg_doc_length': self.avg_doc_length,
            'total_doc_length': self.total_doc_length,
            'inverted_index': dict(self.inverted_index),
            'doc_info': self.doc_info,
            'doc_freq': dict(self.doc_freq),
            'config': {
                'k1': self.config.k1,
                'b': self.config.b,
                'delta': self.config.delta
            }
        }
        
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False)
        
        self._dirty = False
    
    def tokenize(self, text: str) -> List[str]:
        """ÂàÜËØç
        
        Á≠ñÁï•Ôºö
        1. Ëã±ÊñáÊåâÁ©∫Ê†ºÂíåÊ†áÁÇπÂàÜËØç
        2. ‰∏≠ÊñáÊåâÂ≠óÁ¨¶ÂàÜËØçÔºàÁÆÄÂçï‰ΩÜÊúâÊïàÔºâ
        3. ËøáÊª§ÂÅúÁî®ËØçÂíåÂçïÂ≠óÁ¨¶
        """
        tokens = []
        
        # ËΩ¨Â∞èÂÜô
        text = text.lower()
        
        # ÂàÜÁ¶ª‰∏≠Ëã±Êñá
        # Ëã±ÊñáÂçïËØç
        english_words = re.findall(r'[a-z]+', text)
        tokens.extend(w for w in english_words if len(w) > 1 and w not in self.stopwords)
        
        # ‰∏≠ÊñáÔºàÊèêÂèñËøûÁª≠‰∏≠ÊñáÔºåÁÑ∂ÂêéÊåâ2-gramÂàáÂàÜÔºâ
        chinese_segments = re.findall(r'[\u4e00-\u9fff]+', text)
        for segment in chinese_segments:
            if len(segment) >= 2:
                # ‰øùÁïôÂÆåÊï¥ËØç
                if segment not in self.stopwords:
                    tokens.append(segment)
                # 2-gram
                for i in range(len(segment) - 1):
                    bigram = segment[i:i+2]
                    if bigram not in self.stopwords:
                        tokens.append(bigram)
        
        return tokens
    
    def add(self, doc_id: str, text: str):
        """Ê∑ªÂä†ÊñáÊ°£
        
        Args:
            doc_id: ÊñáÊ°£ID
            text: ÊñáÊ°£ÊñáÊú¨
        """
        # Â¶ÇÊûúÂ∑≤Â≠òÂú®ÔºåÂÖàÁßªÈô§
        if doc_id in self.doc_info:
            self.remove(doc_id)
        
        # ÂàÜËØç
        tokens = self.tokenize(text)
        if not tokens:
            return
        
        # ËØçÈ¢ëÁªüËÆ°
        term_freq = Counter(tokens)
        doc_length = len(tokens)
        
        # Êõ¥Êñ∞ÂÄíÊéíÁ¥¢Âºï
        for term, freq in term_freq.items():
            self.inverted_index[term][doc_id] = freq
            
            # Êõ¥Êñ∞ÊñáÊ°£È¢ëÁéáÔºàÂè™Âú®Êñ∞ËØçÊó∂Â¢ûÂä†Ôºâ
            if len(self.inverted_index[term]) == 1 or doc_id not in self.inverted_index[term]:
                pass  # Â∑≤Âú®‰∏äÈù¢Ê∑ªÂä†
            # È¶ñÊ¨°Ê∑ªÂä†ËØ•ËØçÂà∞ËØ•ÊñáÊ°£
            if doc_id not in self.doc_info or term not in self.doc_info.get(doc_id, {}).get('terms', []):
                self.doc_freq[term] += 1
        
        # ‰øÆÊ≠£ÔºöÁõ¥Êé•ËÆæÁΩÆÊñáÊ°£È¢ëÁéá
        for term in term_freq:
            self.doc_freq[term] = len(self.inverted_index[term])
        
        # ‰øùÂ≠òÊñáÊ°£‰ø°ÊÅØ
        self.doc_info[doc_id] = {
            'length': doc_length,
            'terms': list(term_freq.keys())
        }
        
        # Êõ¥Êñ∞ÁªüËÆ°
        self.doc_count += 1
        self.total_doc_length += doc_length
        self.avg_doc_length = self.total_doc_length / self.doc_count if self.doc_count > 0 else 0
        
        self._dirty = True
    
    def remove(self, doc_id: str) -> bool:
        """ÁßªÈô§ÊñáÊ°£
        
        Args:
            doc_id: ÊñáÊ°£ID
        
        Returns:
            ÊòØÂê¶ÊàêÂäüÁßªÈô§
        """
        if doc_id not in self.doc_info:
            return False
        
        info = self.doc_info[doc_id]
        
        # ‰ªéÂÄíÊéíÁ¥¢ÂºïÁßªÈô§
        for term in info['terms']:
            if term in self.inverted_index and doc_id in self.inverted_index[term]:
                del self.inverted_index[term][doc_id]
                
                # Êõ¥Êñ∞ÊñáÊ°£È¢ëÁéá
                self.doc_freq[term] = len(self.inverted_index[term])
                
                # Â¶ÇÊûúËØ•ËØç‰∏çÂÜçÊúâÊñáÊ°£ÔºåÁßªÈô§
                if not self.inverted_index[term]:
                    del self.inverted_index[term]
                    del self.doc_freq[term]
        
        # Êõ¥Êñ∞ÁªüËÆ°
        self.doc_count -= 1
        self.total_doc_length -= info['length']
        self.avg_doc_length = self.total_doc_length / self.doc_count if self.doc_count > 0 else 0
        
        # ÁßªÈô§ÊñáÊ°£‰ø°ÊÅØ
        del self.doc_info[doc_id]
        
        self._dirty = True
        return True
    
    def remove_by_doc_ids(self, doc_ids: Set[str]) -> int:
        """ÊâπÈáèÁßªÈô§ÊñáÊ°£
        
        Args:
            doc_ids: Ë¶ÅÁßªÈô§ÁöÑÊñáÊ°£IDÈõÜÂêà
        
        Returns:
            int: ÊàêÂäüÁßªÈô§ÁöÑÊñáÊ°£Êï∞Èáè
        """
        removed_count = 0
        for doc_id in doc_ids:
            if self.remove(doc_id):
                removed_count += 1
        return removed_count
    
    def search(
        self,
        query: str,
        top_k: int = 10,
        min_score: float = 0.0
    ) -> List[Tuple[str, float]]:
        """ÊêúÁ¥¢ÊñáÊ°£
        
        Args:
            query: Êü•ËØ¢ÊñáÊú¨
            top_k: ËøîÂõûÊï∞Èáè
            min_score: ÊúÄÂ∞èÂàÜÊï∞ÈòàÂÄº
        
        Returns:
            [(doc_id, score), ...] ÊåâÂàÜÊï∞ÈôçÂ∫è
        """
        if self.doc_count == 0:
            return []
        
        # ÂàÜËØç
        query_terms = self.tokenize(query)
        if not query_terms:
            return []
        
        # ËÆ°ÁÆóÊØè‰∏™ÊñáÊ°£ÁöÑ BM25 ÂàÜÊï∞
        scores: Dict[str, float] = defaultdict(float)
        
        for term in query_terms:
            if term not in self.inverted_index:
                continue
            
            # IDF
            df = self.doc_freq.get(term, 0)
            if df == 0:
                continue
            
            idf = math.log((self.doc_count - df + 0.5) / (df + 0.5) + 1)
            
            # ÈÅçÂéÜÂåÖÂê´ËØ•ËØçÁöÑÊñáÊ°£
            for doc_id, tf in self.inverted_index[term].items():
                doc_length = self.doc_info[doc_id]['length']
                
                # BM25 ÂÖ¨Âºè
                numerator = tf * (self.config.k1 + 1)
                denominator = tf + self.config.k1 * (
                    1 - self.config.b + self.config.b * doc_length / self.avg_doc_length
                )
                
                score = idf * numerator / denominator
                
                # BM25+ ‰øÆÊ≠£
                if self.config.delta > 0:
                    score += self.config.delta
                
                scores[doc_id] += score
        
        # ÊéíÂ∫èÂπ∂ËøáÊª§
        results = [
            (doc_id, score) 
            for doc_id, score in scores.items() 
            if score >= min_score
        ]
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results[:top_k]
    
    def search_with_weights(
        self,
        query: str,
        field_weights: Dict[str, float] = None,
        top_k: int = 10
    ) -> List[Tuple[str, float]]:
        """Â∏¶Â≠óÊÆµÊùÉÈáçÁöÑÊêúÁ¥¢ÔºàÈ¢ÑÁïôÊé•Âè£Ôºâ
        
        Args:
            query: Êü•ËØ¢ÊñáÊú¨
            field_weights: Â≠óÊÆµÊùÉÈáç {'title': 2.0, 'content': 1.0}
            top_k: ËøîÂõûÊï∞Èáè
        
        Returns:
            [(doc_id, score), ...]
        """
        # ÂΩìÂâçÁÆÄÂåñÂÆûÁé∞ÔºåÁõ¥Êé•Ë∞ÉÁî® search
        # Êú™Êù•ÂèØ‰ª•Êâ©Â±ïÊîØÊåÅÂ§öÂ≠óÊÆµÁ¥¢Âºï
        return self.search(query, top_k)
    
    def flush(self):
        """Âº∫Âà∂‰øùÂ≠ò"""
        self._dirty = True
        self._save()
    
    def count(self) -> int:
        """ËøîÂõûÊñáÊ°£ÊÄªÊï∞"""
        return self.doc_count
    
    def get_terms(self, doc_id: str) -> List[str]:
        """Ëé∑ÂèñÊñáÊ°£ÁöÑËØçÈ°π"""
        info = self.doc_info.get(doc_id)
        return info['terms'] if info else []
    
    def get_doc_length(self, doc_id: str) -> int:
        """Ëé∑ÂèñÊñáÊ°£ÈïøÂ∫¶"""
        info = self.doc_info.get(doc_id)
        return info['length'] if info else 0
    
    def clear(self):
        """Ê∏ÖÁ©∫Á¥¢Âºï"""
        self.doc_count = 0
        self.avg_doc_length = 0.0
        self.total_doc_length = 0
        self.inverted_index.clear()
        self.doc_info.clear()
        self.doc_freq.clear()
        self._dirty = True
        self._save()
    
    def get_stats(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÁ¥¢ÂºïÁªüËÆ°‰ø°ÊÅØ"""
        return {
            'doc_count': self.doc_count,
            'avg_doc_length': self.avg_doc_length,
            'total_doc_length': self.total_doc_length,
            'vocabulary_size': len(self.inverted_index),
            'config': {
                'k1': self.config.k1,
                'b': self.config.b,
                'delta': self.config.delta
            }
        }


__all__ = [
    'BM25Config',
    'FullTextIndex',
]

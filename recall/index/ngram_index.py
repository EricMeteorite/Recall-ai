"""ä¼˜åŒ–çš„N-gramç´¢å¼• - æ”¯æŒåè¯çŸ­è¯­ç´¢å¼• + åŸæ–‡å…¨æ–‡å…œåº•æœç´¢

Phase 3.6 æ›´æ–°ï¼šæ·»åŠ å¹¶è¡Œåˆ†ç‰‡æ‰«ææ”¯æŒï¼Œæå‡å¤§è§„æ¨¡æ•°æ®çš„å…œåº•é€Ÿåº¦
"""

import os
import re
import json
from typing import List, Dict, Set, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed


# Windows GBK ç¼–ç å…¼å®¹çš„å®‰å…¨æ‰“å°å‡½æ•°
def _safe_print(msg: str) -> None:
    """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œæ›¿æ¢ emoji ä¸º ASCII ç­‰ä»·ç‰©ä»¥é¿å… Windows GBK ç¼–ç é”™è¯¯"""
    emoji_map = {
        'ğŸ“¥': '[IN]', 'ğŸ“¤': '[OUT]', 'ğŸ”': '[SEARCH]', 'âœ…': '[OK]', 'âŒ': '[FAIL]',
        'âš ï¸': '[WARN]', 'ğŸ’¾': '[SAVE]', 'ğŸ—ƒï¸': '[DB]', 'ğŸ§¹': '[CLEAN]', 'ğŸ“Š': '[STATS]',
        'ğŸ”„': '[SYNC]', 'ğŸ“¦': '[PKG]', 'ğŸš€': '[START]', 'ğŸ¯': '[TARGET]', 'ğŸ’¡': '[HINT]',
        'ğŸ”§': '[FIX]', 'ğŸ“': '[NOTE]', 'ğŸ‰': '[DONE]', 'â±ï¸': '[TIME]', 'ğŸŒ': '[NET]',
        'ğŸ§ ': '[BRAIN]', 'ğŸ’¬': '[CHAT]', 'ğŸ·ï¸': '[TAG]', 'ğŸ“': '[DIR]', 'ğŸ”’': '[LOCK]',
        'ğŸŒ±': '[PLANT]', 'ğŸ—‘ï¸': '[DEL]', 'ğŸ’«': '[MAGIC]', 'ğŸ­': '[MASK]', 'ğŸ“–': '[BOOK]',
        'âš¡': '[FAST]', 'ğŸ”¥': '[HOT]', 'ğŸ’': '[GEM]', 'ğŸŒŸ': '[STAR]', 'ğŸ¨': '[ART]'
    }
    for emoji, ascii_equiv in emoji_map.items():
        msg = msg.replace(emoji, ascii_equiv)
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode('ascii', errors='replace').decode('ascii'))


class OptimizedNgramIndex:
    """ä¼˜åŒ–çš„N-gramç´¢å¼• - åè¯çŸ­è¯­ç´¢å¼• + åŸæ–‡å…œåº•
    
    è®¾è®¡åŸåˆ™ï¼ˆæŒ‰è®¡åˆ’åäºŒç‚¹äº”"100%ä¸é—å¿˜"è¦æ±‚ï¼‰ï¼š
    1. ä¸»ç´¢å¼•ï¼šåªç´¢å¼•åè¯çŸ­è¯­ï¼Œé¿å…ç©ºé—´çˆ†ç‚¸
    2. åŸæ–‡å­˜å‚¨ï¼šä¿ç•™å®Œæ•´åŸæ–‡ï¼Œæ”¯æŒ"ç»ˆæå…œåº•"æœç´¢
    3. ä¸¤å±‚æœç´¢ï¼šå…ˆæŸ¥åè¯çŸ­è¯­ç´¢å¼•ï¼Œæ— ç»“æœæ—¶æ‰«æåŸæ–‡
    """
    
    def __init__(self, data_path: str = None):
        # æ•°æ®ç›®å½•
        self.data_path = data_path
        self._index_file = os.path.join(data_path, "ngram_index.json") if data_path else None
        self._raw_content_file = os.path.join(data_path, "ngram_raw_content.jsonl") if data_path else None
        
        # ä¸»ç´¢å¼•ï¼šåè¯çŸ­è¯­ â†’ [memory_ids]
        self.noun_phrases: Dict[str, List[str]] = {}
        
        # åŸæ–‡å­˜å‚¨ï¼šmemory_id â†’ contentï¼ˆç”¨äºå…œåº•æœç´¢ï¼‰
        self._raw_content: Dict[str, str] = {}
        
        # å¯é€‰çš„å¸ƒéš†è¿‡æ»¤å™¨
        self._bloom_filter = None
        
        # ä»ç£ç›˜åŠ è½½å·²æœ‰æ•°æ®
        if data_path:
            self._load()
    
    @property
    def bloom_filter(self):
        """æ‡’åŠ è½½å¸ƒéš†è¿‡æ»¤å™¨"""
        if self._bloom_filter is None:
            try:
                from pybloom_live import BloomFilter
                self._bloom_filter = BloomFilter(capacity=1000000, error_rate=0.01)
            except ImportError:
                # å¦‚æœæ²¡æœ‰å®‰è£… pybloom_liveï¼Œä½¿ç”¨ç®€å• set æ›¿ä»£
                self._bloom_filter = set()
        return self._bloom_filter
    
    def add(self, turn: str, content: str):
        """æ·»åŠ ç´¢å¼•ï¼ˆåè¯çŸ­è¯­ç´¢å¼• + åŸæ–‡å­˜å‚¨ + å¢é‡æŒä¹…åŒ–ï¼‰
        
        Args:
            turn: memory_idï¼ˆå¦‚ mem_xxxï¼‰
            content: è®°å¿†å†…å®¹
        """
        # 1. å­˜å‚¨åŸæ–‡ï¼ˆç”¨äºå…œåº•æœç´¢ï¼‰
        self._raw_content[turn] = content
        
        # 1.5 å¢é‡æŒä¹…åŒ–åŸæ–‡ï¼ˆé¿å…é‡å¯ä¸¢å¤±ï¼‰
        self.append_raw_content(turn, content)
        
        # 2. æå–å¹¶ç´¢å¼•åè¯çŸ­è¯­ï¼ˆä¸»ç´¢å¼•ï¼‰
        phrases = self._extract_noun_phrases(content)
        
        for phrase in phrases:
            # æ·»åŠ åˆ°å¸ƒéš†è¿‡æ»¤å™¨
            if isinstance(self.bloom_filter, set):
                self.bloom_filter.add(phrase)
            else:
                self.bloom_filter.add(phrase)
            
            if phrase not in self.noun_phrases:
                self.noun_phrases[phrase] = []
            self.noun_phrases[phrase].append(turn)
        
        # 3. å®šæœŸä¿å­˜åè¯çŸ­è¯­ç´¢å¼•ï¼ˆæ¯ 100 æ¡ä¿å­˜ä¸€æ¬¡ï¼‰
        if len(self._raw_content) % 100 == 0:
            self._save_noun_phrases()
    
    def _save_noun_phrases(self):
        """åªä¿å­˜åè¯çŸ­è¯­ç´¢å¼•ï¼ˆè½»é‡çº§æ“ä½œï¼‰"""
        if not self._index_file:
            return
        try:
            os.makedirs(os.path.dirname(self._index_file), exist_ok=True)
            with open(self._index_file, 'w', encoding='utf-8') as f:
                json.dump(self.noun_phrases, f, ensure_ascii=False)
        except Exception as e:
            _safe_print(f"[NgramIndex] ä¿å­˜åè¯çŸ­è¯­ç´¢å¼•å¤±è´¥: {e}")
    
    def _extract_noun_phrases(self, content: str) -> List[str]:
        """æå–åè¯çŸ­è¯­ï¼ˆè€Œéæ‰€æœ‰n-gramï¼‰"""
        # ä½¿ç”¨ç®€å•è§„åˆ™ï¼š2-4å­—çš„ä¸­æ–‡è¯ç»„ + è‹±æ–‡å•è¯
        chinese_phrases = re.findall(r'[\u4e00-\u9fa5]{2,4}', content)
        english_words = re.findall(r'[a-zA-Z]{3,}', content)
        
        # è¿‡æ»¤åœç”¨è¯
        stopwords = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'æœ‰', 'è¿™', 'é‚£', 
            'the', 'a', 'is', 'are', 'to', 'and', 'of', 'in'
        }
        phrases = [p for p in chinese_phrases + english_words if p.lower() not in stopwords]
        
        return phrases
    
    def search(self, query: str) -> List[str]:
        """æœç´¢ï¼ˆå…ˆæŸ¥åè¯çŸ­è¯­ç´¢å¼•ï¼Œæ— ç»“æœæ—¶åŸæ–‡å…œåº•ï¼‰
        
        Returns:
            List[str]: åŒ¹é…çš„ memory_id åˆ—è¡¨
        """
        # ç¬¬ä¸€å±‚ï¼šåè¯çŸ­è¯­ç´¢å¼•æœç´¢
        phrases = self._extract_noun_phrases(query)
        
        candidate_turns: Set[str] = set()
        for phrase in phrases:
            # å…ˆç”¨å¸ƒéš†è¿‡æ»¤å™¨å¿«é€Ÿæ’é™¤ï¼ˆä½¿ç”¨ in è¿ç®—ç¬¦ï¼‰
            if phrase not in self.bloom_filter:
                continue
            
            if phrase in self.noun_phrases:
                candidate_turns.update(self.noun_phrases[phrase])
        
        if candidate_turns:
            return list(candidate_turns)
        
        # ç¬¬äºŒå±‚ï¼šåŸæ–‡å…œåº•æœç´¢ï¼ˆ"ç»ˆæå…œåº•"ï¼Œç¡®ä¿100%ä¸é—å¿˜ï¼‰
        return self._raw_text_fallback_search(query)
    
    def _raw_text_fallback_search(self, query: str, max_results: int = 50) -> List[str]:
        """åŸæ–‡å…œåº•æœç´¢ - æ‰«ææ‰€æœ‰åŸæ–‡å¯»æ‰¾åŒ…å«æŸ¥è¯¢å­ä¸²çš„è®°å¿†
        
        è¿™æ˜¯"ç»ˆæå…œåº•"æœºåˆ¶ï¼Œç¡®ä¿å³ä½¿åè¯çŸ­è¯­ç´¢å¼•æœªå‘½ä¸­ï¼Œ
        åªè¦åŸæ–‡ä¸­åŒ…å«æŸ¥è¯¢å†…å®¹å°±èƒ½æ‰¾åˆ°ã€‚
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§è¿”å›æ•°é‡ï¼ˆé¿å…è¿”å›å¤ªå¤šï¼‰
        
        Returns:
            List[str]: åŒ¹é…çš„ memory_id åˆ—è¡¨
        """
        results = []
        query_lower = query.lower()
        
        # æå–æŸ¥è¯¢ä¸­çš„å…³é”®å­ä¸²ï¼ˆ2-6å­—ç¬¦ï¼‰
        search_terms = self._extract_search_terms(query)
        
        for memory_id, content in self._raw_content.items():
            content_lower = content.lower()
            
            # ç›´æ¥å­ä¸²åŒ¹é…
            if query_lower in content_lower:
                results.append(memory_id)
                continue
            
            # æ£€æŸ¥ä»»ä¸€å…³é”®å­ä¸²æ˜¯å¦åŒ¹é…
            for term in search_terms:
                if term in content_lower:
                    results.append(memory_id)
                    break
            
            if len(results) >= max_results:
                break
        
        return results
    
    def _extract_search_terms(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–æœç´¢è¯ï¼ˆç”¨äºå…œåº•æœç´¢ï¼‰"""
        terms = []
        query_lower = query.lower()
        
        # ä¸­æ–‡ï¼š2-6å­—ç¬¦å­ä¸²
        for length in [4, 3, 2, 5, 6]:
            for i in range(len(query) - length + 1):
                substr = query[i:i+length]
                if re.match(r'^[\u4e00-\u9fa5]+$', substr):
                    terms.append(substr.lower())
        
        # è‹±æ–‡ï¼š3-8å­—ç¬¦å•è¯
        english_words = re.findall(r'[a-zA-Z]{3,8}', query)
        terms.extend([w.lower() for w in english_words])
        
        return list(set(terms))[:10]  # å»é‡å¹¶é™åˆ¶æ•°é‡
    
    def raw_search(self, query: str, max_results: int = 50) -> List[str]:
        """ç›´æ¥è¿›è¡ŒåŸæ–‡æœç´¢ï¼ˆè·³è¿‡åè¯çŸ­è¯­ç´¢å¼•ï¼‰
        
        ç”¨äºéœ€è¦ç²¾ç¡®åŸæ–‡åŒ¹é…çš„åœºæ™¯
        """
        return self._raw_text_fallback_search(query, max_results)
    
    def raw_search_parallel(
        self,
        query: str,
        max_results: int = 50,
        num_workers: int = 4
    ) -> List[str]:
        """Phase 3.6: å¹¶è¡Œåˆ†ç‰‡æ‰«æåŸæ–‡
        
        å°†åŸæ–‡æ•°æ®åˆ†æˆå¤šä¸ªåˆ†ç‰‡ï¼Œå¹¶è¡Œæ‰«æï¼Œæ˜¾è‘—æå‡å¤§è§„æ¨¡æ•°æ®çš„å…œåº•é€Ÿåº¦ã€‚
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            num_workers: å¹¶è¡Œçº¿ç¨‹æ•°
            
        Returns:
            åŒ¹é…çš„ memory_id åˆ—è¡¨
        """
        items = list(self._raw_content.items())
        if not items:
            return []
        
        # æ•°æ®é‡å°äº 1000 æ—¶ï¼Œç›´æ¥å•çº¿ç¨‹æœç´¢æ›´å¿«
        if len(items) < 1000:
            return self._raw_text_fallback_search(query, max_results)
        
        # åˆ†ç‰‡
        chunk_size = max(1, len(items) // num_workers)
        chunks = [items[i:i+chunk_size] for i in range(0, len(items), chunk_size)]
        
        # é¢„æå–æœç´¢è¯ï¼ˆé¿å…åœ¨æ¯ä¸ªçº¿ç¨‹ä¸­é‡å¤è®¡ç®—ï¼‰
        search_terms = self._extract_search_terms(query)
        query_lower = query.lower()
        
        # å¹¶è¡Œæ‰«æ
        all_results: List[str] = []
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [
                executor.submit(self._scan_chunk, query_lower, search_terms, chunk)
                for chunk in chunks
            ]
            
            for future in as_completed(futures):
                try:
                    chunk_results = future.result()
                    all_results.extend(chunk_results)
                    if len(all_results) >= max_results:
                        break
                except Exception as e:
                    _safe_print(f"[NgramIndex] å¹¶è¡Œæ‰«æå¤±è´¥: {e}")
                    continue
        
        return all_results[:max_results]
    
    def _scan_chunk(
        self,
        query_lower: str,
        search_terms: List[str],
        chunk: List[Tuple[str, str]]
    ) -> List[str]:
        """æ‰«æå•ä¸ªåˆ†ç‰‡ï¼ˆç”¨äºå¹¶è¡Œæœç´¢ï¼‰"""
        results = []
        
        for memory_id, content in chunk:
            content_lower = content.lower()
            
            # ç›´æ¥å­ä¸²åŒ¹é…
            if query_lower in content_lower:
                results.append(memory_id)
                continue
            
            # æ£€æŸ¥å…³é”®å­ä¸²
            for term in search_terms:
                if term in content_lower:
                    results.append(memory_id)
                    break
        
        return results
    
    def get_raw_content(self, memory_id: str) -> Optional[str]:
        """è·å–åŸæ–‡å†…å®¹"""
        return self._raw_content.get(memory_id)
    
    # ========== æŒä¹…åŒ–æ–¹æ³• ==========
    
    def _load(self):
        """ä»ç£ç›˜åŠ è½½ç´¢å¼•æ•°æ®"""
        # åŠ è½½åè¯çŸ­è¯­ç´¢å¼•
        if self._index_file and os.path.exists(self._index_file):
            try:
                with open(self._index_file, 'r', encoding='utf-8') as f:
                    self.noun_phrases = json.load(f)
                _safe_print(f"[NgramIndex] å·²åŠ è½½ {len(self.noun_phrases)} ä¸ªåè¯çŸ­è¯­ç´¢å¼•")
            except Exception as e:
                _safe_print(f"[NgramIndex] åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
        
        # åŠ è½½åŸæ–‡å†…å®¹ï¼ˆJSONL æ ¼å¼ï¼Œæ”¯æŒå¢é‡å†™å…¥ï¼‰
        if self._raw_content_file and os.path.exists(self._raw_content_file):
            try:
                with open(self._raw_content_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            try:
                                item = json.loads(line)
                                self._raw_content[item['id']] = item['content']
                            except json.JSONDecodeError:
                                continue
                _safe_print(f"[NgramIndex] å·²åŠ è½½ {len(self._raw_content)} æ¡åŸæ–‡å†…å®¹")
            except Exception as e:
                _safe_print(f"[NgramIndex] åŠ è½½åŸæ–‡å¤±è´¥: {e}")
    
    def save(self):
        """ä¿å­˜ç´¢å¼•æ•°æ®åˆ°ç£ç›˜"""
        if not self.data_path:
            return
        
        os.makedirs(self.data_path, exist_ok=True)
        
        # ä¿å­˜åè¯çŸ­è¯­ç´¢å¼•
        if self._index_file:
            try:
                with open(self._index_file, 'w', encoding='utf-8') as f:
                    json.dump(self.noun_phrases, f, ensure_ascii=False)
            except Exception as e:
                _safe_print(f"[NgramIndex] ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")
        
        # ä¿å­˜åŸæ–‡å†…å®¹ï¼ˆå®Œå…¨é‡å†™ï¼Œé¿å…é‡å¤ï¼‰
        if self._raw_content_file:
            try:
                with open(self._raw_content_file, 'w', encoding='utf-8') as f:
                    for memory_id, content in self._raw_content.items():
                        f.write(json.dumps({'id': memory_id, 'content': content}, ensure_ascii=False) + '\n')
            except Exception as e:
                _safe_print(f"[NgramIndex] ä¿å­˜åŸæ–‡å¤±è´¥: {e}")
    
    def append_raw_content(self, memory_id: str, content: str):
        """å¢é‡è¿½åŠ åŸæ–‡ï¼ˆé«˜æ•ˆå†™å…¥ï¼Œä¸ç”¨æ¯æ¬¡å…¨é‡ä¿å­˜ï¼‰"""
        if self._raw_content_file:
            try:
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(self._raw_content_file), exist_ok=True)
                with open(self._raw_content_file, 'a', encoding='utf-8') as f:
                    f.write(json.dumps({'id': memory_id, 'content': content}, ensure_ascii=False) + '\n')
            except Exception as e:
                _safe_print(f"[NgramIndex] è¿½åŠ åŸæ–‡å¤±è´¥: {e}")
    
    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰ç´¢å¼•å’ŒåŸæ–‡å†…å®¹"""
        self.noun_phrases.clear()
        self._raw_content.clear()
        
        # é‡ç½®å¸ƒéš†è¿‡æ»¤å™¨
        if self._bloom_filter is not None:
            if isinstance(self._bloom_filter, set):
                self._bloom_filter.clear()
            else:
                self._bloom_filter = None
        
        # åˆ é™¤ç£ç›˜æ–‡ä»¶
        if self._index_file and os.path.exists(self._index_file):
            try:
                os.remove(self._index_file)
            except Exception:
                pass
        
        if self._raw_content_file and os.path.exists(self._raw_content_file):
            try:
                os.remove(self._raw_content_file)
            except Exception:
                pass
    
    def remove_by_memory_ids(self, memory_ids: Set[str]) -> int:
        """æ ¹æ® memory_id åˆ é™¤ç´¢å¼•é¡¹
        
        Args:
            memory_ids: è¦åˆ é™¤çš„ memory_id é›†åˆ
        
        Returns:
            int: æ¸…ç†çš„åŸæ–‡æ•°é‡
        """
        removed_count = 0
        
        # ä»åŸæ–‡å­˜å‚¨ä¸­åˆ é™¤
        for mid in memory_ids:
            if mid in self._raw_content:
                del self._raw_content[mid]
                removed_count += 1
        
        # ä»åè¯çŸ­è¯­ç´¢å¼•ä¸­åˆ é™¤
        empty_phrases = []
        for phrase, mids in self.noun_phrases.items():
            self.noun_phrases[phrase] = [m for m in mids if m not in memory_ids]
            if len(self.noun_phrases[phrase]) == 0:
                empty_phrases.append(phrase)
        
        for phrase in empty_phrases:
            del self.noun_phrases[phrase]
        
        # ä¿å­˜æ›´æ–°
        if removed_count > 0:
            self.save()
        
        return removed_count
    
    def __len__(self) -> int:
        """è¿”å›ç´¢å¼•çš„åŸæ–‡æ•°é‡"""
        return len(self._raw_content)

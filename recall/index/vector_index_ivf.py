"""FAISS IVF å‘é‡ç´¢å¼•

Phase 3.5: ä¼ä¸šçº§æ€§èƒ½å¼•æ“

æ”¯æŒå¤§è§„æ¨¡å‘é‡æ£€ç´¢ï¼ˆ50ä¸‡-500ä¸‡å‘é‡ï¼‰ï¼Œä½¿ç”¨ç£ç›˜+å†…å­˜æ··åˆå­˜å‚¨ã€‚

ç‰¹ç‚¹ï¼š
- æ”¯æŒç™¾ä¸‡çº§å‘é‡
- ç£ç›˜ + å†…å­˜æ··åˆå­˜å‚¨
- å¯é…ç½®çš„ç²¾åº¦/é€Ÿåº¦æƒè¡¡
- å¤šç§Ÿæˆ·éš”ç¦»ï¼ˆé€šè¿‡ user_id è¿‡æ»¤ï¼‰

é€‚ç”¨åœºæ™¯ï¼š
- 50ä¸‡-500ä¸‡å‘é‡
- å†…å­˜å—é™ç¯å¢ƒ
"""

import os
import json
import logging
from typing import List, Tuple, Optional, Dict, Any

import numpy as np


# æ£€æŸ¥ FAISS æ˜¯å¦å¯ç”¨
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False


logger = logging.getLogger(__name__)


# Windows GBK ç¼–ç å…¼å®¹çš„å®‰å…¨æ‰“å°å‡½æ•°
def _safe_print(msg: str) -> None:
    """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œæ›¿æ¢ emoji ä¸º ASCII ç­‰ä»·ç‰©ä»¥é¿å… Windows GBK ç¼–ç é”™è¯¯"""
    emoji_map = {
        'ğŸ“¥': '[IN]', 'ğŸ“¤': '[OUT]', 'ğŸ”': '[SEARCH]', 'âœ…': '[OK]', 'âŒ': '[FAIL]',
        'âš ï¸': '[WARN]', 'ğŸ’¾': '[SAVE]', 'ğŸ—ƒï¸': '[DB]', 'ğŸ§¹': '[CLEAN]', 'ğŸ“Š': '[STATS]',
        'ğŸ”„': '[SYNC]', 'ğŸ“¦': '[PKG]', 'ğŸš€': '[START]', 'ğŸ¯': '[TARGET]', 'ğŸ’¡': '[HINT]',
        'ğŸ”§': '[FIX]', 'ğŸ“': '[NOTE]', 'ğŸ‰': '[DONE]', 'â±ï¸': '[TIME]', 'ğŸŒ': '[NET]',
    }
    for emoji, ascii_equiv in emoji_map.items():
        msg = msg.replace(emoji, ascii_equiv)
    try:
        print(msg)
    except UnicodeEncodeError:
        print(msg.encode('ascii', errors='replace').decode('ascii'))


class VectorIndexIVF:
    """FAISS IVF å‘é‡ç´¢å¼• - æ”¯æŒç£ç›˜å­˜å‚¨
    
    ç‰¹ç‚¹ï¼š
    - æ”¯æŒç™¾ä¸‡çº§å‘é‡ï¼ˆ50ä¸‡-500ä¸‡ï¼‰
    - ç£ç›˜ + å†…å­˜æ··åˆå­˜å‚¨
    - å¯é…ç½®çš„ç²¾åº¦/é€Ÿåº¦æƒè¡¡
    - å¤šç§Ÿæˆ·éš”ç¦»ï¼ˆé€šè¿‡ user_id è¿‡æ»¤ï¼‰
    
    é€‚ç”¨åœºæ™¯ï¼š
    - 50ä¸‡-500ä¸‡å‘é‡
    - å†…å­˜å—é™ç¯å¢ƒ
    
    ä½¿ç”¨æ–¹å¼ï¼š
        index = VectorIndexIVF(
            data_path="./recall_data/indexes",
            dimension=1024,
            nlist=100,
            nprobe=10
        )
        
        # æ·»åŠ å‘é‡
        index.add("doc_1", embedding, user_id="user_123")
        
        # æœç´¢ï¼ˆæ”¯æŒå¤šç§Ÿæˆ·è¿‡æ»¤ï¼‰
        results = index.search(query_embedding, top_k=10, user_id="user_123")
        
    âš ï¸ éœ€è¦å…ˆå®‰è£… faissï¼š
        pip install faiss-cpu  # CPU ç‰ˆæœ¬
        pip install faiss-gpu  # GPU ç‰ˆæœ¬ï¼ˆéœ€è¦ CUDAï¼‰
    """
    
    def __init__(
        self,
        data_path: str,
        dimension: int = 1024,
        nlist: int = 100,         # èšç±»ä¸­å¿ƒæ•°é‡
        nprobe: int = 10,         # æœç´¢æ—¶æ£€æŸ¥çš„èšç±»æ•°
        use_gpu: bool = False,
        min_train_size: int = None  # æœ€å°è®­ç»ƒæ ·æœ¬æ•°ï¼Œé»˜è®¤ä¸º nlist
    ):
        """åˆå§‹åŒ– IVF ç´¢å¼•
        
        Args:
            data_path: æ•°æ®å­˜å‚¨è·¯å¾„
            dimension: å‘é‡ç»´åº¦
            nlist: èšç±»ä¸­å¿ƒæ•°é‡ï¼ˆè¶Šå¤§ç²¾åº¦è¶Šé«˜ä½†é€Ÿåº¦è¶Šæ…¢ï¼‰
            nprobe: æœç´¢æ—¶æ£€æŸ¥çš„èšç±»æ•°ï¼ˆè¶Šå¤§å¬å›ç‡è¶Šé«˜ä½†é€Ÿåº¦è¶Šæ…¢ï¼‰
            use_gpu: æ˜¯å¦ä½¿ç”¨ GPU
            min_train_size: æœ€å°è®­ç»ƒæ ·æœ¬æ•°
            
        Raises:
            ImportError: å¦‚æœ faiss æœªå®‰è£…
        """
        if not FAISS_AVAILABLE:
            raise ImportError(
                "FAISS not installed. Install with: pip install faiss-cpu\n"
                "Or for GPU: pip install faiss-gpu"
            )
        
        self.data_path = data_path
        self.dimension = dimension
        self.nlist = nlist
        self.nprobe = nprobe
        self.use_gpu = use_gpu
        self.min_train_size = min_train_size or nlist
        
        # æ–‡ä»¶è·¯å¾„
        os.makedirs(data_path, exist_ok=True)
        self.index_file = os.path.join(data_path, "vector_index_ivf.faiss")
        self.mapping_file = os.path.join(data_path, "vector_mapping_ivf.npy")
        self.metadata_file = os.path.join(data_path, "vector_metadata_ivf.json")
        self.pending_file = os.path.join(data_path, "vector_pending_ivf.npy")
        
        # å†…å­˜æ•°æ®
        self.index: Optional[faiss.Index] = None
        self.id_mapping: List[str] = []  # å†…éƒ¨ ID -> æ–‡æ¡£ ID
        self.doc_metadata: Dict[str, Dict[str, Any]] = {}  # æ–‡æ¡£ ID -> å…ƒæ•°æ®ï¼ˆå« user_idï¼‰
        self._pending_vectors: List[np.ndarray] = []  # å¾…è®­ç»ƒçš„å‘é‡
        self._pending_ids: List[str] = []  # å¾…è®­ç»ƒçš„æ–‡æ¡£ ID
        
        self._load_or_create()
        
        logger.info(f"[VectorIndexIVF] Initialized at {data_path}, dimension={dimension}, nlist={nlist}")
    
    def _load_or_create(self):
        """åŠ è½½æˆ–åˆ›å»ºç´¢å¼•"""
        if os.path.exists(self.index_file):
            self._load()
        else:
            self._create()
    
    def _load(self):
        """åŠ è½½å·²æœ‰ç´¢å¼•"""
        try:
            self.index = faiss.read_index(self.index_file)
            self.index.nprobe = self.nprobe
            
            # åŠ è½½ ID æ˜ å°„
            if os.path.exists(self.mapping_file):
                self.id_mapping = list(np.load(self.mapping_file, allow_pickle=True))
            
            # åŠ è½½å…ƒæ•°æ®
            if os.path.exists(self.metadata_file):
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    self.doc_metadata = json.load(f)
            
            # åŠ è½½å¾…å¤„ç†å‘é‡
            if os.path.exists(self.pending_file):
                pending_data = np.load(self.pending_file, allow_pickle=True).item()
                self._pending_vectors = list(pending_data.get('vectors', []))
                self._pending_ids = list(pending_data.get('ids', []))
            
            logger.info(f"[VectorIndexIVF] Loaded {self.index.ntotal} vectors, "
                       f"{len(self._pending_vectors)} pending")
        except Exception as e:
            logger.error(f"[VectorIndexIVF] Failed to load index: {e}")
            self._create()
    
    def _create(self):
        """åˆ›å»ºæ–°ç´¢å¼•"""
        # åˆ›å»º IVF ç´¢å¼•
        quantizer = faiss.IndexFlatIP(self.dimension)  # å†…ç§¯ï¼ˆç”¨äºå½’ä¸€åŒ–å‘é‡ï¼‰
        self.index = faiss.IndexIVFFlat(
            quantizer,
            self.dimension,
            self.nlist,
            faiss.METRIC_INNER_PRODUCT
        )
        self.index.nprobe = self.nprobe
        
        logger.info(f"[VectorIndexIVF] Created new IVF index with nlist={self.nlist}")
    
    def add(
        self,
        doc_id: str,
        embedding: List[float],
        user_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> bool:
        """æ·»åŠ å‘é‡
        
        Args:
            doc_id: æ–‡æ¡£ID
            embedding: å‘é‡ï¼ˆåˆ—è¡¨æˆ– numpy æ•°ç»„ï¼‰
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºå¤šç§Ÿæˆ·éš”ç¦»ï¼‰
            metadata: é¢å¤–å…ƒæ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸæ·»åŠ 
        """
        try:
            vector = np.array([embedding], dtype=np.float32)
            
            # æ£€æŸ¥ç»´åº¦
            if vector.shape[1] != self.dimension:
                logger.warning(f"[VectorIndexIVF] Dimension mismatch: expected {self.dimension}, "
                              f"got {vector.shape[1]}")
                return False
            
            # å½’ä¸€åŒ–ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            faiss.normalize_L2(vector)
            
            # å­˜å‚¨å…ƒæ•°æ®ï¼ˆç”¨äºç”¨æˆ·è¿‡æ»¤ï¼‰
            meta = metadata.copy() if metadata else {}
            if user_id:
                meta['user_id'] = user_id
            if meta:
                self.doc_metadata[doc_id] = meta
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²è®­ç»ƒ
            if not self.index.is_trained:
                # IVF ç´¢å¼•éœ€è¦è®­ç»ƒï¼Œå…ˆç´¯ç§¯æ•°æ®
                self._pending_vectors.append(vector[0])
                self._pending_ids.append(doc_id)
                
                # æ£€æŸ¥æ˜¯å¦å¯ä»¥å¼€å§‹è®­ç»ƒ
                if len(self._pending_vectors) >= self.min_train_size:
                    self._train_and_add()
                else:
                    self._save_pending()
                
                return True
            
            # å·²è®­ç»ƒï¼Œç›´æ¥æ·»åŠ 
            self.index.add(vector)
            self.id_mapping.append(doc_id)
            self._save()
            
            return True
        except Exception as e:
            logger.error(f"[VectorIndexIVF] Failed to add vector for {doc_id}: {e}")
            return False
    
    def _train_and_add(self):
        """è®­ç»ƒç´¢å¼•å¹¶æ·»åŠ å¾…å¤„ç†çš„å‘é‡"""
        if not self._pending_vectors:
            return
        
        vectors = np.array(self._pending_vectors, dtype=np.float32)
        
        _safe_print(f"[VectorIndexIVF] Training on {len(vectors)} vectors...")
        
        # è®­ç»ƒ
        self.index.train(vectors)
        
        # æ·»åŠ æ‰€æœ‰å¾…å¤„ç†å‘é‡
        self.index.add(vectors)
        self.id_mapping.extend(self._pending_ids)
        
        # æ¸…ç©ºå¾…å¤„ç†
        self._pending_vectors = []
        self._pending_ids = []
        
        # åˆ é™¤å¾…å¤„ç†æ–‡ä»¶
        if os.path.exists(self.pending_file):
            os.remove(self.pending_file)
        
        self._save()
        
        _safe_print(f"[VectorIndexIVF] [DONE] Training complete, {self.index.ntotal} vectors indexed")
    
    def train(self, embeddings: List[List[float]]):
        """æ‰‹åŠ¨è®­ç»ƒç´¢å¼•ï¼ˆç”¨äºæ‰¹é‡å¯¼å…¥åœºæ™¯ï¼‰
        
        Args:
            embeddings: è®­ç»ƒç”¨çš„å‘é‡åˆ—è¡¨
        """
        if len(embeddings) < self.nlist:
            _safe_print(f"[VectorIndexIVF] [WARN] Not enough vectors for training "
                       f"({len(embeddings)} < {self.nlist})")
            return
        
        vectors = np.array(embeddings, dtype=np.float32)
        faiss.normalize_L2(vectors)
        
        self.index.train(vectors)
        self._save()
        
        _safe_print(f"[VectorIndexIVF] [DONE] Manual training complete")
    
    def search(
        self,
        query_embedding: List[float],
        top_k: int = 10,
        user_id: Optional[str] = None
    ) -> List[Tuple[str, float]]:
        """æœç´¢ç›¸ä¼¼å‘é‡
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›æ•°é‡
            user_id: ç”¨æˆ·IDè¿‡æ»¤ï¼ˆå¤šç§Ÿæˆ·éš”ç¦»ï¼‰
            
        Returns:
            [(æ–‡æ¡£ID, ç›¸ä¼¼åº¦åˆ†æ•°), ...]
        """
        if not self.index.is_trained or self.index.ntotal == 0:
            return []
        
        try:
            query = np.array([query_embedding], dtype=np.float32)
            
            # æ£€æŸ¥ç»´åº¦
            if query.shape[1] != self.dimension:
                logger.warning(f"[VectorIndexIVF] Query dimension mismatch: "
                              f"expected {self.dimension}, got {query.shape[1]}")
                return []
            
            faiss.normalize_L2(query)
            
            # å¤šå–ä¸€äº›ç”¨äºè¿‡æ»¤ï¼ˆå¦‚æœéœ€è¦ç”¨æˆ·è¿‡æ»¤ï¼‰
            search_k = top_k * 5 if user_id else top_k
            search_k = min(search_k, self.index.ntotal)
            
            distances, indices = self.index.search(query, search_k)
            
            results = []
            for dist, idx in zip(distances[0], indices[0]):
                if idx < 0 or idx >= len(self.id_mapping):
                    continue
                
                doc_id = self.id_mapping[idx]
                
                # ç”¨æˆ·è¿‡æ»¤ï¼ˆå¤šç§Ÿæˆ·éš”ç¦»ä¿éšœï¼‰
                if user_id and doc_id in self.doc_metadata:
                    meta = self.doc_metadata[doc_id]
                    if meta.get('user_id') != user_id:
                        continue  # è·³è¿‡å…¶ä»–ç”¨æˆ·çš„æ–‡æ¡£
                
                results.append((doc_id, float(dist)))
                
                if len(results) >= top_k:
                    break
            
            return results
        except Exception as e:
            logger.error(f"[VectorIndexIVF] Search failed: {e}")
            return []
    
    def _save(self):
        """ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®"""
        try:
            # ä¿å­˜ç´¢å¼•
            faiss.write_index(self.index, self.index_file)
            
            # ä¿å­˜ ID æ˜ å°„
            np.save(self.mapping_file, np.array(self.id_mapping, dtype=object))
            
            # ä¿å­˜å…ƒæ•°æ®
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.doc_metadata, f, ensure_ascii=False)
            
            logger.debug(f"[VectorIndexIVF] Saved {self.index.ntotal} vectors")
        except Exception as e:
            logger.error(f"[VectorIndexIVF] Failed to save: {e}")
    
    def _save_pending(self):
        """ä¿å­˜å¾…å¤„ç†çš„å‘é‡"""
        try:
            pending_data = {
                'vectors': self._pending_vectors,
                'ids': self._pending_ids
            }
            np.save(self.pending_file, pending_data)
        except Exception as e:
            logger.error(f"[VectorIndexIVF] Failed to save pending vectors: {e}")
    
    def remove(self, doc_id: str) -> bool:
        """ç§»é™¤å‘é‡ï¼ˆæ ‡è®°åˆ é™¤ï¼Œä¸å®é™…åˆ é™¤ï¼‰
        
        æ³¨æ„ï¼šFAISS IVF ä¸æ”¯æŒç›´æ¥åˆ é™¤å‘é‡ï¼Œè¿™é‡Œåªæ˜¯æ ‡è®°ã€‚
        å®é™…åˆ é™¤éœ€è¦é‡å»ºç´¢å¼•ã€‚
        
        Args:
            doc_id: æ–‡æ¡£ID
            
        Returns:
            æ˜¯å¦æ‰¾åˆ°å¹¶æ ‡è®°
        """
        if doc_id in self.doc_metadata:
            self.doc_metadata[doc_id]['_deleted'] = True
            self._save()
            return True
        return False
    
    def rebuild(self):
        """é‡å»ºç´¢å¼•ï¼ˆåˆ é™¤æ ‡è®°ä¸ºåˆ é™¤çš„å‘é‡ï¼‰"""
        if not self.index.is_trained or self.index.ntotal == 0:
            return
        
        # æ”¶é›†æœªåˆ é™¤çš„å‘é‡
        valid_ids = []
        valid_indices = []
        
        for i, doc_id in enumerate(self.id_mapping):
            if doc_id not in self.doc_metadata or not self.doc_metadata.get(doc_id, {}).get('_deleted'):
                valid_ids.append(doc_id)
                valid_indices.append(i)
        
        if len(valid_indices) == len(self.id_mapping):
            logger.info("[VectorIndexIVF] No deleted vectors to rebuild")
            return
        
        # è·å–æœ‰æ•ˆå‘é‡
        valid_vectors = np.zeros((len(valid_indices), self.dimension), dtype=np.float32)
        for new_idx, old_idx in enumerate(valid_indices):
            valid_vectors[new_idx] = self.index.reconstruct(old_idx)
        
        # é‡å»ºç´¢å¼•
        self._create()
        self.index.train(valid_vectors)
        self.index.add(valid_vectors)
        self.id_mapping = valid_ids
        
        # æ¸…ç†å…ƒæ•°æ®
        self.doc_metadata = {k: v for k, v in self.doc_metadata.items() if not v.get('_deleted')}
        
        self._save()
        
        _safe_print(f"[VectorIndexIVF] [DONE] Rebuilt index with {self.index.ntotal} vectors")
    
    @property
    def size(self) -> int:
        """å‘é‡æ•°é‡"""
        return self.index.ntotal if self.index else 0
    
    @property
    def pending_size(self) -> int:
        """å¾…å¤„ç†å‘é‡æ•°é‡"""
        return len(self._pending_vectors)
    
    @property
    def is_trained(self) -> bool:
        """ç´¢å¼•æ˜¯å¦å·²è®­ç»ƒ"""
        return self.index.is_trained if self.index else False
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "size": self.size,
            "pending_size": self.pending_size,
            "is_trained": self.is_trained,
            "dimension": self.dimension,
            "nlist": self.nlist,
            "nprobe": self.nprobe,
            "metadata_count": len(self.doc_metadata),
        }

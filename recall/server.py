"""Recall API Server - FastAPI HTTP æ¥å£"""

import os
import time
import asyncio
from typing import List, Dict, Any, Optional
from contextlib import asynccontextmanager
from pathlib import Path

from fastapi import FastAPI, HTTPException, Query, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from .version import __version__
from .engine import RecallEngine


# ==================== é…ç½®æ–‡ä»¶ç®¡ç† ====================

# æ”¯æŒçš„é…ç½®é¡¹ï¼ˆç»Ÿä¸€ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼ï¼‰
SUPPORTED_CONFIG_KEYS = {
    # Embedding é…ç½®
    'EMBEDDING_API_KEY',
    'EMBEDDING_API_BASE',
    'EMBEDDING_MODEL',
    'EMBEDDING_DIMENSION',
    # Embedding é€Ÿç‡é™åˆ¶
    'EMBEDDING_RATE_LIMIT',       # æ¯æ—¶é—´çª—å£æœ€å¤§è¯·æ±‚æ•°
    'EMBEDDING_RATE_WINDOW',      # é€Ÿç‡é™åˆ¶æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
    # Embedding æ¨¡å¼
    'RECALL_EMBEDDING_MODE',
    # LLM é…ç½®ï¼ˆç”¨äºä¼ç¬”åˆ†æå™¨ç­‰åŠŸèƒ½ï¼‰
    'LLM_API_KEY',
    'LLM_API_BASE',
    'LLM_MODEL',
    # ä¼ç¬”åˆ†æå™¨é…ç½®
    'FORESHADOWING_LLM_ENABLED',
    'FORESHADOWING_TRIGGER_INTERVAL',
    'FORESHADOWING_AUTO_PLANT',
    'FORESHADOWING_AUTO_RESOLVE',
    'FORESHADOWING_MAX_RETURN',       # ä¼ç¬”å¬å›æ•°é‡
    'FORESHADOWING_MAX_ACTIVE',       # æ´»è·ƒä¼ç¬”æ•°é‡ä¸Šé™
    # æŒä¹…æ¡ä»¶ç³»ç»Ÿé…ç½®
    'CONTEXT_TRIGGER_INTERVAL',       # æ¡ä»¶æå–è§¦å‘é—´éš”ï¼ˆæ¯Nè½®ï¼‰
    'CONTEXT_MAX_CONTEXT_TURNS',      # å¯¹è¯è·å–èŒƒå›´ï¼ˆç”¨äºåˆ†ææ—¶è·å–çš„è½®æ•°ï¼‰
    'CONTEXT_MAX_PER_TYPE',           # æ¯ç±»å‹æ¡ä»¶ä¸Šé™
    'CONTEXT_MAX_TOTAL',              # æ¡ä»¶æ€»æ•°ä¸Šé™
    'CONTEXT_DECAY_DAYS',             # è¡°å‡å¼€å§‹å¤©æ•°
    'CONTEXT_DECAY_RATE',             # æ¯æ¬¡è¡°å‡æ¯”ä¾‹
    'CONTEXT_MIN_CONFIDENCE',         # æœ€ä½ç½®ä¿¡åº¦ï¼ˆä½äºæ­¤è‡ªåŠ¨å½’æ¡£ï¼‰
    # ä¸Šä¸‹æ–‡æ„å»ºé…ç½®ï¼ˆbuild_contextï¼‰
    'BUILD_CONTEXT_INCLUDE_RECENT',   # æ„å»ºä¸Šä¸‹æ–‡æ—¶åŒ…å«çš„æœ€è¿‘å¯¹è¯æ•°
    'PROACTIVE_REMINDER_ENABLED',     # æ˜¯å¦å¯ç”¨ä¸»åŠ¨æé†’ï¼ˆé‡è¦ä¿¡æ¯é•¿æœŸæœªæåŠæ—¶æé†’ï¼‰
    'PROACTIVE_REMINDER_TURNS',       # ä¸»åŠ¨æé†’é˜ˆå€¼ï¼ˆè¶…è¿‡å¤šå°‘è½®æœªæåŠåˆ™æé†’ï¼‰
    # æ™ºèƒ½å»é‡é…ç½®ï¼ˆæŒä¹…æ¡ä»¶å’Œä¼ç¬”ç³»ç»Ÿï¼‰
    'DEDUP_EMBEDDING_ENABLED',
    'DEDUP_HIGH_THRESHOLD',
    'DEDUP_LOW_THRESHOLD',
    
    # ====== v4.0 Phase 1/2 æ–°å¢é…ç½®é¡¹ ======
    # æ—¶æ€çŸ¥è¯†å›¾è°±é…ç½®
    'TEMPORAL_GRAPH_ENABLED',         # æ˜¯å¦å¯ç”¨æ—¶æ€çŸ¥è¯†å›¾è°±
    'TEMPORAL_GRAPH_BACKEND',         # å›¾è°±åç«¯: file/neo4j/falkordb
    'TEMPORAL_DECAY_RATE',            # æ—¶æ€ä¿¡æ¯è¡°å‡ç‡ (0.0-1.0)
    'TEMPORAL_MAX_HISTORY',           # ä¿ç•™çš„æœ€å¤§æ—¶æ€å†å²è®°å½•æ•°
    # çŸ›ç›¾æ£€æµ‹ä¸ç®¡ç†é…ç½®
    'CONTRADICTION_DETECTION_ENABLED',  # æ˜¯å¦å¯ç”¨çŸ›ç›¾æ£€æµ‹
    'CONTRADICTION_AUTO_RESOLVE',     # æ˜¯å¦è‡ªåŠ¨è§£å†³çŸ›ç›¾ï¼ˆæ¨è falseï¼‰
    'CONTRADICTION_DETECTION_STRATEGY',  # æ£€æµ‹ç­–ç•¥: RULE/LLM/MIXED/AUTO
    'CONTRADICTION_SIMILARITY_THRESHOLD',  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆç”¨äºæ£€æµ‹æ½œåœ¨çŸ›ç›¾ï¼‰
    # å…¨æ–‡æ£€ç´¢é…ç½® (BM25)
    'FULLTEXT_ENABLED',               # æ˜¯å¦å¯ç”¨å…¨æ–‡æ£€ç´¢
    'FULLTEXT_K1',                    # BM25 k1 å‚æ•°ï¼ˆè¯é¢‘é¥±å’Œåº¦ï¼‰
    'FULLTEXT_B',                     # BM25 b å‚æ•°ï¼ˆæ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–ï¼‰
    'FULLTEXT_WEIGHT',                # å…¨æ–‡æ£€ç´¢åœ¨æ··åˆæœç´¢ä¸­çš„æƒé‡
    # æ™ºèƒ½æŠ½å–å™¨é…ç½® (SmartExtractor)
    'SMART_EXTRACTOR_MODE',           # æ¨¡å¼: RULES/ADAPTIVE/LLM
    'SMART_EXTRACTOR_COMPLEXITY_THRESHOLD',  # å¤æ‚åº¦é˜ˆå€¼ï¼ˆè¶…è¿‡æ­¤å€¼ä½¿ç”¨ LLMï¼‰
    'SMART_EXTRACTOR_ENABLE_TEMPORAL',  # æ˜¯å¦å¯ç”¨æ—¶æ€æ£€æµ‹
    # é¢„ç®—ç®¡ç†é…ç½® (BudgetManager)
    'BUDGET_DAILY_LIMIT',             # æ¯æ—¥é¢„ç®—ä¸Šé™ (USD)
    'BUDGET_HOURLY_LIMIT',            # æ¯å°æ—¶é¢„ç®—ä¸Šé™ (USD)
    'BUDGET_RESERVE',                 # ä¿ç•™é¢„ç®—æ¯”ä¾‹ (0.0-1.0)
    'BUDGET_ALERT_THRESHOLD',         # é¢„ç®—è­¦å‘Šé˜ˆå€¼ (0.0-1.0)
    # ä¸‰é˜¶æ®µå»é‡é…ç½® (ThreeStageDeduplicator)
    'DEDUP_JACCARD_THRESHOLD',        # Jaccard ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé˜¶æ®µ1ï¼‰
    'DEDUP_SEMANTIC_THRESHOLD',       # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé˜¶æ®µ2 é«˜ï¼‰
    'DEDUP_SEMANTIC_LOW_THRESHOLD',   # è¯­ä¹‰ç›¸ä¼¼åº¦ä½é˜ˆå€¼ï¼ˆé˜¶æ®µ2 ä½ï¼‰
    'DEDUP_LLM_ENABLED',              # æ˜¯å¦å¯ç”¨ LLM ç¡®è®¤ï¼ˆé˜¶æ®µ3ï¼‰
    
    # ====== v4.0 Phase 3 åä¸€å±‚æ£€ç´¢å™¨é…ç½®é¡¹ ======
    'ELEVEN_LAYER_RETRIEVER_ENABLED', # æ˜¯å¦å¯ç”¨åä¸€å±‚æ£€ç´¢å™¨
    # å±‚å¼€å…³é…ç½®
    'RETRIEVAL_L1_BLOOM_ENABLED',
    'RETRIEVAL_L2_TEMPORAL_ENABLED',
    'RETRIEVAL_L3_INVERTED_ENABLED',
    'RETRIEVAL_L4_ENTITY_ENABLED',
    'RETRIEVAL_L5_GRAPH_ENABLED',
    'RETRIEVAL_L6_NGRAM_ENABLED',
    'RETRIEVAL_L7_VECTOR_COARSE_ENABLED',
    'RETRIEVAL_L8_VECTOR_FINE_ENABLED',
    'RETRIEVAL_L9_RERANK_ENABLED',
    'RETRIEVAL_L10_CROSS_ENCODER_ENABLED',
    'RETRIEVAL_L11_LLM_ENABLED',
    # Top-K é…ç½®
    'RETRIEVAL_L2_TEMPORAL_TOP_K',
    'RETRIEVAL_L3_INVERTED_TOP_K',
    'RETRIEVAL_L4_ENTITY_TOP_K',
    'RETRIEVAL_L5_GRAPH_TOP_K',
    'RETRIEVAL_L6_NGRAM_TOP_K',
    'RETRIEVAL_L7_VECTOR_TOP_K',
    'RETRIEVAL_L10_CROSS_ENCODER_TOP_K',
    'RETRIEVAL_L11_LLM_TOP_K',
    # é˜ˆå€¼ä¸æœ€ç»ˆè¾“å‡ºé…ç½®
    'RETRIEVAL_FINE_RANK_THRESHOLD',
    'RETRIEVAL_FINAL_TOP_K',
    # L5 å›¾éå†é…ç½®
    'RETRIEVAL_L5_GRAPH_MAX_DEPTH',
    'RETRIEVAL_L5_GRAPH_MAX_ENTITIES',
    'RETRIEVAL_L5_GRAPH_DIRECTION',
    # L10 CrossEncoder é…ç½®
    'RETRIEVAL_L10_CROSS_ENCODER_MODEL',
    # L11 LLM é…ç½®
    'RETRIEVAL_L11_LLM_TIMEOUT',
    # æƒé‡é…ç½®
    'RETRIEVAL_WEIGHT_INVERTED',
    'RETRIEVAL_WEIGHT_ENTITY',
    'RETRIEVAL_WEIGHT_GRAPH',
    'RETRIEVAL_WEIGHT_NGRAM',
    'RETRIEVAL_WEIGHT_VECTOR',
    'RETRIEVAL_WEIGHT_TEMPORAL',
}


def get_config_file_path() -> Path:
    """è·å–é…ç½®æ–‡ä»¶è·¯å¾„"""
    # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ•°æ®ç›®å½•
    data_root = os.environ.get('RECALL_DATA_ROOT', './recall_data')
    return Path(data_root) / 'config' / 'api_keys.env'


def get_default_config_content() -> str:
    """è·å–é»˜è®¤é…ç½®æ–‡ä»¶å†…å®¹"""
    return '''# ============================================================================
# Recall-AI é…ç½®æ–‡ä»¶
# Recall-AI Configuration File
# ============================================================================
#
# âš¡ å¿«é€Ÿå¼€å§‹ (90%çš„ç”¨æˆ·åªéœ€è¦é…ç½®è¿™é‡Œ)
# âš¡ Quick Start (90% users only need to configure this section)
#
# 1. å¡«å†™ EMBEDDING_API_KEY å’Œ EMBEDDING_API_BASE (å¿…é¡»)
# 2. å¡«å†™ LLM_API_KEY å’Œ LLM_API_BASE (å¯é€‰ï¼Œç”¨äºä¼ç¬”/çŸ›ç›¾ç­‰é«˜çº§åŠŸèƒ½)
# 3. å¯åŠ¨æœåŠ¡: ./start.ps1 æˆ– ./start.sh
#
# å…¶ä»–æ‰€æœ‰é…ç½®é¡¹éƒ½æœ‰åˆç†çš„é»˜è®¤å€¼ï¼Œæ— éœ€ä¿®æ”¹ï¼
# All other settings have sensible defaults, no changes needed!
#
# ============================================================================

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  â­ å¿…å¡«é…ç½® - REQUIRED CONFIGURATION                                    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ----------------------------------------------------------------------------
# Embedding é…ç½® (OpenAI å…¼å®¹æ¥å£) - å¿…å¡«!
# Embedding Configuration (OpenAI Compatible API) - REQUIRED!
# ----------------------------------------------------------------------------
# ç¤ºä¾‹ (Examples):
#   OpenAI:      https://api.openai.com/v1
#   SiliconFlow: https://api.siliconflow.cn/v1  (æ¨èå›½å†…ç”¨æˆ·)
#   Ollama:      http://localhost:11434/v1
# ----------------------------------------------------------------------------
EMBEDDING_API_KEY=
EMBEDDING_API_BASE=
EMBEDDING_MODEL=
EMBEDDING_DIMENSION=1024

# Embedding æ¨¡å¼: auto(è‡ªåŠ¨æ£€æµ‹), local(æœ¬åœ°), api(è¿œç¨‹API)
# Embedding Mode: auto(auto detect), local(local model), api(remote API)
RECALL_EMBEDDING_MODE=auto

# ----------------------------------------------------------------------------
# LLM é…ç½® (OpenAI å…¼å®¹æ¥å£) - ç”¨äºä¼ç¬”åˆ†æã€çŸ›ç›¾æ£€æµ‹ç­‰é«˜çº§åŠŸèƒ½
# LLM Configuration (OpenAI Compatible API) - For foreshadowing, contradiction, etc.
# ----------------------------------------------------------------------------
LLM_API_KEY=
LLM_API_BASE=
LLM_MODEL=

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  âš™ï¸ å¯é€‰é…ç½® - OPTIONAL CONFIGURATION (ä»¥ä¸‹å†…å®¹å¯ä¿æŒé»˜è®¤å€¼)              â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ----------------------------------------------------------------------------
# Embedding API é€Ÿç‡é™åˆ¶
# Embedding API Rate Limiting
# ----------------------------------------------------------------------------
# æ¯æ—¶é—´çª—å£æœ€å¤§è¯·æ±‚æ•°ï¼ˆé»˜è®¤10ï¼Œè®¾ä¸º0ç¦ç”¨ï¼‰
# Max requests per time window (default 10, set 0 to disable)
EMBEDDING_RATE_LIMIT=10

# é€Ÿç‡é™åˆ¶æ—¶é—´çª—å£ï¼ˆç§’ï¼Œé»˜è®¤60ï¼‰
# Rate limit time window in seconds (default 60)
EMBEDDING_RATE_WINDOW=60

# ----------------------------------------------------------------------------
# ä¼ç¬”åˆ†æå™¨é…ç½®
# Foreshadowing Analyzer Configuration
# ----------------------------------------------------------------------------
# æ˜¯å¦å¯ç”¨ LLM ä¼ç¬”åˆ†æ (true/false)
# Enable LLM-based foreshadowing analysis
FORESHADOWING_LLM_ENABLED=false

# åˆ†æè§¦å‘é—´éš”ï¼ˆæ¯Nè½®å¯¹è¯è§¦å‘ä¸€æ¬¡åˆ†æï¼Œæœ€å°1ï¼‰
# Analysis trigger interval (trigger analysis every N turns, minimum 1)
FORESHADOWING_TRIGGER_INTERVAL=10

# è‡ªåŠ¨åŸ‹ä¸‹ä¼ç¬” (true/false)
# Automatically plant detected foreshadowing
FORESHADOWING_AUTO_PLANT=true

# è‡ªåŠ¨è§£å†³ä¼ç¬” (true/false) - å»ºè®®ä¿æŒ falseï¼Œè®©ç”¨æˆ·æ‰‹åŠ¨ç¡®è®¤
# Automatically resolve detected foreshadowing (recommend false)
FORESHADOWING_AUTO_RESOLVE=false

# ä¼ç¬”å¬å›æ•°é‡ï¼ˆæ„å»ºä¸Šä¸‹æ–‡æ—¶è¿”å›çš„ä¼ç¬”æ•°é‡ï¼‰
# Number of foreshadowings to return when building context
FORESHADOWING_MAX_RETURN=10

# æ´»è·ƒä¼ç¬”æ•°é‡ä¸Šé™ï¼ˆè¶…å‡ºæ—¶è‡ªåŠ¨å½’æ¡£ä½ä¼˜å…ˆçº§çš„ä¼ç¬”ï¼‰
# Max active foreshadowings (auto-archive low-priority ones when exceeded)
FORESHADOWING_MAX_ACTIVE=50

# ----------------------------------------------------------------------------
# æŒä¹…æ¡ä»¶ç³»ç»Ÿé…ç½®
# Persistent Context Configuration
# ----------------------------------------------------------------------------
# æ¡ä»¶æå–è§¦å‘é—´éš”ï¼ˆæ¯Nè½®å¯¹è¯è§¦å‘ä¸€æ¬¡LLMæå–ï¼Œæœ€å°1ï¼‰
# Context extraction trigger interval (trigger every N turns, minimum 1)
CONTEXT_TRIGGER_INTERVAL=5

# å¯¹è¯è·å–èŒƒå›´ï¼ˆåˆ†ææ—¶è·å–çš„å†å²è½®æ•°ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿä¸Šä¸‹æ–‡ï¼‰
# Max context turns for analysis (history turns to fetch for analysis)
CONTEXT_MAX_CONTEXT_TURNS=20

# æ¯ç±»å‹æœ€å¤§æ¡ä»¶æ•° / Max conditions per type
CONTEXT_MAX_PER_TYPE=10

# æ¡ä»¶æ€»æ•°ä¸Šé™ / Max total conditions
CONTEXT_MAX_TOTAL=100

# ç½®ä¿¡åº¦è¡°å‡å¼€å§‹å¤©æ•° / Days before decay starts
CONTEXT_DECAY_DAYS=14

# æ¯æ¬¡è¡°å‡æ¯”ä¾‹ (0.0-1.0) / Decay rate per check
CONTEXT_DECAY_RATE=0.05

# æœ€ä½ç½®ä¿¡åº¦ï¼ˆä½äºæ­¤å€¼è‡ªåŠ¨å½’æ¡£ï¼‰/ Min confidence before archive
CONTEXT_MIN_CONFIDENCE=0.1

# ----------------------------------------------------------------------------
# ä¸Šä¸‹æ–‡æ„å»ºé…ç½®ï¼ˆ100%ä¸é—å¿˜ä¿è¯ï¼‰
# Context Building Configuration (100% Memory Guarantee)
# ----------------------------------------------------------------------------
# æ„å»ºä¸Šä¸‹æ–‡æ—¶åŒ…å«çš„æœ€è¿‘å¯¹è¯æ•°ï¼ˆç¡®ä¿å¯¹è¯è¿è´¯æ€§ï¼‰
# Recent turns to include when building context
BUILD_CONTEXT_INCLUDE_RECENT=10

# æ˜¯å¦å¯ç”¨ä¸»åŠ¨æé†’ï¼ˆé‡è¦ä¿¡æ¯é•¿æœŸæœªæåŠæ—¶ä¸»åŠ¨æé†’AIï¼‰
# Enable proactive reminders for important info not mentioned for a while
PROACTIVE_REMINDER_ENABLED=true

# ä¸»åŠ¨æé†’é˜ˆå€¼ï¼ˆè¶…è¿‡å¤šå°‘è½®æœªæåŠåˆ™è§¦å‘æé†’ï¼‰
# Turns threshold to trigger proactive reminder
PROACTIVE_REMINDER_TURNS=50

# ----------------------------------------------------------------------------
# æ™ºèƒ½å»é‡é…ç½®ï¼ˆæŒä¹…æ¡ä»¶å’Œä¼ç¬”ç³»ç»Ÿï¼‰
# Smart Deduplication Configuration (Persistent Context & Foreshadowing)
# ----------------------------------------------------------------------------
# æ˜¯å¦å¯ç”¨ Embedding è¯­ä¹‰å»é‡ (true/false)
# å¯ç”¨åä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦åˆ¤æ–­é‡å¤ï¼Œæ›´æ™ºèƒ½ï¼›ç¦ç”¨åˆ™ä½¿ç”¨ç®€å•è¯é‡å 
# Enable Embedding-based semantic deduplication
DEDUP_EMBEDDING_ENABLED=true

# é«˜ç›¸ä¼¼åº¦é˜ˆå€¼ï¼šè¶…è¿‡æ­¤å€¼ç›´æ¥åˆå¹¶ï¼ˆ0.0-1.0ï¼Œæ¨è0.85ï¼‰
# High similarity threshold: auto-merge when exceeded (recommend 0.85)
DEDUP_HIGH_THRESHOLD=0.85

# ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼šä½äºæ­¤å€¼è§†ä¸ºä¸ç›¸ä¼¼ï¼ˆ0.0-1.0ï¼Œæ¨è0.70ï¼‰
# Low similarity threshold: considered different when below (recommend 0.70)
DEDUP_LOW_THRESHOLD=0.70

# ============================================================================
# v4.0 Phase 1/2 æ–°å¢é…ç½®
# v4.0 Phase 1/2 New Configurations
# ============================================================================

# ----------------------------------------------------------------------------
# æ—¶æ€çŸ¥è¯†å›¾è°±é…ç½®
# Temporal Knowledge Graph Configuration
# ----------------------------------------------------------------------------
# æ˜¯å¦å¯ç”¨æ—¶æ€çŸ¥è¯†å›¾è°±ï¼ˆè¿½è¸ªäº‹å®éšæ—¶é—´çš„å˜åŒ–ï¼‰
# Enable temporal knowledge graph (track facts over time)
TEMPORAL_GRAPH_ENABLED=true

# å›¾è°±å­˜å‚¨åç«¯: file(æœ¬åœ°JSON), neo4j, falkordb
# Graph storage backend: file(local JSON), neo4j, falkordb
TEMPORAL_GRAPH_BACKEND=file

# æ—¶æ€ä¿¡æ¯è¡°å‡ç‡ï¼ˆ0.0-1.0ï¼Œå€¼è¶Šå¤§è¡°å‡è¶Šå¿«ï¼‰
# Temporal decay rate (0.0-1.0, higher = faster decay)
TEMPORAL_DECAY_RATE=0.1

# ä¿ç•™çš„æœ€å¤§æ—¶æ€å†å²è®°å½•æ•°
# Max temporal history records to keep
TEMPORAL_MAX_HISTORY=1000

# ----------------------------------------------------------------------------
# çŸ›ç›¾æ£€æµ‹ä¸ç®¡ç†é…ç½®
# Contradiction Detection & Management Configuration
# ----------------------------------------------------------------------------
# æ˜¯å¦å¯ç”¨çŸ›ç›¾æ£€æµ‹
# Enable contradiction detection
CONTRADICTION_DETECTION_ENABLED=true

# æ˜¯å¦è‡ªåŠ¨è§£å†³çŸ›ç›¾ï¼ˆæ¨è falseï¼Œè®©ç”¨æˆ·ç¡®è®¤ï¼‰
# Auto-resolve contradictions (recommend false, let user confirm)
CONTRADICTION_AUTO_RESOLVE=false

# æ£€æµ‹ç­–ç•¥: RULE(è§„åˆ™), LLM(å¤§æ¨¡å‹åˆ¤æ–­), MIXED(æ··åˆ), AUTO(è‡ªåŠ¨é€‰æ‹©)
# Detection strategy: RULE/LLM/MIXED/AUTO (HYBRID is deprecated alias for MIXED)
CONTRADICTION_DETECTION_STRATEGY=MIXED

# ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆç”¨äºæ£€æµ‹æ½œåœ¨çŸ›ç›¾ï¼Œ0.0-1.0ï¼‰
# Similarity threshold for detecting potential contradictions
CONTRADICTION_SIMILARITY_THRESHOLD=0.8

# ----------------------------------------------------------------------------
# å…¨æ–‡æ£€ç´¢é…ç½® (BM25)
# Full-text Search Configuration (BM25)
# ----------------------------------------------------------------------------
# æ˜¯å¦å¯ç”¨ BM25 å…¨æ–‡æ£€ç´¢
# Enable BM25 full-text search
FULLTEXT_ENABLED=true

# BM25 k1 å‚æ•°ï¼ˆè¯é¢‘é¥±å’Œåº¦ï¼Œæ¨è 1.2-2.0ï¼‰
# BM25 k1 parameter (term frequency saturation)
FULLTEXT_K1=1.5

# BM25 b å‚æ•°ï¼ˆæ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–ï¼Œ0.0-1.0ï¼‰
# BM25 b parameter (document length normalization)
FULLTEXT_B=0.75

# å…¨æ–‡æ£€ç´¢åœ¨æ··åˆæœç´¢ä¸­çš„æƒé‡ï¼ˆ0.0-1.0ï¼‰
# Full-text search weight in hybrid search
FULLTEXT_WEIGHT=0.3

# ----------------------------------------------------------------------------
# æ™ºèƒ½æŠ½å–å™¨é…ç½® (SmartExtractor)
# Smart Extractor Configuration
# ----------------------------------------------------------------------------
# æŠ½å–æ¨¡å¼: RULES(è§„åˆ™), ADAPTIVE(è‡ªé€‚åº”), LLM(å…¨LLM)
# Extraction mode: RULES/ADAPTIVE/LLM (LOCAL/HYBRID/LLM_FULL are deprecated aliases)
SMART_EXTRACTOR_MODE=ADAPTIVE

# å¤æ‚åº¦é˜ˆå€¼ï¼ˆè¶…è¿‡æ­¤å€¼ä½¿ç”¨ LLM è¾…åŠ©æŠ½å–ï¼Œ0.0-1.0ï¼‰
# Complexity threshold (use LLM when exceeded)
SMART_EXTRACTOR_COMPLEXITY_THRESHOLD=0.6

# æ˜¯å¦å¯ç”¨æ—¶æ€æ£€æµ‹ï¼ˆè¯†åˆ«æ—¶é—´ç›¸å…³ä¿¡æ¯ï¼‰
# Enable temporal detection
SMART_EXTRACTOR_ENABLE_TEMPORAL=true

# ----------------------------------------------------------------------------
# é¢„ç®—ç®¡ç†é…ç½® (BudgetManager)
# Budget Management Configuration
# ----------------------------------------------------------------------------
# æ¯æ—¥é¢„ç®—ä¸Šé™ï¼ˆç¾å…ƒï¼Œ0=æ— é™åˆ¶ï¼‰
# Daily budget limit in USD (0 = unlimited)
BUDGET_DAILY_LIMIT=0

# æ¯å°æ—¶é¢„ç®—ä¸Šé™ï¼ˆç¾å…ƒï¼Œ0=æ— é™åˆ¶ï¼‰
# Hourly budget limit in USD (0 = unlimited)
BUDGET_HOURLY_LIMIT=0

# ä¿ç•™é¢„ç®—æ¯”ä¾‹ï¼ˆä¸ºé‡è¦æ“ä½œé¢„ç•™çš„é¢„ç®—æ¯”ä¾‹ï¼Œ0.0-1.0ï¼‰
# Reserve budget ratio for critical operations
BUDGET_RESERVE=0.1

# é¢„ç®—è­¦å‘Šé˜ˆå€¼ï¼ˆä½¿ç”¨é‡è¶…è¿‡æ­¤æ¯”ä¾‹æ—¶å‘å‡ºè­¦å‘Šï¼Œ0.0-1.0ï¼‰
# Budget alert threshold (warn when usage exceeds this ratio)
BUDGET_ALERT_THRESHOLD=0.8

# ----------------------------------------------------------------------------
# ä¸‰é˜¶æ®µå»é‡é…ç½® (ThreeStageDeduplicator)
# Three-Stage Deduplication Configuration
# ----------------------------------------------------------------------------
# Jaccard ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé˜¶æ®µ1 MinHash+LSHï¼Œ0.0-1.0ï¼‰
# Jaccard similarity threshold (Stage 1)
DEDUP_JACCARD_THRESHOLD=0.7

# è¯­ä¹‰ç›¸ä¼¼åº¦é«˜é˜ˆå€¼ï¼ˆé˜¶æ®µ2ï¼Œè¶…è¿‡æ­¤å€¼ç›´æ¥åˆå¹¶ï¼‰
# Semantic similarity high threshold (Stage 2, auto-merge when exceeded)
DEDUP_SEMANTIC_THRESHOLD=0.85

# è¯­ä¹‰ç›¸ä¼¼åº¦ä½é˜ˆå€¼ï¼ˆé˜¶æ®µ2ï¼Œä½äºæ­¤å€¼è§†ä¸ºä¸åŒï¼‰
# Semantic similarity low threshold (Stage 2, considered different when below)
DEDUP_SEMANTIC_LOW_THRESHOLD=0.70

# æ˜¯å¦å¯ç”¨ LLM ç¡®è®¤ï¼ˆé˜¶æ®µ3ï¼Œç”¨äºè¾¹ç•Œæƒ…å†µï¼‰
# Enable LLM confirmation (Stage 3, for borderline cases)
DEDUP_LLM_ENABLED=false

# ============================================================================
# v4.0 Phase 3 åä¸€å±‚æ£€ç´¢å™¨é…ç½®
# v4.0 Phase 3 Eleven-Layer Retriever Configuration
# ============================================================================

# ----------------------------------------------------------------------------
# ä¸»å¼€å…³
# Master Switch
# ----------------------------------------------------------------------------
# æ˜¯å¦å¯ç”¨åä¸€å±‚æ£€ç´¢å™¨ï¼ˆæ›¿ä»£é»˜è®¤çš„å…«å±‚æ£€ç´¢å™¨ï¼‰
# Enable eleven-layer retriever (replaces default eight-layer)
ELEVEN_LAYER_RETRIEVER_ENABLED=false

# ----------------------------------------------------------------------------
# å±‚å¼€å…³é…ç½®
# Layer Enable/Disable Configuration
# ----------------------------------------------------------------------------
# L1: Bloom Filter å¿«é€Ÿå¦å®šï¼ˆæä½æˆæœ¬æ’é™¤ä¸ç›¸å…³è®°å¿†ï¼‰
RETRIEVAL_L1_BLOOM_ENABLED=true

# L2: æ—¶æ€è¿‡æ»¤ï¼ˆæ ¹æ®æ—¶é—´èŒƒå›´ç­›é€‰ï¼Œéœ€è¦ TEMPORAL_GRAPH_ENABLED=trueï¼‰
RETRIEVAL_L2_TEMPORAL_ENABLED=true

# L3: å€’æ’ç´¢å¼•ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰
RETRIEVAL_L3_INVERTED_ENABLED=true

# L4: å®ä½“ç´¢å¼•ï¼ˆå‘½åå®ä½“åŒ¹é…ï¼‰
RETRIEVAL_L4_ENTITY_ENABLED=true

# L5: çŸ¥è¯†å›¾è°±éå†ï¼ˆå®ä½“å…³ç³»æ‰©å±•ï¼Œéœ€è¦ TEMPORAL_GRAPH_ENABLED=trueï¼‰
RETRIEVAL_L5_GRAPH_ENABLED=true

# L6: N-gram åŒ¹é…ï¼ˆæ¨¡ç³Šæ–‡æœ¬åŒ¹é…ï¼‰
RETRIEVAL_L6_NGRAM_ENABLED=true

# L7: å‘é‡ç²—æ’ï¼ˆANN è¿‘ä¼¼æœ€è¿‘é‚»ï¼‰
RETRIEVAL_L7_VECTOR_COARSE_ENABLED=true

# L8: å‘é‡ç²¾æ’ï¼ˆç²¾ç¡®ç›¸ä¼¼åº¦è®¡ç®—ï¼‰
RETRIEVAL_L8_VECTOR_FINE_ENABLED=true

# L9: é‡æ’åºï¼ˆç»¼åˆè¯„åˆ†ï¼‰
RETRIEVAL_L9_RERANK_ENABLED=true

# L10: CrossEncoder ç²¾æ’ï¼ˆæ·±åº¦è¯­ä¹‰åŒ¹é…ï¼Œéœ€è¦ sentence-transformersï¼‰
RETRIEVAL_L10_CROSS_ENCODER_ENABLED=false

# L11: LLM è¿‡æ»¤ï¼ˆå¤§æ¨¡å‹æœ€ç»ˆç¡®è®¤ï¼Œæ¶ˆè€— APIï¼‰
RETRIEVAL_L11_LLM_ENABLED=false

# ----------------------------------------------------------------------------
# Top-K é…ç½®ï¼ˆæ¯å±‚è¿”å›çš„å€™é€‰æ•°é‡ï¼‰
# Top-K Configuration (candidates returned per layer)
# ----------------------------------------------------------------------------
RETRIEVAL_L2_TEMPORAL_TOP_K=500
RETRIEVAL_L3_INVERTED_TOP_K=100
RETRIEVAL_L4_ENTITY_TOP_K=50
RETRIEVAL_L5_GRAPH_TOP_K=100
RETRIEVAL_L6_NGRAM_TOP_K=30
RETRIEVAL_L7_VECTOR_TOP_K=200
RETRIEVAL_L10_CROSS_ENCODER_TOP_K=50
RETRIEVAL_L11_LLM_TOP_K=20

# ----------------------------------------------------------------------------
# é˜ˆå€¼ä¸æœ€ç»ˆè¾“å‡ºé…ç½®
# Thresholds and Final Output Configuration
# ----------------------------------------------------------------------------
# ç²¾æ’é˜ˆå€¼ï¼ˆè¿›å…¥ç²¾æ’é˜¶æ®µçš„å€™é€‰æ•°ï¼‰
RETRIEVAL_FINE_RANK_THRESHOLD=100

# æœ€ç»ˆè¿”å›çš„è®°å¿†æ•°é‡
RETRIEVAL_FINAL_TOP_K=20

# ----------------------------------------------------------------------------
# L5 çŸ¥è¯†å›¾è°±éå†é…ç½®
# L5 Knowledge Graph Traversal Configuration
# ----------------------------------------------------------------------------
# å›¾éå†æœ€å¤§æ·±åº¦
RETRIEVAL_L5_GRAPH_MAX_DEPTH=2

# å›¾éå†èµ·å§‹å®ä½“æ•°é‡
RETRIEVAL_L5_GRAPH_MAX_ENTITIES=3

# éå†æ–¹å‘: both(åŒå‘), outgoing(å‡ºè¾¹), incoming(å…¥è¾¹)
RETRIEVAL_L5_GRAPH_DIRECTION=both

# ----------------------------------------------------------------------------
# L10 CrossEncoder é…ç½®
# L10 CrossEncoder Configuration
# ----------------------------------------------------------------------------
# CrossEncoder æ¨¡å‹åç§°ï¼ˆéœ€è¦å®‰è£… sentence-transformersï¼‰
RETRIEVAL_L10_CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# ----------------------------------------------------------------------------
# L11 LLM é…ç½®
# L11 LLM Configuration
# ----------------------------------------------------------------------------
# LLM åˆ¤æ–­è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
RETRIEVAL_L11_LLM_TIMEOUT=10.0

# ----------------------------------------------------------------------------
# æƒé‡é…ç½®ï¼ˆè°ƒæ•´å„æ£€ç´¢å±‚çš„ç›¸å¯¹æƒé‡ï¼‰
# Weight Configuration (adjust relative weight of each layer)
# ----------------------------------------------------------------------------
RETRIEVAL_WEIGHT_INVERTED=1.0
RETRIEVAL_WEIGHT_ENTITY=1.2
RETRIEVAL_WEIGHT_GRAPH=1.0
RETRIEVAL_WEIGHT_NGRAM=0.8
RETRIEVAL_WEIGHT_VECTOR=1.0
RETRIEVAL_WEIGHT_TEMPORAL=0.5
'''


def load_api_keys_from_file():
    """ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®åˆ°ç¯å¢ƒå˜é‡
    
    é…ç½®æ–‡ä»¶ä½ç½®: recall_data/config/api_keys.env
    ç”¨æˆ·å¯ä»¥ç›´æ¥ç¼–è¾‘è¿™ä¸ªæ–‡ä»¶ï¼Œç„¶åè°ƒç”¨çƒ­æ›´æ–°æ¥å£
    """
    config_file = get_config_file_path()
    
    if not config_file.exists():
        # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
        config_file.parent.mkdir(parents=True, exist_ok=True)
        config_file.write_text(get_default_config_content(), encoding='utf-8')
        print(f"[Config] å·²åˆ›å»ºé…ç½®æ–‡ä»¶: {config_file}")
        return
    
    # å…ˆæ¸…é™¤æ‰€æœ‰æ”¯æŒçš„é…ç½®é¡¹ï¼ˆçƒ­æ›´æ–°æ—¶ç¡®ä¿æ—§é…ç½®è¢«æ¸…é™¤ï¼‰
    for key in SUPPORTED_CONFIG_KEYS:
        if key in os.environ:
            del os.environ[key]
    
    # è¯»å–é…ç½®æ–‡ä»¶
    loaded_configs = []
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
                if not line or line.startswith('#'):
                    continue
                # è§£æ KEY=VALUE æ ¼å¼
                if '=' in line:
                    key, value = line.split('=', 1)
                    key = key.strip()
                    value = value.strip()
                    # åªè®¾ç½®æ”¯æŒçš„é…ç½®é¡¹ï¼Œä¸”å€¼éç©º
                    if value and key in SUPPORTED_CONFIG_KEYS:
                        os.environ[key] = value
                        # æ•æ„Ÿä¿¡æ¯è„±æ•æ˜¾ç¤º
                        if 'KEY' in key:
                            display_value = value[:8] + '...' if len(value) > 8 else '***'
                        else:
                            display_value = value
                        loaded_configs.append(f"{key}={display_value}")
        
        if loaded_configs:
            print(f"[Config] å·²åŠ è½½é…ç½®: {', '.join(loaded_configs)}")
        else:
            print(f"[Config] é…ç½®æ–‡ä»¶ä¸ºç©ºæˆ–æ— æœ‰æ•ˆé…ç½®")
    except Exception as e:
        print(f"[Config] è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")


def save_config_to_file(updates: Dict[str, str]):
    """å°†é…ç½®æ›´æ–°ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
    
    Args:
        updates: è¦æ›´æ–°çš„é…ç½®é¡¹ {KEY: VALUE}
    """
    config_file = get_config_file_path()
    
    # ç¡®ä¿é…ç½®æ–‡ä»¶å­˜åœ¨
    if not config_file.exists():
        config_file.parent.mkdir(parents=True, exist_ok=True)
        config_file.write_text(get_default_config_content(), encoding='utf-8')
    
    # è¯»å–ç°æœ‰é…ç½®
    lines = []
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except Exception as e:
        print(f"[Config] è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # æ›´æ–°é…ç½®
    updated_keys = set()
    new_lines = []
    
    for line in lines:
        stripped = line.strip()
        # ä¿ç•™æ³¨é‡Šå’Œç©ºè¡Œ
        if not stripped or stripped.startswith('#'):
            new_lines.append(line)
            continue
        
        # è§£æ KEY=VALUE
        if '=' in stripped:
            key = stripped.split('=', 1)[0].strip()
            if key in updates:
                # æ›´æ–°è¿™ä¸€è¡Œ
                new_lines.append(f"{key}={updates[key]}\n")
                updated_keys.add(key)
                # åŒæ—¶æ›´æ–°ç¯å¢ƒå˜é‡
                os.environ[key] = updates[key]
            else:
                new_lines.append(line)
        else:
            new_lines.append(line)
    
    # æ·»åŠ æ–°é…ç½®é¡¹ï¼ˆä¸å­˜åœ¨äºæ–‡ä»¶ä¸­çš„ï¼‰
    for key, value in updates.items():
        if key not in updated_keys and key in SUPPORTED_CONFIG_KEYS:
            new_lines.append(f"{key}={value}\n")
            os.environ[key] = value
    
    # å†™å…¥æ–‡ä»¶
    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            f.writelines(new_lines)
        print(f"[Config] å·²ä¿å­˜é…ç½®: {list(updates.keys())}")
    except Exception as e:
        print(f"[Config] ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")


# ==================== è¯·æ±‚/å“åº”æ¨¡å‹ ====================

class AddMemoryRequest(BaseModel):
    """æ·»åŠ è®°å¿†è¯·æ±‚"""
    content: str = Field(..., description="è®°å¿†å†…å®¹")
    user_id: str = Field(default="default", description="ç”¨æˆ·ID")
    metadata: Optional[Dict[str, Any]] = Field(default=None, description="å…ƒæ•°æ®")


class AddMemoryResponse(BaseModel):
    """æ·»åŠ è®°å¿†å“åº”"""
    id: str
    success: bool
    entities: List[str] = []
    message: str = ""
    consistency_warnings: List[str] = []  # ä¸€è‡´æ€§æ£€æŸ¥è­¦å‘Š


class TemporalFilterRequest(BaseModel):
    """æ—¶æ€è¿‡æ»¤è¯·æ±‚ - Phase 3"""
    start: Optional[str] = Field(default=None, description="æ—¶é—´èŒƒå›´èµ·ç‚¹ (ISO æ ¼å¼)")
    end: Optional[str] = Field(default=None, description="æ—¶é—´èŒƒå›´ç»ˆç‚¹ (ISO æ ¼å¼)")


class GraphExpandRequest(BaseModel):
    """å›¾éå†æ‰©å±•è¯·æ±‚ - Phase 3"""
    center_entities: List[str] = Field(default=[], description="ä¸­å¿ƒå®ä½“åˆ—è¡¨")
    max_depth: int = Field(default=2, ge=1, le=5, description="BFS æœ€å¤§æ·±åº¦")
    direction: str = Field(default="both", description="éå†æ–¹å‘: out|in|both")


class SearchRequest(BaseModel):
    """æœç´¢è¯·æ±‚"""
    query: str = Field(..., description="æœç´¢æŸ¥è¯¢")
    user_id: str = Field(default="default", description="ç”¨æˆ·ID")
    top_k: int = Field(default=10, ge=1, le=100, description="è¿”å›æ•°é‡")
    filters: Optional[Dict[str, Any]] = Field(default=None, description="è¿‡æ»¤æ¡ä»¶")
    # Phase 3 æ–°å¢å‚æ•°
    temporal_filter: Optional[TemporalFilterRequest] = Field(default=None, description="æ—¶æ€è¿‡æ»¤ï¼ˆPhase 3ï¼‰")
    graph_expand: Optional[GraphExpandRequest] = Field(default=None, description="å›¾éå†æ‰©å±•ï¼ˆPhase 3ï¼‰")
    config_preset: Optional[str] = Field(default=None, description="é…ç½®é¢„è®¾: default|fast|accurateï¼ˆPhase 3ï¼‰")


class SearchResultItem(BaseModel):
    """æœç´¢ç»“æœé¡¹"""
    id: str
    content: str
    score: float
    metadata: Dict[str, Any] = {}
    entities: List[str] = []


class RetrievalConfigRequest(BaseModel):
    """æ£€ç´¢é…ç½®è¯·æ±‚ - Phase 3"""
    # å±‚å¼€å…³
    l1_enabled: Optional[bool] = Field(default=None, description="L1 Bloom Filter")
    l2_enabled: Optional[bool] = Field(default=None, description="L2 Temporal Filter")
    l3_enabled: Optional[bool] = Field(default=None, description="L3 Inverted Index")
    l4_enabled: Optional[bool] = Field(default=None, description="L4 Entity Index")
    l5_enabled: Optional[bool] = Field(default=None, description="L5 Graph Traversal")
    l6_enabled: Optional[bool] = Field(default=None, description="L6 N-gram Index")
    l7_enabled: Optional[bool] = Field(default=None, description="L7 Vector Coarse")
    l8_enabled: Optional[bool] = Field(default=None, description="L8 Vector Fine")
    l9_enabled: Optional[bool] = Field(default=None, description="L9 Rerank")
    l10_enabled: Optional[bool] = Field(default=None, description="L10 CrossEncoder")
    l11_enabled: Optional[bool] = Field(default=None, description="L11 LLM Filter")
    # Top-K é…ç½®
    l7_vector_top_k: Optional[int] = Field(default=None, ge=10, le=1000, description="å‘é‡ç²—ç­›æ•°é‡")
    final_top_k: Optional[int] = Field(default=None, ge=1, le=100, description="æœ€ç»ˆè¿”å›æ•°é‡")
    # é¢„è®¾
    preset: Optional[str] = Field(default=None, description="åº”ç”¨é¢„è®¾: default|fast|accurate")


class RetrievalConfigResponse(BaseModel):
    """æ£€ç´¢é…ç½®å“åº” - Phase 3"""
    retriever_type: str = Field(description="æ£€ç´¢å™¨ç±»å‹: ElevenLayer|EightLayer")
    l1_enabled: bool
    l2_enabled: bool
    l3_enabled: bool
    l4_enabled: bool
    l5_enabled: bool
    l6_enabled: bool
    l7_enabled: bool
    l8_enabled: bool
    l9_enabled: bool
    l10_enabled: bool
    l11_enabled: bool
    l7_vector_top_k: int
    final_top_k: int
    weights: Dict[str, float] = {}


class ContextRequest(BaseModel):
    """æ„å»ºä¸Šä¸‹æ–‡è¯·æ±‚"""
    query: str = Field(..., description="å½“å‰æŸ¥è¯¢")
    user_id: str = Field(default="default", description="ç”¨æˆ·ID")
    character_id: str = Field(default="default", description="è§’è‰²ID")
    max_tokens: int = Field(default=2000, description="æœ€å¤§tokenæ•°")
    include_recent: int = Field(default=5, description="åŒ…å«çš„æœ€è¿‘å¯¹è¯æ•°")
    include_core_facts: bool = Field(default=True, description="æ˜¯å¦åŒ…å«æ ¸å¿ƒäº‹å®æ‘˜è¦")
    auto_extract_context: bool = Field(default=False, description="æ˜¯å¦è‡ªåŠ¨ä»æŸ¥è¯¢æå–æŒä¹…æ¡ä»¶ï¼ˆé»˜è®¤å…³é—­ï¼Œé¿å…é‡å¤æå–ï¼‰")


# ==================== L0 æ ¸å¿ƒè®¾å®šæ¨¡å‹ ====================

class CoreSettingsRequest(BaseModel):
    """L0æ ¸å¿ƒè®¾å®šè¯·æ±‚"""
    character_card: Optional[str] = Field(default=None, description="è§’è‰²å¡ï¼ˆâ‰¤2000å­—ï¼‰")
    world_setting: Optional[str] = Field(default=None, description="ä¸–ç•Œè§‚ï¼ˆâ‰¤1000å­—ï¼‰")
    writing_style: Optional[str] = Field(default=None, description="å†™ä½œé£æ ¼è¦æ±‚")
    code_standards: Optional[str] = Field(default=None, description="ä»£ç è§„èŒƒ")
    project_structure: Optional[str] = Field(default=None, description="é¡¹ç›®ç»“æ„è¯´æ˜")
    naming_conventions: Optional[str] = Field(default=None, description="å‘½åè§„èŒƒ")
    absolute_rules: Optional[List[str]] = Field(default=None, description="ç»å¯¹ä¸èƒ½è¿åçš„è§„åˆ™")


class CoreSettingsResponse(BaseModel):
    """L0æ ¸å¿ƒè®¾å®šå“åº”"""
    character_card: str = ""
    world_setting: str = ""
    writing_style: str = ""
    code_standards: str = ""
    project_structure: str = ""
    naming_conventions: str = ""
    user_preferences: Dict[str, Any] = {}
    absolute_rules: List[str] = []
    # è§„åˆ™æ£€æµ‹æ¨¡å¼: "llm" = LLMè¯­ä¹‰æ£€æµ‹, "fallback" = å…³é”®è¯å›é€€æ£€æµ‹
    rule_detection_mode: str = "fallback"


class ForeshadowingRequest(BaseModel):
    """ä¼ç¬”è¯·æ±‚"""
    content: str = Field(..., description="ä¼ç¬”å†…å®¹")
    user_id: str = Field(default="default", description="ç”¨æˆ·IDï¼ˆè§’è‰²åï¼‰")
    character_id: str = Field(default="default", description="è§’è‰²ID")
    related_entities: Optional[List[str]] = Field(default=None, description="ç›¸å…³å®ä½“")
    importance: float = Field(default=0.5, ge=0, le=1, description="é‡è¦æ€§")


# ==================== æŒä¹…æ¡ä»¶æ¨¡å‹ ====================

class PersistentContextRequest(BaseModel):
    """æ·»åŠ æŒä¹…æ¡ä»¶è¯·æ±‚"""
    content: str = Field(..., description="æ¡ä»¶å†…å®¹")
    context_type: str = Field(default="custom", description="æ¡ä»¶ç±»å‹ï¼šuser_identity, user_goal, user_preference, environment, project, character_trait, world_setting, relationship, assumption, constraint, custom")
    user_id: str = Field(default="default", description="ç”¨æˆ·ID")
    character_id: str = Field(default="default", description="è§’è‰²ID")
    keywords: Optional[List[str]] = Field(default=None, description="å…³é”®è¯")


class PersistentContextUpdateRequest(BaseModel):
    """æ›´æ–°æŒä¹…æ¡ä»¶è¯·æ±‚"""
    content: Optional[str] = Field(default=None, description="æ–°å†…å®¹")
    context_type: Optional[str] = Field(default=None, description="æ–°ç±»å‹")
    confidence: Optional[float] = Field(default=None, ge=0, le=1, description="æ–°ç½®ä¿¡åº¦")
    keywords: Optional[List[str]] = Field(default=None, description="æ–°å…³é”®è¯")


class PersistentContextItem(BaseModel):
    """æŒä¹…æ¡ä»¶é¡¹"""
    id: str
    content: str
    context_type: str
    confidence: float
    is_active: bool
    use_count: int
    created_at: float
    keywords: List[str] = []


class PersistentContextStats(BaseModel):
    """æŒä¹…æ¡ä»¶ç»Ÿè®¡"""
    total: int
    active: int
    by_type: Dict[str, int]
    avg_confidence: float
    oldest_days: float
    limits: Dict[str, int]


class ForeshadowingItem(BaseModel):
    """ä¼ç¬”é¡¹"""
    id: str
    content: str
    status: str
    importance: float
    hints: List[str] = []
    resolution: Optional[str] = None


class ForeshadowingAnalysisRequest(BaseModel):
    """ä¼ç¬”åˆ†æè¯·æ±‚"""
    content: str = Field(..., description="å¯¹è¯å†…å®¹")
    role: str = Field(default="user", description="è§’è‰²ï¼ˆuser/assistantï¼‰")
    user_id: str = Field(default="default", description="ç”¨æˆ·IDï¼ˆè§’è‰²åï¼‰")
    character_id: str = Field(default="default", description="è§’è‰²ID")


class ForeshadowingAnalysisResult(BaseModel):
    """ä¼ç¬”åˆ†æç»“æœ"""
    triggered: bool = Field(default=False, description="æ˜¯å¦è§¦å‘äº†åˆ†æ")
    new_foreshadowings: List[dict] = Field(default=[], description="æ–°æ£€æµ‹åˆ°çš„ä¼ç¬”")
    potentially_resolved: List[dict] = Field(default=[], description="å¯èƒ½å·²è§£å†³çš„ä¼ç¬”")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")


class ForeshadowingConfigUpdate(BaseModel):
    """ä¼ç¬”åˆ†æå™¨é…ç½®æ›´æ–°"""
    llm_enabled: Optional[bool] = Field(default=None, description="å¯ç”¨ LLM åˆ†æ")
    trigger_interval: Optional[int] = Field(default=None, ge=1, description="è§¦å‘é—´éš”ï¼ˆè½®æ¬¡ï¼‰")
    auto_plant: Optional[bool] = Field(default=None, description="è‡ªåŠ¨åŸ‹ä¸‹ä¼ç¬”")
    auto_resolve: Optional[bool] = Field(default=None, description="è‡ªåŠ¨è§£å†³ä¼ç¬”")


# ==================== å…¨å±€å¼•æ“ ====================

_engine: Optional[RecallEngine] = None


def _build_foreshadowing_config():
    """æ„å»ºä¼ç¬”åˆ†æå™¨é…ç½®ï¼ˆå†…éƒ¨è¾…åŠ©å‡½æ•°ï¼‰
    
    Returns:
        ForeshadowingAnalyzerConfig æˆ– None
    """
    llm_api_key = os.environ.get('LLM_API_KEY')
    llm_enabled_str = os.environ.get('FORESHADOWING_LLM_ENABLED', 'false').lower()
    llm_enabled = llm_enabled_str in ('true', '1', 'yes')
    
    if llm_api_key and llm_enabled:
        # LLM å·²é…ç½®ä¸”å·²å¯ç”¨
        from .processor.foreshadowing_analyzer import ForeshadowingAnalyzerConfig
        
        trigger_interval = int(os.environ.get('FORESHADOWING_TRIGGER_INTERVAL', '10'))
        auto_plant_str = os.environ.get('FORESHADOWING_AUTO_PLANT', 'true').lower()
        auto_resolve_str = os.environ.get('FORESHADOWING_AUTO_RESOLVE', 'false').lower()
        
        config = ForeshadowingAnalyzerConfig.llm_based(
            api_key=llm_api_key,
            model=os.environ.get('LLM_MODEL', 'gpt-4o-mini'),
            base_url=os.environ.get('LLM_API_BASE'),
            trigger_interval=trigger_interval,
            auto_plant=auto_plant_str in ('true', '1', 'yes'),
            auto_resolve=auto_resolve_str in ('true', '1', 'yes')
        )
        print(f"[Recall] ä¼ç¬”åˆ†æå™¨: LLM æ¨¡å¼å·²å¯ç”¨")
        return config
    else:
        if llm_api_key and not llm_enabled:
            print("[Recall] ä¼ç¬”åˆ†æå™¨: æ‰‹åŠ¨æ¨¡å¼ (LLM å·²é…ç½®ä½†æœªå¯ç”¨)")
        else:
            print("[Recall] ä¼ç¬”åˆ†æå™¨: æ‰‹åŠ¨æ¨¡å¼ (æœªé…ç½® LLM API)")
        return None


def _create_engine():
    """åˆ›å»ºå¼•æ“å®ä¾‹ï¼ˆå†…éƒ¨è¾…åŠ©å‡½æ•°ï¼‰
    
    Returns:
        RecallEngine å®ä¾‹
    """
    embedding_mode = os.environ.get('RECALL_EMBEDDING_MODE', '').lower()
    foreshadowing_config = _build_foreshadowing_config()
    
    lightweight = (embedding_mode == 'none')
    return RecallEngine(lightweight=lightweight, foreshadowing_config=foreshadowing_config)


def get_engine() -> RecallEngine:
    """è·å–å…¨å±€å¼•æ“å®ä¾‹
    
    æ ¹æ®ç¯å¢ƒå˜é‡ RECALL_EMBEDDING_MODE è‡ªåŠ¨é€‰æ‹©æ¨¡å¼ï¼š
    - none: Lite æ¨¡å¼ï¼ˆè½»é‡ï¼‰
    - local: Local æ¨¡å¼ï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰
    - openai: Cloud æ¨¡å¼-OpenAI
    - siliconflow: Cloud æ¨¡å¼-ç¡…åŸºæµåŠ¨
    """
    global _engine
    if _engine is None:
        # é¦–æ¬¡å¯åŠ¨æ—¶åŠ è½½é…ç½®æ–‡ä»¶
        load_api_keys_from_file()
        _engine = _create_engine()
    return _engine


def reload_engine():
    """é‡æ–°åŠ è½½å¼•æ“ï¼ˆçƒ­æ›´æ–°ï¼‰
    
    ç”¨äºåœ¨ä¿®æ”¹é…ç½®æ–‡ä»¶åé‡æ–°åˆå§‹åŒ–å¼•æ“
    """
    global _engine
    
    # å…³é—­æ—§å¼•æ“
    if _engine is not None:
        try:
            _engine.close()
        except:
            pass
        _engine = None
    
    # é‡æ–°åŠ è½½é…ç½®å¹¶åˆ›å»ºå¼•æ“
    load_api_keys_from_file()
    _engine = _create_engine()
    
    return _engine


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶
    print(f"[Recall API] æœåŠ¡å¯åŠ¨ v{__version__}")
    get_engine()  # é¢„åˆå§‹åŒ–
    
    yield
    
    # å…³é—­æ—¶
    if _engine:
        _engine.close()
    print("[Recall API] æœåŠ¡å…³é—­")


# ==================== FastAPI åº”ç”¨ ====================

app = FastAPI(
    title="Recall API",
    description="Recall - æ™ºèƒ½è®°å¿†ç®¡ç†ç³»ç»Ÿ API",
    version=__version__,
    lifespan=lifespan
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== å¥åº·æ£€æŸ¥ ====================

@app.get("/", tags=["Health"])
async def root():
    """æ ¹è·¯å¾„ - æœåŠ¡ä¿¡æ¯"""
    return {
        "service": "Recall API",
        "version": __version__,
        "status": "running"
    }


@app.get("/health", tags=["Health"])
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": time.time()
    }


# ==================== è®°å¿†ç®¡ç† API ====================

@app.post("/v1/memories", response_model=AddMemoryResponse, tags=["Memories"])
async def add_memory(request: AddMemoryRequest):
    """æ·»åŠ è®°å¿†
    
    å½“ä¿å­˜ç”¨æˆ·æ¶ˆæ¯æ—¶ï¼ˆmetadata.role='user'ï¼‰ï¼Œä¼šè‡ªåŠ¨ä»å†…å®¹ä¸­æå–æŒä¹…æ¡ä»¶ã€‚
    è¿™æ˜¯æ¡ä»¶è‡ªåŠ¨æå–çš„æ­£ç¡®æ—¶æœºï¼Œé¿å…åœ¨æ¯æ¬¡ç”Ÿæˆæ—¶é‡å¤æå–ã€‚
    """
    engine = get_engine()
    
    # æå– user_id å’Œ character_id
    user_id = request.user_id
    character_id = request.metadata.get('character_id', 'default') if request.metadata else 'default'
    role = request.metadata.get('role', 'unknown') if request.metadata else 'unknown'
    
    content_preview = request.content[:80].replace('\n', ' ') if len(request.content) > 80 else request.content.replace('\n', ' ')
    print(f"[Recall][Memory] ğŸ“¥ æ·»åŠ è¯·æ±‚: user={user_id}, char={character_id}, role={role}")
    print(f"[Recall][Memory]    å†…å®¹({len(request.content)}å­—): {content_preview}{'...' if len(request.content) > 80 else ''}")
    
    result = engine.add(
        content=request.content,
        user_id=request.user_id,
        metadata=request.metadata
    )
    
    # è®°å½•ç»“æœï¼ˆåŒ…æ‹¬å»é‡è·³è¿‡çš„æƒ…å†µï¼‰
    if result.success:
        print(f"[Recall][Memory] âœ… ä¿å­˜æˆåŠŸ: id={result.id}, entities={result.entities}")
    else:
        print(f"[Recall][Memory] â­ï¸ è·³è¿‡: {result.message}")
    
    # ã€æ³¨æ„ã€‘æ¡ä»¶æå–å·²ç§»è‡³ /v1/foreshadowing/analyze/turn ç«¯ç‚¹
    # ä¸ä¼ç¬”åˆ†æä½¿ç”¨ç›¸åŒçš„è§¦å‘é—´éš”æœºåˆ¶ï¼ˆé»˜è®¤æ¯5è½®ï¼‰ï¼Œé¿å…é‡å¤åˆ†æç›¸åŒå¯¹è¯å†å²
    # è¿™æ ·ç¡®ä¿æ¡ä»¶æå–èƒ½è·å–å®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯åªçœ‹å•æ¡æ¶ˆæ¯
    
    return AddMemoryResponse(
        id=result.id,
        success=result.success,
        entities=result.entities,
        message=result.message,
        consistency_warnings=result.consistency_warnings
    )


@app.post("/v1/memories/search", response_model=List[SearchResultItem], tags=["Memories"])
async def search_memories(request: SearchRequest):
    """æœç´¢è®°å¿†
    
    Phase 3 æ–°å¢å‚æ•°ï¼š
    - temporal_filter: æ—¶æ€è¿‡æ»¤ï¼ˆæ—¶é—´èŒƒå›´ï¼‰
    - graph_expand: å›¾éå†æ‰©å±•ï¼ˆå…³è”å®ä½“å‘ç°ï¼‰
    - config_preset: é…ç½®é¢„è®¾ï¼ˆdefault/fast/accurateï¼‰
    """
    query_preview = request.query[:50].replace('\n', ' ') if len(request.query) > 50 else request.query.replace('\n', ' ')
    print(f"[Recall][Memory] ğŸ” æœç´¢è¯·æ±‚: user={request.user_id}, top_k={request.top_k}")
    print(f"[Recall][Memory]    æŸ¥è¯¢: {query_preview}{'...' if len(request.query) > 50 else ''}")
    
    # Phase 3: å¤„ç†æ–°å‚æ•°
    temporal_context = None
    if request.temporal_filter:
        from datetime import datetime
        try:
            start = datetime.fromisoformat(request.temporal_filter.start) if request.temporal_filter.start else None
            end = datetime.fromisoformat(request.temporal_filter.end) if request.temporal_filter.end else None
            from recall.retrieval.config import TemporalContext
            temporal_context = TemporalContext(start=start, end=end)
            print(f"[Recall][Memory]    æ—¶æ€è¿‡æ»¤: {start} ~ {end}")
        except Exception as e:
            print(f"[Recall][Memory]    æ—¶æ€è¿‡æ»¤è§£æå¤±è´¥: {e}")
    
    # Phase 3: å¤„ç†å›¾éå†æ‰©å±•å‚æ•°ï¼ˆæ·»åŠ åˆ° filtersï¼‰
    filters = request.filters or {}
    if request.graph_expand and request.graph_expand.center_entities:
        filters['graph_expand'] = {
            'center_entities': request.graph_expand.center_entities,
            'max_depth': request.graph_expand.max_depth,
            'direction': request.graph_expand.direction
        }
        print(f"[Recall][Memory]    å›¾éå†: å®ä½“={request.graph_expand.center_entities}, æ·±åº¦={request.graph_expand.max_depth}")
    
    # Phase 3: å¤„ç†é…ç½®é¢„è®¾
    config_preset = None
    if request.config_preset:
        config_preset = request.config_preset
        filters['config_preset'] = request.config_preset
        print(f"[Recall][Memory]    é…ç½®é¢„è®¾: {request.config_preset}")
    
    engine = get_engine()
    results = engine.search(
        query=request.query,
        user_id=request.user_id,
        top_k=request.top_k,
        filters=filters,
        temporal_context=temporal_context,
        config_preset=config_preset
    )
    
    print(f"[Recall][Memory] ğŸ“Š æœç´¢ç»“æœ: æ‰¾åˆ° {len(results)} æ¡è®°å¿†")
    for i, r in enumerate(results[:3]):  # åªæ‰“å°å‰3æ¡
        content_preview = r.content[:40].replace('\n', ' ')
        print(f"[Recall][Memory]    [{i+1}] score={r.score:.3f}: {content_preview}...")
    
    return [
        SearchResultItem(
            id=r.id,
            content=r.content,
            score=r.score,
            metadata=r.metadata,
            entities=r.entities
        )
        for r in results
    ]


@app.get("/v1/memories", tags=["Memories"])
async def list_memories(
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    limit: int = Query(default=100, ge=1, le=1000, description="é™åˆ¶æ•°é‡"),
    offset: int = Query(default=0, ge=0, description="åç§»é‡ï¼Œç”¨äºåˆ†é¡µ")
):
    """è·å–æ‰€æœ‰è®°å¿†
    
    æ”¯æŒåˆ†é¡µï¼š
    - limit: æ¯é¡µæ•°é‡
    - offset: è·³è¿‡çš„è®°å½•æ•°
    
    ç¤ºä¾‹ï¼š
    - ç¬¬ä¸€é¡µ: ?limit=20&offset=0
    - ç¬¬äºŒé¡µ: ?limit=20&offset=20
    """
    engine = get_engine()
    
    # ä½¿ç”¨é«˜æ•ˆçš„åˆ†é¡µæ–¹æ³•ï¼Œé¿å…åŠ è½½å…¨éƒ¨æ•°æ®
    memories, total_count = engine.get_paginated(
        user_id=user_id,
        offset=offset,
        limit=limit
    )
    
    print(f"[Recall][Memory] ğŸ“‹ è·å–åˆ—è¡¨: user={user_id}, offset={offset}, limit={limit}")
    print(f"[Recall][Memory]    è¿”å› {len(memories)}/{total_count} æ¡è®°å¿†")
    
    return {
        "memories": memories, 
        "count": len(memories),
        "total": total_count,
        "offset": offset,
        "limit": limit
    }


@app.get("/v1/memories/{memory_id}", tags=["Memories"])
async def get_memory(memory_id: str, user_id: str = Query(default="default")):
    """è·å–å•æ¡è®°å¿†"""
    engine = get_engine()
    memory = engine.get(memory_id, user_id=user_id)
    
    if memory is None:
        raise HTTPException(status_code=404, detail="è®°å¿†ä¸å­˜åœ¨")
    
    return memory


@app.delete("/v1/memories/{memory_id}", tags=["Memories"])
async def delete_memory(memory_id: str, user_id: str = Query(default="default")):
    """åˆ é™¤è®°å¿†"""
    engine = get_engine()
    success = engine.delete(memory_id, user_id=user_id)
    
    if not success:
        raise HTTPException(status_code=404, detail="è®°å¿†ä¸å­˜åœ¨æˆ–åˆ é™¤å¤±è´¥")
    
    return {"success": True, "message": "åˆ é™¤æˆåŠŸ"}


@app.delete("/v1/memories", tags=["Memories"])
async def clear_memories(
    user_id: str = Query(..., description="ç”¨æˆ·IDï¼ˆè§’è‰²åï¼‰ï¼Œå¿…å¡«"),
    confirm: bool = Query(default=False, description="ç¡®è®¤åˆ é™¤ï¼Œå¿…é¡»ä¸º true")
):
    """æ¸…ç©ºæŒ‡å®šè§’è‰²çš„æ‰€æœ‰è®°å¿†
    
    âš ï¸ å±é™©æ“ä½œï¼è¿™å°†åˆ é™¤è¯¥è§’è‰²çš„å…¨éƒ¨è®°å¿†æ•°æ®ï¼Œæ— æ³•æ¢å¤ã€‚
    
    ä½¿ç”¨åœºæ™¯ï¼š
    - åˆ é™¤è§’è‰²å¡åæ¸…ç†å¯¹åº”çš„è®°å¿†æ•°æ®
    - é‡ç½®æŸä¸ªè§’è‰²çš„æ‰€æœ‰è®°å¿†
    
    ç¤ºä¾‹:
        DELETE /v1/memories?user_id=è§’è‰²å&confirm=true
    """
    if not confirm:
        raise HTTPException(
            status_code=400, 
            detail="è¯·æ·»åŠ  confirm=true å‚æ•°ç¡®è®¤åˆ é™¤æ“ä½œ"
        )
    
    if user_id == "default":
        raise HTTPException(
            status_code=400,
            detail="ä¸èƒ½åˆ é™¤ default ç”¨æˆ·çš„è®°å¿†ï¼Œè¯·æŒ‡å®šå…·ä½“çš„è§’è‰²å"
        )
    
    engine = get_engine()
    
    # ä½¿ç”¨é«˜æ•ˆçš„è®¡æ•°æ–¹æ³•è·å–æ•°é‡
    count = engine.count_memories(user_id=user_id)
    print(f"[Recall][Memory] ğŸ—‘ï¸ æ¸…ç©ºè¯·æ±‚: user={user_id}, åç«¯è®¡æ•°={count}")
    
    if count == 0:
        return {"success": True, "message": "è¯¥è§’è‰²æ²¡æœ‰è®°å¿†æ•°æ®", "deleted_count": 0}
    
    # æ¸…ç©º
    success = engine.clear(user_id=user_id)
    print(f"[Recall][Memory] {'âœ…' if success else 'âŒ'} æ¸…ç©ºå®Œæˆ: user={user_id}, success={success}")
    
    if success:
        return {
            "success": True, 
            "message": f"å·²æ¸…ç©ºè§’è‰² '{user_id}' çš„æ‰€æœ‰è®°å¿†",
            "deleted_count": count
        }
    else:
        raise HTTPException(status_code=500, detail="æ¸…ç©ºå¤±è´¥")


@app.put("/v1/memories/{memory_id}", tags=["Memories"])
async def update_memory(
    memory_id: str,
    content: str = Body(...),
    user_id: str = Query(default="default"),
    metadata: Optional[Dict[str, Any]] = Body(default=None)
):
    """æ›´æ–°è®°å¿†"""
    engine = get_engine()
    success = engine.update(memory_id, content, user_id=user_id, metadata=metadata)
    
    if not success:
        raise HTTPException(status_code=404, detail="è®°å¿†ä¸å­˜åœ¨æˆ–æ›´æ–°å¤±è´¥")
    
    return {"success": True, "message": "æ›´æ–°æˆåŠŸ"}


# ==================== L0 æ ¸å¿ƒè®¾å®š API ====================

@app.get("/v1/core-settings", response_model=CoreSettingsResponse, tags=["Core Settings"])
async def get_core_settings():
    """è·å–L0æ ¸å¿ƒè®¾å®š
    
    è¿”å›å€¼ä¸­åŒ…å« rule_detection_mode å­—æ®µ:
    - "llm": LLM è¯­ä¹‰æ£€æµ‹å·²å¯ç”¨ï¼ˆéœ€é…ç½® LLM_API_KEYï¼‰
    - "fallback": ä½¿ç”¨å…³é”®è¯å›é€€æ£€æµ‹ï¼ˆæœªé…ç½® LLM æˆ– LLM ä¸å¯ç”¨ï¼‰
    """
    engine = get_engine()
    settings = engine.core_settings
    
    # åˆ¤æ–­è§„åˆ™æ£€æµ‹æ¨¡å¼
    rule_mode = "llm" if (engine.consistency_checker._llm_client is not None) else "fallback"
    
    return CoreSettingsResponse(
        character_card=settings.character_card,
        world_setting=settings.world_setting,
        writing_style=settings.writing_style,
        code_standards=settings.code_standards,
        project_structure=settings.project_structure,
        naming_conventions=settings.naming_conventions,
        user_preferences=settings.user_preferences,
        absolute_rules=settings.absolute_rules,
        rule_detection_mode=rule_mode
    )


@app.put("/v1/core-settings", response_model=CoreSettingsResponse, tags=["Core Settings"])
async def update_core_settings(request: CoreSettingsRequest):
    """æ›´æ–°L0æ ¸å¿ƒè®¾å®šï¼ˆéƒ¨åˆ†æ›´æ–°ï¼‰"""
    engine = get_engine()
    settings = engine.core_settings
    
    # åªæ›´æ–°æä¾›çš„å­—æ®µ
    if request.character_card is not None:
        settings.character_card = request.character_card
    if request.world_setting is not None:
        settings.world_setting = request.world_setting
    if request.writing_style is not None:
        settings.writing_style = request.writing_style
    if request.code_standards is not None:
        settings.code_standards = request.code_standards
    if request.project_structure is not None:
        settings.project_structure = request.project_structure
    if request.naming_conventions is not None:
        settings.naming_conventions = request.naming_conventions
    if request.absolute_rules is not None:
        settings.absolute_rules = request.absolute_rules
        # åŒæ­¥æ›´æ–° ConsistencyChecker çš„è§„åˆ™
        engine.consistency_checker.update_rules(request.absolute_rules)
    
    # ä¿å­˜æ›´æ–°
    settings.save()
    
    # è·å–æ£€æµ‹æ¨¡å¼
    rule_mode = "llm" if engine.consistency_checker._llm_client else "fallback"
    
    return CoreSettingsResponse(
        character_card=settings.character_card,
        world_setting=settings.world_setting,
        writing_style=settings.writing_style,
        code_standards=settings.code_standards,
        project_structure=settings.project_structure,
        naming_conventions=settings.naming_conventions,
        user_preferences=settings.user_preferences,
        absolute_rules=settings.absolute_rules,
        rule_detection_mode=rule_mode
    )


# ==================== ä¸Šä¸‹æ–‡æ„å»º API ====================

@app.post("/v1/context", tags=["Context"])
async def build_context(request: ContextRequest):
    """æ„å»ºä¸Šä¸‹æ–‡
    
    æ³¨æ„ï¼šauto_extract_context é»˜è®¤ä¸º Falseï¼Œæ¡ä»¶æå–å·²æ”¹ä¸ºåœ¨ä¿å­˜ç”¨æˆ·æ¶ˆæ¯æ—¶è¿›è¡Œã€‚
    å¦‚æœéœ€è¦å¼ºåˆ¶æå–æ¡ä»¶ï¼Œè¯·æ˜¾å¼ä¼ å…¥ auto_extract_context=Trueã€‚
    """
    query_preview = request.query[:60].replace('\n', ' ') if len(request.query) > 60 else request.query.replace('\n', ' ')
    print(f"[Recall][Context] ğŸ“¦ æ„å»ºä¸Šä¸‹æ–‡: user={request.user_id}, auto_extract={request.auto_extract_context}")
    print(f"[Recall][Context]    æŸ¥è¯¢: {query_preview}{'...' if len(request.query) > 60 else ''}")
    
    engine = get_engine()
    context = engine.build_context(
        query=request.query,
        user_id=request.user_id,
        character_id=request.character_id,
        max_tokens=request.max_tokens,
        include_recent=request.include_recent,
        include_core_facts=request.include_core_facts,
        auto_extract_context=request.auto_extract_context
    )
    
    print(f"[Recall][Context] âœ… ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ: æ€»é•¿åº¦={len(context)}å­—ç¬¦")
    return {"context": context}


# ==================== æŒä¹…æ¡ä»¶ API ====================

@app.post("/v1/persistent-contexts", response_model=PersistentContextItem, tags=["Persistent Contexts"])
async def add_persistent_context(request: PersistentContextRequest):
    """æ·»åŠ æŒä¹…æ¡ä»¶
    
    æŒä¹…æ¡ä»¶æ˜¯å·²ç¡®ç«‹çš„èƒŒæ™¯è®¾å®šï¼Œä¼šåœ¨æ‰€æœ‰åç»­å¯¹è¯ä¸­è‡ªåŠ¨åŒ…å«ã€‚
    ä¾‹å¦‚ï¼šç”¨æˆ·èº«ä»½ã€æŠ€æœ¯ç¯å¢ƒã€è§’è‰²è®¾å®šç­‰ã€‚
    """
    from .processor.context_tracker import ContextType
    
    engine = get_engine()
    
    # è½¬æ¢ç±»å‹
    try:
        ctx_type = ContextType(request.context_type)
    except ValueError:
        ctx_type = ContextType.CUSTOM
    
    ctx = engine.add_persistent_context(
        content=request.content,
        context_type=ctx_type,
        user_id=request.user_id,
        character_id=request.character_id,
        keywords=request.keywords
    )
    
    # add_persistent_context è¿”å› PersistentContext å¯¹è±¡
    return PersistentContextItem(
        id=ctx.id,
        content=ctx.content,
        context_type=ctx.context_type.value,
        confidence=ctx.confidence,
        is_active=ctx.is_active,
        use_count=ctx.use_count,
        created_at=ctx.created_at,
        keywords=ctx.keywords
    )


@app.get("/v1/persistent-contexts", response_model=List[PersistentContextItem], tags=["Persistent Contexts"])
async def list_persistent_contexts(
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID"),
    context_type: Optional[str] = Query(default=None, description="æŒ‰ç±»å‹è¿‡æ»¤")
):
    """è·å–æ‰€æœ‰æ´»è·ƒçš„æŒä¹…æ¡ä»¶"""
    engine = get_engine()
    contexts = engine.get_persistent_contexts(user_id, character_id)
    
    # æŒ‰ç±»å‹è¿‡æ»¤
    if context_type:
        contexts = [c for c in contexts if c['context_type'] == context_type]
    
    print(f"[Recall][Context] ğŸ“‹ è·å–æ¡ä»¶åˆ—è¡¨: user={user_id}, char={character_id}")
    print(f"[Recall][Context]    æ´»è·ƒæ¡ä»¶: {len(contexts)} æ¡")
    if contexts:
        types_summary = {}
        for c in contexts:
            t = c.get('context_type', 'unknown')
            types_summary[t] = types_summary.get(t, 0) + 1
        print(f"[Recall][Context]    ç±»å‹åˆ†å¸ƒ: {types_summary}")
    
    return [
        PersistentContextItem(
            id=c['id'],
            content=c['content'],
            context_type=c['context_type'],
            confidence=c['confidence'],
            is_active=c['is_active'],
            use_count=c['use_count'],
            created_at=c['created_at'],
            keywords=c.get('keywords', [])
        )
        for c in contexts
    ]


# æ³¨æ„ï¼šå›ºå®šè·¯å¾„å¿…é¡»åœ¨å‚æ•°è·¯å¾„ä¹‹å‰å®šä¹‰ï¼Œå¦åˆ™ /stats ä¼šè¢«å½“ä½œ {context_id}

@app.get("/v1/persistent-contexts/stats", response_model=PersistentContextStats, tags=["Persistent Contexts"])
async def get_persistent_context_stats(
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """è·å–æŒä¹…æ¡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
    engine = get_engine()
    stats = engine.context_tracker.get_stats(user_id, character_id)
    return PersistentContextStats(**stats)


@app.post("/v1/persistent-contexts/consolidate", tags=["Persistent Contexts"])
async def consolidate_contexts(
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID"),
    force: bool = Query(default=False, description="æ˜¯å¦å¼ºåˆ¶æ‰§è¡Œï¼ˆä¸ç®¡æ•°é‡æ˜¯å¦è¶…è¿‡é˜ˆå€¼ï¼‰")
):
    """å‹ç¼©åˆå¹¶æŒä¹…æ¡ä»¶
    
    å½“æŒä¹…æ¡ä»¶æ•°é‡è¿‡å¤šæ—¶ï¼Œæ™ºèƒ½åˆå¹¶ç›¸ä¼¼çš„æ¡ä»¶ã€‚
    å¦‚æœé…ç½®äº†LLMï¼Œä¼šä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å‹ç¼©ã€‚
    """
    engine = get_engine()
    result = engine.consolidate_persistent_contexts(user_id, character_id, force)
    return result


@app.post("/v1/persistent-contexts/extract", tags=["Persistent Contexts"])
async def extract_contexts_from_text(
    text: str = Body(..., embed=True, description="è¦åˆ†æçš„æ–‡æœ¬"),
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """ä»æ–‡æœ¬ä¸­è‡ªåŠ¨æå–æŒä¹…æ¡ä»¶
    
    ä½¿ç”¨ LLMï¼ˆå¦‚æœå¯ç”¨ï¼‰æˆ–è§„åˆ™ä»æ–‡æœ¬ä¸­æå–åº”è¯¥æŒä¹…åŒ–çš„æ¡ä»¶ã€‚
    """
    engine = get_engine()
    contexts = engine.extract_contexts_from_text(text, user_id, character_id)
    return {
        "extracted": len(contexts),
        "contexts": contexts
    }


@app.delete("/v1/persistent-contexts/{context_id}", tags=["Persistent Contexts"])
async def remove_persistent_context(
    context_id: str,
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """åœç”¨æŒä¹…æ¡ä»¶"""
    engine = get_engine()
    success = engine.remove_persistent_context(context_id, user_id, character_id)
    
    if not success:
        raise HTTPException(status_code=404, detail="æŒä¹…æ¡ä»¶ä¸å­˜åœ¨")
    
    return {"success": True, "message": "æŒä¹…æ¡ä»¶å·²åœç”¨"}


@app.delete("/v1/persistent-contexts", tags=["Persistent Contexts"])
async def clear_all_persistent_contexts(
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """æ¸…ç©ºå½“å‰è§’è‰²çš„æ‰€æœ‰æŒä¹…æ¡ä»¶"""
    engine = get_engine()
    
    # è·å–æ‰€æœ‰æ´»è·ƒæ¡ä»¶
    contexts = engine.get_persistent_contexts(user_id, character_id)
    count = len(contexts)
    
    # é€ä¸ªåˆ é™¤
    for ctx in contexts:
        engine.remove_persistent_context(ctx['id'], user_id, character_id)
    
    print(f"[Recall][Context] ğŸ—‘ï¸ æ¸…ç©ºæ¡ä»¶: user={user_id}, char={character_id}, åˆ é™¤={count}æ¡")
    return {"success": True, "message": f"å·²æ¸…ç©º {count} ä¸ªæŒä¹…æ¡ä»¶", "count": count}


@app.post("/v1/persistent-contexts/{context_id}/used", tags=["Persistent Contexts"])
async def mark_context_used(
    context_id: str,
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """æ ‡è®°æŒä¹…æ¡ä»¶å·²ä½¿ç”¨
    
    è°ƒç”¨æ­¤æ¥å£å¯ä»¥æ›´æ–°æ¡ä»¶çš„ä½¿ç”¨æ—¶é—´å’Œä½¿ç”¨æ¬¡æ•°ï¼Œ
    è¿™å¯¹äºç½®ä¿¡åº¦è¡°å‡æœºåˆ¶å¾ˆé‡è¦ã€‚
    """
    engine = get_engine()
    engine.context_tracker.mark_used(context_id, user_id, character_id)
    return {"success": True, "message": "å·²æ ‡è®°ä½¿ç”¨"}


@app.get("/v1/persistent-contexts/archived", tags=["Persistent Contexts"])
async def list_archived_contexts(
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID"),
    page: int = Query(default=1, ge=1, description="é¡µç "),
    page_size: int = Query(default=20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    search: Optional[str] = Query(default=None, description="æœç´¢å…³é”®è¯"),
    context_type: Optional[str] = Query(default=None, description="ç±»å‹ç­›é€‰")
):
    """è·å–å½’æ¡£çš„æŒä¹…æ¡ä»¶åˆ—è¡¨ï¼ˆåˆ†é¡µã€æœç´¢ã€ç­›é€‰ï¼‰"""
    engine = get_engine()
    result = engine.context_tracker.get_archived_contexts(
        user_id=user_id,
        character_id=character_id,
        page=page,
        page_size=page_size,
        search=search,
        context_type=context_type
    )
    return result


@app.post("/v1/persistent-contexts/{context_id}/restore", tags=["Persistent Contexts"])
async def restore_context_from_archive(
    context_id: str,
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """ä»å½’æ¡£æ¢å¤æŒä¹…æ¡ä»¶åˆ°æ´»è·ƒåˆ—è¡¨"""
    engine = get_engine()
    ctx = engine.context_tracker.restore_from_archive(context_id, user_id, character_id)
    
    if not ctx:
        raise HTTPException(status_code=404, detail="å½’æ¡£æ¡ä»¶ä¸å­˜åœ¨")
    
    return {
        "success": True,
        "message": "å·²æ¢å¤æ¡ä»¶",
        "context": {
            "id": ctx.id,
            "content": ctx.content,
            "context_type": ctx.context_type.value,
            "confidence": ctx.confidence
        }
    }


@app.delete("/v1/persistent-contexts/archived/{context_id}", tags=["Persistent Contexts"])
async def delete_archived_context(
    context_id: str,
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """å½»åº•åˆ é™¤å½’æ¡£ä¸­çš„æŒä¹…æ¡ä»¶"""
    engine = get_engine()
    success = engine.context_tracker.delete_archived(context_id, user_id, character_id)
    
    if not success:
        raise HTTPException(status_code=404, detail="å½’æ¡£æ¡ä»¶ä¸å­˜åœ¨")
    
    return {"success": True, "message": "å·²å½»åº•åˆ é™¤å½’æ¡£æ¡ä»¶"}


@app.delete("/v1/persistent-contexts/archived", tags=["Persistent Contexts"])
async def clear_all_archived_contexts(
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """æ¸…ç©ºæ‰€æœ‰å½’æ¡£çš„æŒä¹…æ¡ä»¶"""
    engine = get_engine()
    count = engine.context_tracker.clear_archived(user_id, character_id)
    return {"success": True, "message": f"å·²æ¸…ç©º {count} ä¸ªå½’æ¡£æ¡ä»¶", "count": count}


@app.post("/v1/persistent-contexts/{context_id}/archive", tags=["Persistent Contexts"])
async def archive_context(
    context_id: str,
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """æ‰‹åŠ¨å°†æ´»è·ƒæ¡ä»¶å½’æ¡£"""
    engine = get_engine()
    success = engine.context_tracker.archive_context(context_id, user_id, character_id)
    
    if not success:
        raise HTTPException(status_code=404, detail="æ¡ä»¶ä¸å­˜åœ¨")
    
    return {"success": True, "message": "å·²å½’æ¡£æ¡ä»¶"}


@app.put("/v1/persistent-contexts/{context_id}", tags=["Persistent Contexts"])
async def update_persistent_context(
    context_id: str,
    request: PersistentContextUpdateRequest,
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """ç¼–è¾‘æŒä¹…æ¡ä»¶çš„å­—æ®µ"""
    engine = get_engine()
    ctx = engine.context_tracker.update_context(
        context_id=context_id,
        user_id=user_id,
        character_id=character_id,
        content=request.content,
        context_type=request.context_type,
        confidence=request.confidence,
        keywords=request.keywords
    )
    
    if not ctx:
        raise HTTPException(status_code=404, detail="æ¡ä»¶ä¸å­˜åœ¨")
    
    return {
        "success": True,
        "message": "å·²æ›´æ–°æ¡ä»¶",
        "context": {
            "id": ctx.id,
            "content": ctx.content,
            "context_type": ctx.context_type.value,
            "confidence": ctx.confidence,
            "keywords": ctx.keywords
        }
    }


# ==================== ä¼ç¬” API ====================

@app.post("/v1/foreshadowing", response_model=ForeshadowingItem, tags=["Foreshadowing"])
async def plant_foreshadowing(request: ForeshadowingRequest):
    """åŸ‹ä¸‹ä¼ç¬”"""
    engine = get_engine()
    fsh = engine.plant_foreshadowing(
        content=request.content,
        user_id=request.user_id,
        character_id=request.character_id,
        related_entities=request.related_entities,
        importance=request.importance
    )
    return ForeshadowingItem(
        id=fsh.id,
        content=fsh.content,
        status=fsh.status.value,
        importance=fsh.importance,
        hints=fsh.hints,
        resolution=fsh.resolution
    )


@app.get("/v1/foreshadowing", response_model=List[ForeshadowingItem], tags=["Foreshadowing"])
async def list_foreshadowing(
    user_id: str = Query(default="default", description="ç”¨æˆ·IDï¼ˆè§’è‰²åï¼‰"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """è·å–æ´»è·ƒä¼ç¬”"""
    engine = get_engine()
    active = engine.get_active_foreshadowings(user_id, character_id)
    print(f"[Recall][Foreshadow] ğŸ“‹ è·å–ä¼ç¬”åˆ—è¡¨: user={user_id}, char={character_id}")
    print(f"[Recall][Foreshadow]    æ´»è·ƒä¼ç¬”: {len(active)} æ¡")
    if active:
        status_summary = {}
        for f in active:
            s = f.status.value
            status_summary[s] = status_summary.get(s, 0) + 1
        print(f"[Recall][Foreshadow]    çŠ¶æ€åˆ†å¸ƒ: {status_summary}")
        for i, f in enumerate(active[:3]):
            preview = f.content[:40].replace('\n', ' ')
            print(f"[Recall][Foreshadow]    [{i+1}] {f.status.value}: {preview}...")
    return [
        ForeshadowingItem(
            id=f.id,
            content=f.content,
            status=f.status.value,
            importance=f.importance,
            hints=f.hints,
            resolution=f.resolution
        )
        for f in active
    ]


@app.post("/v1/foreshadowing/{foreshadowing_id}/resolve", tags=["Foreshadowing"])
async def resolve_foreshadowing(
    foreshadowing_id: str,
    resolution: str = Body(..., embed=True),
    user_id: str = Query(default="default", description="ç”¨æˆ·IDï¼ˆè§’è‰²åï¼‰"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """è§£å†³ä¼ç¬”"""
    engine = get_engine()
    success = engine.resolve_foreshadowing(foreshadowing_id, resolution, user_id, character_id)
    
    if not success:
        raise HTTPException(status_code=404, detail="ä¼ç¬”ä¸å­˜åœ¨")
    
    return {"success": True, "message": "ä¼ç¬”å·²è§£å†³"}


@app.post("/v1/foreshadowing/{foreshadowing_id}/hint", tags=["Foreshadowing"])
async def add_foreshadowing_hint(
    foreshadowing_id: str,
    hint: str = Body(..., embed=True, description="æç¤ºå†…å®¹"),
    user_id: str = Query(default="default", description="ç”¨æˆ·IDï¼ˆè§’è‰²åï¼‰"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """æ·»åŠ ä¼ç¬”æç¤º
    
    ä¸ºä¼ç¬”æ·»åŠ è¿›å±•æç¤ºï¼Œä¼šå°†çŠ¶æ€ä» PLANTED æ›´æ–°ä¸º DEVELOPING
    """
    engine = get_engine()
    success = engine.add_foreshadowing_hint(foreshadowing_id, hint, user_id, character_id)
    
    if not success:
        raise HTTPException(status_code=404, detail="ä¼ç¬”ä¸å­˜åœ¨")
    
    return {"success": True, "message": "æç¤ºå·²æ·»åŠ "}


@app.delete("/v1/foreshadowing/{foreshadowing_id}", tags=["Foreshadowing"])
async def abandon_foreshadowing(
    foreshadowing_id: str,
    user_id: str = Query(default="default", description="ç”¨æˆ·IDï¼ˆè§’è‰²åï¼‰"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """æ”¾å¼ƒ/åˆ é™¤ä¼ç¬”
    
    å°†ä¼ç¬”æ ‡è®°ä¸ºå·²æ”¾å¼ƒçŠ¶æ€ï¼ˆä¸ä¼šç‰©ç†åˆ é™¤ï¼Œä¿ç•™å†å²è®°å½•ï¼‰
    """
    engine = get_engine()
    success = engine.abandon_foreshadowing(foreshadowing_id, user_id, character_id)
    
    if not success:
        raise HTTPException(status_code=404, detail="ä¼ç¬”ä¸å­˜åœ¨")
    
    return {"success": True, "message": "ä¼ç¬”å·²æ”¾å¼ƒ"}


@app.delete("/v1/foreshadowing", tags=["Foreshadowing"])
async def clear_all_foreshadowings(
    user_id: str = Query(default="default", description="ç”¨æˆ·IDï¼ˆè§’è‰²åï¼‰"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """æ¸…ç©ºå½“å‰è§’è‰²çš„æ‰€æœ‰ä¼ç¬”"""
    engine = get_engine()
    
    # è·å–æ‰€æœ‰æ´»è·ƒä¼ç¬”
    foreshadowings = engine.get_foreshadowings(user_id, character_id)
    count = len(foreshadowings)
    
    # é€ä¸ªæ”¾å¼ƒ
    for f in foreshadowings:
        engine.abandon_foreshadowing(f['id'], user_id, character_id)
    
    print(f"[Recall][Foreshadow] ğŸ—‘ï¸ æ¸…ç©ºä¼ç¬”: user={user_id}, char={character_id}, åˆ é™¤={count}æ¡")
    return {"success": True, "message": f"å·²æ¸…ç©º {count} ä¸ªä¼ç¬”", "count": count}


@app.get("/v1/foreshadowing/archived", tags=["Foreshadowing"])
async def list_archived_foreshadowings(
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID"),
    page: int = Query(default=1, ge=1, description="é¡µç "),
    page_size: int = Query(default=20, ge=1, le=100, description="æ¯é¡µæ•°é‡"),
    search: Optional[str] = Query(default=None, description="æœç´¢å…³é”®è¯"),
    status: Optional[str] = Query(default=None, description="çŠ¶æ€ç­›é€‰ï¼ˆresolved/abandonedï¼‰")
):
    """è·å–å½’æ¡£çš„ä¼ç¬”åˆ—è¡¨ï¼ˆåˆ†é¡µã€æœç´¢ã€ç­›é€‰ï¼‰"""
    engine = get_engine()
    result = engine.foreshadowing_tracker.get_archived_foreshadowings(
        user_id=user_id,
        character_id=character_id,
        page=page,
        page_size=page_size,
        search=search,
        status=status
    )
    return result


@app.post("/v1/foreshadowing/{foreshadowing_id}/restore", tags=["Foreshadowing"])
async def restore_foreshadowing_from_archive(
    foreshadowing_id: str,
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """ä»å½’æ¡£æ¢å¤ä¼ç¬”åˆ°æ´»è·ƒåˆ—è¡¨"""
    engine = get_engine()
    fsh = engine.foreshadowing_tracker.restore_from_archive(foreshadowing_id, user_id, character_id)
    
    if not fsh:
        raise HTTPException(status_code=404, detail="å½’æ¡£ä¼ç¬”ä¸å­˜åœ¨")
    
    return {
        "success": True,
        "message": "å·²æ¢å¤ä¼ç¬”",
        "foreshadowing": {
            "id": fsh.id,
            "content": fsh.content,
            "status": fsh.status.value,
            "importance": fsh.importance
        }
    }


@app.delete("/v1/foreshadowing/archived/{foreshadowing_id}", tags=["Foreshadowing"])
async def delete_archived_foreshadowing(
    foreshadowing_id: str,
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """å½»åº•åˆ é™¤å½’æ¡£ä¸­çš„ä¼ç¬”"""
    engine = get_engine()
    success = engine.foreshadowing_tracker.delete_archived(foreshadowing_id, user_id, character_id)
    
    if not success:
        raise HTTPException(status_code=404, detail="å½’æ¡£ä¼ç¬”ä¸å­˜åœ¨")
    
    return {"success": True, "message": "å·²å½»åº•åˆ é™¤å½’æ¡£ä¼ç¬”"}


@app.delete("/v1/foreshadowing/archived", tags=["Foreshadowing"])
async def clear_all_archived_foreshadowings(
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """æ¸…ç©ºæ‰€æœ‰å½’æ¡£çš„ä¼ç¬”"""
    engine = get_engine()
    count = engine.foreshadowing_tracker.clear_archived(user_id, character_id)
    return {"success": True, "message": f"å·²æ¸…ç©º {count} ä¸ªå½’æ¡£ä¼ç¬”", "count": count}


@app.post("/v1/foreshadowing/{foreshadowing_id}/archive", tags=["Foreshadowing"])
async def archive_foreshadowing_manually(
    foreshadowing_id: str,
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """æ‰‹åŠ¨å°†æ´»è·ƒä¼ç¬”å½’æ¡£"""
    engine = get_engine()
    success = engine.foreshadowing_tracker.archive_foreshadowing(foreshadowing_id, user_id, character_id)
    
    if not success:
        raise HTTPException(status_code=404, detail="ä¼ç¬”ä¸å­˜åœ¨")
    
    return {"success": True, "message": "å·²å½’æ¡£ä¼ç¬”"}


@app.put("/v1/foreshadowing/{foreshadowing_id}", tags=["Foreshadowing"])
async def update_foreshadowing(
    foreshadowing_id: str,
    content: Optional[str] = Body(default=None, description="æ–°å†…å®¹"),
    status: Optional[str] = Body(default=None, description="æ–°çŠ¶æ€"),
    importance: Optional[float] = Body(default=None, ge=0, le=1, description="æ–°é‡è¦æ€§"),
    hints: Optional[List[str]] = Body(default=None, description="æ–°æç¤ºåˆ—è¡¨"),
    resolution: Optional[str] = Body(default=None, description="è§£å†³æ–¹æ¡ˆ"),
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """ç¼–è¾‘ä¼ç¬”çš„å­—æ®µ"""
    engine = get_engine()
    fsh = engine.foreshadowing_tracker.update_foreshadowing(
        foreshadowing_id=foreshadowing_id,
        user_id=user_id,
        character_id=character_id,
        content=content,
        status=status,
        importance=importance,
        hints=hints,
        resolution=resolution
    )
    
    if not fsh:
        raise HTTPException(status_code=404, detail="ä¼ç¬”ä¸å­˜åœ¨")
    
    return {
        "success": True,
        "message": "å·²æ›´æ–°ä¼ç¬”",
        "foreshadowing": {
            "id": fsh.id,
            "content": fsh.content,
            "status": fsh.status.value,
            "importance": fsh.importance,
            "hints": fsh.hints,
            "resolution": fsh.resolution
        }
    }


# ==================== ä¼ç¬”åˆ†æ API ====================

# åå°åˆ†æä»»åŠ¡é›†åˆï¼ˆé˜²æ­¢è¢«åƒåœ¾å›æ”¶ï¼‰
_background_analysis_tasks: set = set()


async def _background_foreshadowing_analysis(engine: RecallEngine, content: str, role: str, user_id: str, character_id: str):
    """åå°å¼‚æ­¥æ‰§è¡Œä¼ç¬”åˆ†æå’Œæ¡ä»¶æå–
    
    è¿™ä¸ªå‡½æ•°åœ¨åå°è¿è¡Œï¼Œä¸é˜»å¡ API å“åº”ã€‚
    ä½¿ç”¨å¼•æ“çš„å¼‚æ­¥åˆ†ææ–¹æ³•æ¥é¿å…é˜»å¡äº‹ä»¶å¾ªç¯ã€‚
    è®¾ç½® 60 ç§’è¶…æ—¶ï¼Œé˜²æ­¢ LLM è°ƒç”¨å¡ä½å¯¼è‡´çº¿ç¨‹æ± è€—å°½ã€‚
    
    åŒæ—¶è§¦å‘ï¼š
    1. ä¼ç¬”åˆ†æï¼ˆForeshadowingAnalyzer.on_turnï¼‰
    2. æ¡ä»¶æå–ï¼ˆContextTracker.on_turnï¼‰
    
    ä¸¤è€…ä½¿ç”¨ç›¸åŒçš„è§¦å‘é—´éš”æœºåˆ¶ï¼Œé¿å…é‡å¤åˆ†æç›¸åŒå¯¹è¯å†å²ã€‚
    """
    try:
        content_preview = content[:60].replace('\n', ' ') if len(content) > 60 else content.replace('\n', ' ')
        print(f"[Recall][Analysis] ğŸ”„ åå°åˆ†æ: user={user_id}, role={role}")
        print(f"[Recall][Analysis]    å†…å®¹({len(content)}å­—): {content_preview}{'...' if len(content) > 60 else ''}")
        
        loop = asyncio.get_event_loop()
        
        # 1. ä¼ç¬”åˆ†æ
        foreshadow_result = await asyncio.wait_for(
            loop.run_in_executor(
                None,
                lambda: engine.on_foreshadowing_turn(
                    content=content,
                    role=role,
                    user_id=user_id,
                    character_id=character_id
                )
            ),
            timeout=60.0
        )
        if foreshadow_result.triggered:
            print(f"[Recall][Foreshadow] âœ… åˆ†æå®Œæˆ: æ–°ä¼ç¬”={len(foreshadow_result.new_foreshadowings)}, å¯èƒ½è§£å†³={len(foreshadow_result.potentially_resolved)}")
            for f in foreshadow_result.new_foreshadowings[:2]:
                print(f"[Recall][Foreshadow]    ğŸŒ± æ–°ä¼ç¬”: {f[:50]}..." if len(f) > 50 else f"[Recall][Foreshadow]    ğŸŒ± æ–°ä¼ç¬”: {f}")
        else:
            print(f"[Recall][Foreshadow] â­ï¸ æœªè¾¾è§¦å‘æ¡ä»¶")
        if foreshadow_result.error:
            print(f"[Recall][Foreshadow] âš ï¸ è­¦å‘Š: {foreshadow_result.error}")
        
        # 2. æ¡ä»¶æå–ï¼ˆä½¿ç”¨åŒæ ·çš„è§¦å‘é—´éš”æœºåˆ¶ï¼‰
        try:
            context_result = await asyncio.wait_for(
                loop.run_in_executor(
                    None,
                    lambda: engine.context_tracker.on_turn(user_id, character_id)
                ),
                timeout=60.0
            )
            if context_result.get('triggered'):
                print(f"[Recall][Context] âœ… æå–å®Œæˆ: æ–°æ¡ä»¶={context_result.get('extracted_count', 0)}")
                for ctx in context_result.get('extracted', [])[:3]:
                    print(f"[Recall][Context]    ğŸŒ± [{ctx['type']}] {ctx['content'][:40]}..." if len(ctx['content']) > 40 else f"[Recall][Context]    ğŸŒ± [{ctx['type']}] {ctx['content']}")
            else:
                turns_left = context_result.get('turns_until_next', '?')
                print(f"[Recall][Context] â­ï¸ æœªè¾¾è§¦å‘æ¡ä»¶ (è¿˜éœ€ {turns_left} è½®)")
        except Exception as e:
            print(f"[Recall][Context] âš ï¸ æ¡ä»¶æå–å¤±è´¥: {e}")
            
    except asyncio.TimeoutError:
        print(f"[Recall][Analysis] â±ï¸ åˆ†æè¶…æ—¶ (>60s)")
    except Exception as e:
        print(f"[Recall][Analysis] âŒ åˆ†æå¤±è´¥: {e}")


@app.post("/v1/foreshadowing/analyze/turn", response_model=ForeshadowingAnalysisResult, tags=["Foreshadowing Analysis"])
async def analyze_foreshadowing_turn(request: ForeshadowingAnalysisRequest):
    """å¤„ç†æ–°çš„ä¸€è½®å¯¹è¯ï¼ˆç”¨äºä¼ç¬”åˆ†æï¼‰
    
    ã€éé˜»å¡ã€‘: ç«‹å³è¿”å›å“åº”ï¼Œåˆ†æåœ¨åå°å¼‚æ­¥æ‰§è¡Œã€‚
    å®¢æˆ·ç«¯ä¸éœ€è¦ç­‰å¾… LLM åˆ†æå®Œæˆã€‚
    
    åœ¨æ¯è½®å¯¹è¯åè°ƒç”¨æ­¤ç«¯ç‚¹ï¼Œåˆ†æå™¨ä¼šæ ¹æ®é…ç½®å†³å®šæ˜¯å¦è§¦å‘åˆ†æï¼š
    - æ‰‹åŠ¨æ¨¡å¼ï¼šä¸åšä»»ä½•æ“ä½œï¼Œè¿”å›ç©ºç»“æœ
    - LLMæ¨¡å¼ï¼šç´¯ç§¯å¯¹è¯ï¼Œè¾¾åˆ°è§¦å‘æ¡ä»¶æ—¶åœ¨åå°è‡ªåŠ¨åˆ†æ
    """
    engine = get_engine()
    
    # åˆ›å»ºåå°ä»»åŠ¡æ‰§è¡Œåˆ†æï¼ˆä¸ç­‰å¾…ç»“æœï¼‰
    task = asyncio.create_task(
        _background_foreshadowing_analysis(
            engine=engine,
            content=request.content,
            role=request.role,
            user_id=request.user_id,
            character_id=request.character_id
        )
    )
    
    # ä¿å­˜ä»»åŠ¡å¼•ç”¨é˜²æ­¢è¢«åƒåœ¾å›æ”¶
    _background_analysis_tasks.add(task)
    task.add_done_callback(_background_analysis_tasks.discard)
    
    # ç«‹å³è¿”å›ï¼Œä¸ç­‰å¾…åˆ†æå®Œæˆ
    return ForeshadowingAnalysisResult(
        triggered=False,  # å®é™…è§¦å‘çŠ¶æ€åœ¨åå°å¤„ç†
        new_foreshadowings=[],
        potentially_resolved=[],
        error=None
    )


@app.post("/v1/foreshadowing/analyze/trigger", response_model=ForeshadowingAnalysisResult, tags=["Foreshadowing Analysis"])
async def trigger_foreshadowing_analysis(
    user_id: str = Query(default="default", description="ç”¨æˆ·IDï¼ˆè§’è‰²åï¼‰"),
    character_id: str = Query(default="default", description="è§’è‰²ID")
):
    """æ‰‹åŠ¨è§¦å‘ä¼ç¬”åˆ†æ
    
    å¼ºåˆ¶è§¦å‘ LLM åˆ†æï¼ˆå¦‚æœå·²é…ç½®ï¼‰ã€‚å¯ä»¥åœ¨ä»»ä½•æ—¶å€™è°ƒç”¨ã€‚
    """
    engine = get_engine()
    result = engine.trigger_foreshadowing_analysis(user_id, character_id)
    return ForeshadowingAnalysisResult(
        triggered=result.triggered,
        new_foreshadowings=result.new_foreshadowings,
        potentially_resolved=result.potentially_resolved,
        error=result.error
    )


@app.get("/v1/foreshadowing/analyzer/config", tags=["Foreshadowing Analysis"])
async def get_foreshadowing_analyzer_config():
    """è·å–ä¼ç¬”åˆ†æå™¨é…ç½®
    
    é…ç½®é¡¹ï¼š
    - llm_enabled: æ˜¯å¦å¯ç”¨ LLM åˆ†æ
    - trigger_interval: è§¦å‘é—´éš”
    - auto_plant: è‡ªåŠ¨åŸ‹ä¼ç¬”
    - auto_resolve: è‡ªåŠ¨è§£å†³ä¼ç¬”
    - llm_configured: LLM API æ˜¯å¦å·²é…ç½®ï¼ˆåªè¯»ï¼‰
    """
    engine = get_engine()
    analyzer_config = engine.get_foreshadowing_analyzer_config()
    
    # æ£€æŸ¥å®é™…çš„åˆ†æå™¨çŠ¶æ€ï¼ˆbackend == 'llm' è¡¨ç¤º LLM æ¨¡å¼å·²å¯ç”¨ï¼‰
    actual_backend = analyzer_config.get('backend', 'manual')
    llm_enabled = (actual_backend == 'llm')
    
    # æ£€æŸ¥ LLM API æ˜¯å¦å·²é…ç½®
    llm_api_key = os.environ.get('LLM_API_KEY', '')
    llm_configured = bool(llm_api_key)
    
    return {
        "success": True,
        "config": {
            "llm_enabled": llm_enabled,
            "llm_configured": llm_configured,
            "trigger_interval": analyzer_config.get('trigger_interval', 10),
            "auto_plant": analyzer_config.get('auto_plant', True),
            "auto_resolve": analyzer_config.get('auto_resolve', True),
            "backend": actual_backend,
            "llm_model": analyzer_config.get('llm_model', '')
        }
    }


@app.put("/v1/foreshadowing/analyzer/config", tags=["Foreshadowing Analysis"])
async def update_foreshadowing_analyzer_config(config: ForeshadowingConfigUpdate):
    """æ›´æ–°ä¼ç¬”åˆ†æå™¨é…ç½®
    
    é…ç½®ä¼šåŒæ—¶ä¿å­˜åˆ°ï¼š
    1. å†…å­˜ä¸­çš„åˆ†æå™¨å®ä¾‹
    2. api_keys.env é…ç½®æ–‡ä»¶ï¼ˆæŒä¹…åŒ–ï¼‰
    
    æ— éœ€é‡å¯æœåŠ¡ï¼Œé…ç½®ç«‹å³ç”Ÿæ•ˆã€‚
    """
    engine = get_engine()
    
    # å‡†å¤‡è¦æ›´æ–°åˆ°é…ç½®æ–‡ä»¶çš„å†…å®¹
    config_updates = {}
    llm_enable_error = None  # è®°å½• LLM å¯ç”¨å¤±è´¥çš„é”™è¯¯
    
    # å¤„ç† LLM å¯ç”¨å¼€å…³
    if config.llm_enabled is not None:
        config_updates['FORESHADOWING_LLM_ENABLED'] = 'true' if config.llm_enabled else 'false'
        
        # åŠ¨æ€åˆ‡æ¢åˆ†æå™¨æ¨¡å¼
        if config.llm_enabled:
            # å¯ç”¨ LLM æ¨¡å¼
            llm_api_key = os.environ.get('LLM_API_KEY')
            if llm_api_key:
                engine.enable_foreshadowing_llm_mode(
                    api_key=llm_api_key,
                    model=os.environ.get('LLM_MODEL', 'gpt-4o-mini'),
                    base_url=os.environ.get('LLM_API_BASE')
                )
            else:
                # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†å…¶ä»–é…ç½®
                llm_enable_error = "æ— æ³•å¯ç”¨ LLM æ¨¡å¼ï¼šæœªé…ç½® LLM API Key"
                del config_updates['FORESHADOWING_LLM_ENABLED']  # ä¸ä¿å­˜å¤±è´¥çš„é…ç½®
        else:
            # ç¦ç”¨ LLM æ¨¡å¼ï¼Œåˆ‡æ¢åˆ°æ‰‹åŠ¨æ¨¡å¼
            engine.disable_foreshadowing_llm_mode()
    
    # å¤„ç†å…¶ä»–é…ç½®ï¼ˆå³ä½¿ LLM å¯ç”¨å¤±è´¥ä¹Ÿç»§ç»­å¤„ç†ï¼‰
    if config.trigger_interval is not None:
        config_updates['FORESHADOWING_TRIGGER_INTERVAL'] = str(config.trigger_interval)
    if config.auto_plant is not None:
        config_updates['FORESHADOWING_AUTO_PLANT'] = 'true' if config.auto_plant else 'false'
    if config.auto_resolve is not None:
        config_updates['FORESHADOWING_AUTO_RESOLVE'] = 'true' if config.auto_resolve else 'false'
    
    # æ›´æ–°å†…å­˜ä¸­çš„åˆ†æå™¨é…ç½®
    engine.update_foreshadowing_analyzer_config(
        trigger_interval=config.trigger_interval,
        auto_plant=config.auto_plant,
        auto_resolve=config.auto_resolve
    )
    
    # ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
    if config_updates:
        save_config_to_file(config_updates)
    
    # å¦‚æœ LLM å¯ç”¨å¤±è´¥ï¼Œè¿”å›éƒ¨åˆ†æˆåŠŸçš„å“åº”
    if llm_enable_error:
        return {
            "success": False, 
            "message": llm_enable_error,
            "partial_success": True,  # è¡¨ç¤ºå…¶ä»–é…ç½®å·²ä¿å­˜
            "config": (await get_foreshadowing_analyzer_config())["config"]
        }
    
    return {"success": True, "config": (await get_foreshadowing_analyzer_config())["config"]}


# ==================== v4.0 æ—¶æ€çŸ¥è¯†å›¾è°± API ====================

class TemporalQueryRequest(BaseModel):
    """æ—¶æ€æŸ¥è¯¢è¯·æ±‚"""
    entity_name: str = Field(..., description="å®ä½“åç§°")
    timestamp: Optional[str] = Field(None, description="æŸ¥è¯¢æ—¶é—´ç‚¹ (ISO æ ¼å¼)")
    user_id: str = Field(default="default", description="ç”¨æˆ·ID")


class TemporalRangeRequest(BaseModel):
    """æ—¶æ€èŒƒå›´æŸ¥è¯¢è¯·æ±‚"""
    entity_name: str = Field(..., description="å®ä½“åç§°")
    start_time: Optional[str] = Field(None, description="å¼€å§‹æ—¶é—´ (ISO æ ¼å¼)")
    end_time: Optional[str] = Field(None, description="ç»“æŸæ—¶é—´ (ISO æ ¼å¼)")
    user_id: str = Field(default="default", description="ç”¨æˆ·ID")


@app.post("/v1/temporal/at", tags=["Temporal"])
async def get_facts_at_time(request: TemporalQueryRequest):
    """è·å–å®ä½“åœ¨ç‰¹å®šæ—¶é—´ç‚¹çš„äº‹å®
    
    æŸ¥è¯¢æŸä¸ªå®ä½“åœ¨æŒ‡å®šæ—¶é—´ç‚¹çš„çŠ¶æ€/å±æ€§å€¼ã€‚
    å¦‚æœä¸æŒ‡å®šæ—¶é—´ï¼Œè¿”å›æœ€æ–°çŠ¶æ€ã€‚
    """
    engine = get_engine()
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†æ—¶æ€å›¾è°±
    if not hasattr(engine, 'temporal_graph') or engine.temporal_graph is None:
        return {
            "success": False,
            "error": "æ—¶æ€çŸ¥è¯†å›¾è°±æœªå¯ç”¨",
            "facts": []
        }
    
    try:
        from datetime import datetime
        timestamp = None
        if request.timestamp:
            timestamp = datetime.fromisoformat(request.timestamp.replace('Z', '+00:00'))
        
        facts = engine.temporal_graph.get_facts_at_time(
            entity_name=request.entity_name,
            timestamp=timestamp
        )
        
        return {
            "success": True,
            "entity": request.entity_name,
            "timestamp": request.timestamp,
            "facts": [
                {
                    "attribute": f.attribute,
                    "value": f.value,
                    "valid_from": f.valid_from.isoformat() if f.valid_from else None,
                    "valid_to": f.valid_to.isoformat() if f.valid_to else None,
                    "source_turn": f.source_turn,
                    "confidence": f.confidence
                }
                for f in facts
            ]
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "facts": []
        }


@app.post("/v1/temporal/range", tags=["Temporal"])
async def get_facts_in_range(request: TemporalRangeRequest):
    """è·å–å®ä½“åœ¨æ—¶é—´èŒƒå›´å†…çš„æ‰€æœ‰äº‹å®å˜åŒ–
    
    æŸ¥è¯¢æŸä¸ªå®ä½“åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ‰€æœ‰çŠ¶æ€å˜åŒ–å†å²ã€‚
    """
    engine = get_engine()
    
    if not hasattr(engine, 'temporal_graph') or engine.temporal_graph is None:
        return {
            "success": False,
            "error": "æ—¶æ€çŸ¥è¯†å›¾è°±æœªå¯ç”¨",
            "timeline": []
        }
    
    try:
        from datetime import datetime
        start = None
        end = None
        
        if request.start_time:
            start = datetime.fromisoformat(request.start_time.replace('Z', '+00:00'))
        if request.end_time:
            end = datetime.fromisoformat(request.end_time.replace('Z', '+00:00'))
        
        timeline = engine.temporal_graph.get_entity_timeline(
            entity_name=request.entity_name,
            start_time=start,
            end_time=end
        )
        
        return {
            "success": True,
            "entity": request.entity_name,
            "start_time": request.start_time,
            "end_time": request.end_time,
            "timeline": timeline
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "timeline": []
        }


@app.get("/v1/temporal/timeline/{entity_name}", tags=["Temporal"])
async def get_entity_timeline(
    entity_name: str,
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    limit: int = Query(default=50, ge=1, le=200, description="è¿”å›æ•°é‡")
):
    """è·å–å®ä½“çš„å®Œæ•´æ—¶é—´çº¿
    
    è¿”å›å®ä½“çš„æ‰€æœ‰å±æ€§å˜åŒ–å†å²ï¼ŒæŒ‰æ—¶é—´æ’åºã€‚
    """
    engine = get_engine()
    
    if not hasattr(engine, 'temporal_graph') or engine.temporal_graph is None:
        return {
            "success": False,
            "error": "æ—¶æ€çŸ¥è¯†å›¾è°±æœªå¯ç”¨",
            "timeline": []
        }
    
    try:
        timeline = engine.temporal_graph.get_entity_timeline(
            entity_name=entity_name,
            limit=limit
        )
        
        return {
            "success": True,
            "entity": entity_name,
            "timeline": timeline,
            "count": len(timeline)
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "timeline": []
        }


@app.get("/v1/temporal/stats", tags=["Temporal"])
async def get_temporal_stats(user_id: str = Query(default="default", description="ç”¨æˆ·ID")):
    """è·å–æ—¶æ€å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
    engine = get_engine()
    
    if not hasattr(engine, 'temporal_graph') or engine.temporal_graph is None:
        return {
            "success": False,
            "error": "æ—¶æ€çŸ¥è¯†å›¾è°±æœªå¯ç”¨",
            "stats": {}
        }
    
    try:
        stats = engine.temporal_graph.get_stats()
        return {
            "success": True,
            "stats": stats
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "stats": {}
        }


class SnapshotResponse(BaseModel):
    """å¿«ç…§å“åº”"""
    success: bool
    snapshot_id: Optional[str] = None
    timestamp: Optional[str] = None
    entities: Optional[List[Dict[str, Any]]] = None
    error: Optional[str] = None


@app.post("/v1/temporal/snapshot", tags=["Temporal"])
async def create_snapshot(
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    description: str = Query(default="", description="å¿«ç…§æè¿°")
):
    """åˆ›å»ºçŸ¥è¯†å›¾è°±å¿«ç…§
    
    ä¿å­˜å½“å‰æ—¶åˆ»çš„çŸ¥è¯†å›¾è°±çŠ¶æ€ï¼Œç”¨äºåç»­æ¯”è¾ƒæˆ–æ¢å¤ã€‚
    """
    engine = get_engine()
    
    if not hasattr(engine, 'temporal_graph') or engine.temporal_graph is None:
        return {
            "success": False,
            "error": "æ—¶æ€çŸ¥è¯†å›¾è°±æœªå¯ç”¨"
        }
    
    try:
        # åˆ›å»ºå¿«ç…§
        import time
        snapshot_id = f"snap_{int(time.time() * 1000)}"
        timestamp = datetime.now().isoformat()
        
        # è·å–å½“å‰æ‰€æœ‰å®ä½“çŠ¶æ€
        if hasattr(engine.temporal_graph, 'create_snapshot'):
            snapshot = engine.temporal_graph.create_snapshot(
                snapshot_id=snapshot_id,
                description=description
            )
            return {
                "success": True,
                "snapshot_id": snapshot.get('id', snapshot_id),
                "timestamp": snapshot.get('timestamp', timestamp),
                "entity_count": snapshot.get('entity_count', 0),
                "description": description
            }
        else:
            # å›é€€æ–¹æ¡ˆï¼šè®°å½•å½“å‰çŠ¶æ€
            return {
                "success": True,
                "snapshot_id": snapshot_id,
                "timestamp": timestamp,
                "description": description,
                "note": "å¿«ç…§åŠŸèƒ½éœ€è¦ TemporalKnowledgeGraph æ”¯æŒ"
            }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


@app.post("/v1/temporal/snapshot/compare", tags=["Temporal"])
async def compare_snapshots(
    snapshot_id_1: str = Query(..., description="ç¬¬ä¸€ä¸ªå¿«ç…§ID"),
    snapshot_id_2: str = Query(..., description="ç¬¬äºŒä¸ªå¿«ç…§IDï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸å½“å‰çŠ¶æ€æ¯”è¾ƒï¼‰"),
    user_id: str = Query(default="default", description="ç”¨æˆ·ID")
):
    """æ¯”è¾ƒä¸¤ä¸ªå¿«ç…§ä¹‹é—´çš„å·®å¼‚
    
    è¿”å›ä¸¤ä¸ªæ—¶é—´ç‚¹ä¹‹é—´çŸ¥è¯†å›¾è°±çš„å˜åŒ–ï¼šæ–°å¢ã€ä¿®æ”¹ã€åˆ é™¤çš„å®ä½“å’Œå±æ€§ã€‚
    """
    engine = get_engine()
    
    if not hasattr(engine, 'temporal_graph') or engine.temporal_graph is None:
        return {
            "success": False,
            "error": "æ—¶æ€çŸ¥è¯†å›¾è°±æœªå¯ç”¨"
        }
    
    try:
        if hasattr(engine.temporal_graph, 'compare_snapshots'):
            diff = engine.temporal_graph.compare_snapshots(
                snapshot_id_1=snapshot_id_1,
                snapshot_id_2=snapshot_id_2
            )
            return {
                "success": True,
                "snapshot_1": snapshot_id_1,
                "snapshot_2": snapshot_id_2,
                "diff": diff
            }
        else:
            return {
                "success": False,
                "error": "å¿«ç…§æ¯”è¾ƒåŠŸèƒ½éœ€è¦ TemporalKnowledgeGraph æ”¯æŒ",
                "snapshot_1": snapshot_id_1,
                "snapshot_2": snapshot_id_2
            }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


# ==================== v4.0 çŸ›ç›¾æ£€æµ‹ä¸ç®¡ç† API ====================

class ContradictionItem(BaseModel):
    """çŸ›ç›¾é¡¹"""
    id: str
    fact1: Dict[str, Any]
    fact2: Dict[str, Any]
    contradiction_type: str
    detected_at: str
    status: str
    resolution: Optional[str] = None


class ResolveContradictionRequest(BaseModel):
    """è§£å†³çŸ›ç›¾è¯·æ±‚"""
    strategy: str = Field(..., description="è§£å†³ç­–ç•¥: KEEP_NEWER/KEEP_OLDER/KEEP_BOTH/MANUAL")
    manual_resolution: Optional[str] = Field(None, description="æ‰‹åŠ¨è§£å†³è¯´æ˜ï¼ˆstrategy=MANUAL æ—¶ä½¿ç”¨ï¼‰")


@app.get("/v1/contradictions", tags=["Contradictions"])
async def list_contradictions(
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    status: str = Query(default="pending", description="çŠ¶æ€: pending/resolved/all"),
    limit: int = Query(default=50, ge=1, le=200, description="è¿”å›æ•°é‡")
):
    """è·å–çŸ›ç›¾åˆ—è¡¨
    
    è¿”å›æ£€æµ‹åˆ°çš„æ‰€æœ‰çŸ›ç›¾ï¼Œå¯æŒ‰çŠ¶æ€è¿‡æ»¤ã€‚
    """
    engine = get_engine()
    
    if not hasattr(engine, 'contradiction_manager') or engine.contradiction_manager is None:
        return {
            "success": False,
            "error": "çŸ›ç›¾ç®¡ç†å™¨æœªå¯ç”¨",
            "contradictions": []
        }
    
    try:
        contradictions = engine.contradiction_manager.get_contradictions(
            status=status if status != "all" else None,
            limit=limit
        )
        
        return {
            "success": True,
            "contradictions": [
                {
                    "id": c.id,
                    "fact1": c.fact1,
                    "fact2": c.fact2,
                    "contradiction_type": c.contradiction_type.value if hasattr(c.contradiction_type, 'value') else str(c.contradiction_type),
                    "detected_at": c.detected_at.isoformat() if hasattr(c.detected_at, 'isoformat') else str(c.detected_at),
                    "status": c.status,
                    "resolution": c.resolution
                }
                for c in contradictions
            ],
            "count": len(contradictions)
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "contradictions": []
        }


@app.get("/v1/contradictions/{contradiction_id}", tags=["Contradictions"])
async def get_contradiction(contradiction_id: str):
    """è·å–å•ä¸ªçŸ›ç›¾è¯¦æƒ…"""
    engine = get_engine()
    
    if not hasattr(engine, 'contradiction_manager') or engine.contradiction_manager is None:
        raise HTTPException(status_code=503, detail="çŸ›ç›¾ç®¡ç†å™¨æœªå¯ç”¨")
    
    try:
        contradiction = engine.contradiction_manager.get_contradiction(contradiction_id)
        if not contradiction:
            raise HTTPException(status_code=404, detail="çŸ›ç›¾ä¸å­˜åœ¨")
        
        return {
            "success": True,
            "contradiction": {
                "id": contradiction.id,
                "fact1": contradiction.fact1,
                "fact2": contradiction.fact2,
                "contradiction_type": contradiction.contradiction_type.value if hasattr(contradiction.contradiction_type, 'value') else str(contradiction.contradiction_type),
                "detected_at": contradiction.detected_at.isoformat() if hasattr(contradiction.detected_at, 'isoformat') else str(contradiction.detected_at),
                "status": contradiction.status,
                "resolution": contradiction.resolution
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/v1/contradictions/{contradiction_id}/resolve", tags=["Contradictions"])
async def resolve_contradiction(
    contradiction_id: str,
    request: ResolveContradictionRequest
):
    """è§£å†³çŸ›ç›¾
    
    ç­–ç•¥:
    - KEEP_NEWER: ä¿ç•™è¾ƒæ–°çš„äº‹å®
    - KEEP_OLDER: ä¿ç•™è¾ƒæ—§çš„äº‹å®
    - KEEP_BOTH: ä¿ç•™ä¸¤è€…ï¼ˆæ ‡è®°ä¸ºå·²å¤„ç†ä½†ä¸åˆ é™¤ï¼‰
    - MANUAL: æ‰‹åŠ¨æä¾›è§£å†³æ–¹æ¡ˆ
    """
    engine = get_engine()
    
    if not hasattr(engine, 'contradiction_manager') or engine.contradiction_manager is None:
        raise HTTPException(status_code=503, detail="çŸ›ç›¾ç®¡ç†å™¨æœªå¯ç”¨")
    
    try:
        result = engine.contradiction_manager.resolve_contradiction(
            contradiction_id=contradiction_id,
            strategy=request.strategy,
            manual_resolution=request.manual_resolution
        )
        
        return {
            "success": result.success if hasattr(result, 'success') else True,
            "message": result.message if hasattr(result, 'message') else "çŸ›ç›¾å·²è§£å†³",
            "resolution": result.resolution if hasattr(result, 'resolution') else request.strategy
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/v1/contradictions/stats", tags=["Contradictions"])
async def get_contradiction_stats(user_id: str = Query(default="default", description="ç”¨æˆ·ID")):
    """è·å–çŸ›ç›¾ç»Ÿè®¡ä¿¡æ¯"""
    engine = get_engine()
    
    if not hasattr(engine, 'contradiction_manager') or engine.contradiction_manager is None:
        return {
            "success": False,
            "error": "çŸ›ç›¾ç®¡ç†å™¨æœªå¯ç”¨",
            "stats": {}
        }
    
    try:
        stats = engine.contradiction_manager.get_stats()
        return {
            "success": True,
            "stats": stats
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "stats": {}
        }


# ==================== v4.0 å…¨æ–‡æ£€ç´¢ API ====================

class FulltextSearchRequest(BaseModel):
    """å…¨æ–‡æ£€ç´¢è¯·æ±‚"""
    query: str = Field(..., description="æœç´¢æŸ¥è¯¢")
    user_id: str = Field(default="default", description="ç”¨æˆ·ID")
    top_k: int = Field(default=10, ge=1, le=100, description="è¿”å›æ•°é‡")


@app.post("/v1/search/fulltext", tags=["Search"])
async def fulltext_search(request: FulltextSearchRequest):
    """BM25 å…¨æ–‡æ£€ç´¢
    
    ä½¿ç”¨ BM25 ç®—æ³•è¿›è¡Œå…¨æ–‡æ£€ç´¢ï¼Œé€‚åˆå…³é”®è¯ç²¾ç¡®åŒ¹é…åœºæ™¯ã€‚
    ä¸å‘é‡æœç´¢äº’è¡¥ï¼Œå¯ç”¨äºæ··åˆæœç´¢ã€‚
    """
    engine = get_engine()
    
    if not hasattr(engine, 'fulltext_index') or engine.fulltext_index is None:
        return {
            "success": False,
            "error": "å…¨æ–‡ç´¢å¼•æœªå¯ç”¨",
            "results": []
        }
    
    try:
        results = engine.fulltext_index.search(
            query=request.query,
            top_k=request.top_k
        )
        
        return {
            "success": True,
            "query": request.query,
            "results": [
                {
                    "id": r.get("id"),
                    "content": r.get("content"),
                    "score": r.get("score"),
                    "metadata": r.get("metadata", {})
                }
                for r in results
            ],
            "count": len(results)
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }


@app.post("/v1/search/hybrid", tags=["Search"])
async def hybrid_search(request: SearchRequest):
    """æ··åˆæœç´¢
    
    ç»“åˆå‘é‡æœç´¢å’Œ BM25 å…¨æ–‡æ£€ç´¢çš„æ··åˆæœç´¢ã€‚
    åŒæ—¶åˆ©ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…ã€‚
    """
    engine = get_engine()
    
    try:
        # å°è¯•ä½¿ç”¨å¼•æ“çš„æ··åˆæœç´¢
        if hasattr(engine, 'hybrid_search'):
            results = engine.hybrid_search(
                query=request.query,
                user_id=request.user_id,
                top_k=request.top_k,
                filters=request.filters
            )
        else:
            # å›é€€åˆ°æ™®é€šæœç´¢
            results = engine.search(
                query=request.query,
                user_id=request.user_id,
                top_k=request.top_k,
                filters=request.filters
            )
        
        return {
            "success": True,
            "query": request.query,
            "results": [
                {
                    "id": r.id,
                    "content": r.content,
                    "score": r.score,
                    "metadata": r.metadata,
                    "entities": r.entities if hasattr(r, 'entities') else []
                }
                for r in results
            ],
            "count": len(results)
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "results": []
        }


# ==================== Phase 3: æ£€ç´¢é…ç½® API ====================

@app.get("/v1/search/config", response_model=RetrievalConfigResponse, tags=["Search"])
async def get_search_config():
    """è·å–å½“å‰æ£€ç´¢é…ç½®ï¼ˆPhase 3ï¼‰
    
    è¿”å›å½“å‰æ£€ç´¢å™¨çš„é…ç½®çŠ¶æ€ï¼ŒåŒ…æ‹¬ï¼š
    - æ£€ç´¢å™¨ç±»å‹ï¼ˆElevenLayer/EightLayerï¼‰
    - å„å±‚å¼€å…³çŠ¶æ€
    - Top-K å‚æ•°
    - æƒé‡é…ç½®
    """
    engine = get_engine()
    retriever = engine.retriever
    
    # åˆ¤æ–­æ£€ç´¢å™¨ç±»å‹
    from recall.retrieval.eleven_layer import ElevenLayerRetriever
    is_eleven_layer = isinstance(retriever, ElevenLayerRetriever)
    
    if is_eleven_layer and hasattr(retriever, 'config'):
        config = retriever.config
        return RetrievalConfigResponse(
            retriever_type="ElevenLayer",
            l1_enabled=config.l1_enabled,
            l2_enabled=config.l2_enabled,
            l3_enabled=config.l3_enabled,
            l4_enabled=config.l4_enabled,
            l5_enabled=config.l5_enabled,
            l6_enabled=config.l6_enabled,
            l7_enabled=config.l7_enabled,
            l8_enabled=config.l8_enabled,
            l9_enabled=config.l9_enabled,
            l10_enabled=config.l10_enabled,
            l11_enabled=config.l11_enabled,
            l7_vector_top_k=config.l7_vector_top_k,
            final_top_k=config.final_top_k,
            weights={
                "inverted": config.weights.inverted,
                "entity": config.weights.entity,
                "graph": config.weights.graph,
                "ngram": config.weights.ngram,
                "vector": config.weights.vector,
                "temporal": config.weights.temporal,
            }
        )
    else:
        # EightLayerRetriever æˆ–å…¼å®¹æ¨¡å¼
        old_config = getattr(retriever, 'config', {})
        return RetrievalConfigResponse(
            retriever_type="EightLayer",
            l1_enabled=old_config.get('l1_enabled', True),
            l2_enabled=False,  # æ—§ç‰ˆæ—  L2
            l3_enabled=old_config.get('l2_enabled', True),  # æ—§ L2 = æ–° L3
            l4_enabled=old_config.get('l3_enabled', True),  # æ—§ L3 = æ–° L4
            l5_enabled=False,  # æ—§ç‰ˆæ—  L5
            l6_enabled=old_config.get('l4_enabled', True),  # æ—§ L4 = æ–° L6
            l7_enabled=old_config.get('l5_enabled', True),  # æ—§ L5 = æ–° L7
            l8_enabled=old_config.get('l6_enabled', True),  # æ—§ L6 = æ–° L8
            l9_enabled=old_config.get('l7_enabled', True),  # æ—§ L7 = æ–° L9
            l10_enabled=False,  # æ—§ç‰ˆæ—  L10
            l11_enabled=old_config.get('l8_enabled', False),  # æ—§ L8 = æ–° L11
            l7_vector_top_k=200,
            final_top_k=20,
            weights={}
        )


@app.put("/v1/search/config", response_model=RetrievalConfigResponse, tags=["Search"])
async def update_search_config(request: RetrievalConfigRequest):
    """åŠ¨æ€æ›´æ–°æ£€ç´¢é…ç½®ï¼ˆPhase 3ï¼‰
    
    å…è®¸åœ¨è¿è¡Œæ—¶è°ƒæ•´æ£€ç´¢ç­–ç•¥ï¼Œæ— éœ€é‡å¯æœåŠ¡ã€‚
    
    ä½¿ç”¨æ–¹å¼ï¼š
    - ä¼ å…¥ preset="fast" å¿«é€Ÿåº”ç”¨é¢„è®¾
    - æˆ–å•ç‹¬è®¾ç½®å„å±‚å¼€å…³å’Œå‚æ•°
    
    æ³¨æ„ï¼šæ­¤æ›´æ”¹ä»…å½±å“å½“å‰è¿›ç¨‹ï¼Œé‡å¯åä¼šæ¢å¤ä¸ºç¯å¢ƒå˜é‡é…ç½®ã€‚
    """
    engine = get_engine()
    retriever = engine.retriever
    
    from recall.retrieval.eleven_layer import ElevenLayerRetriever
    from recall.retrieval.config import RetrievalConfig
    
    if not isinstance(retriever, ElevenLayerRetriever):
        raise HTTPException(
            status_code=400,
            detail="å½“å‰ä½¿ç”¨ EightLayerRetrieverï¼Œä¸æ”¯æŒåŠ¨æ€é…ç½®ã€‚è¯·è®¾ç½® ELEVEN_LAYER_RETRIEVER_ENABLED=true å¯ç”¨ ElevenLayerRetrieverã€‚"
        )
    
    config = retriever.config
    
    # åº”ç”¨é¢„è®¾
    if request.preset:
        if request.preset == "fast":
            new_config = RetrievalConfig.fast()
        elif request.preset == "accurate":
            new_config = RetrievalConfig.accurate()
        elif request.preset == "default":
            new_config = RetrievalConfig.default()
        else:
            raise HTTPException(status_code=400, detail=f"æœªçŸ¥é¢„è®¾: {request.preset}")
        retriever.config = new_config
        config = new_config
    else:
        # å•ç‹¬æ›´æ–°å„å­—æ®µ
        if request.l1_enabled is not None:
            config.l1_enabled = request.l1_enabled
        if request.l2_enabled is not None:
            config.l2_enabled = request.l2_enabled
        if request.l3_enabled is not None:
            config.l3_enabled = request.l3_enabled
        if request.l4_enabled is not None:
            config.l4_enabled = request.l4_enabled
        if request.l5_enabled is not None:
            config.l5_enabled = request.l5_enabled
        if request.l6_enabled is not None:
            config.l6_enabled = request.l6_enabled
        if request.l7_enabled is not None:
            config.l7_enabled = request.l7_enabled
        if request.l8_enabled is not None:
            config.l8_enabled = request.l8_enabled
        if request.l9_enabled is not None:
            config.l9_enabled = request.l9_enabled
        if request.l10_enabled is not None:
            config.l10_enabled = request.l10_enabled
        if request.l11_enabled is not None:
            config.l11_enabled = request.l11_enabled
        if request.l7_vector_top_k is not None:
            config.l7_vector_top_k = request.l7_vector_top_k
        if request.final_top_k is not None:
            config.final_top_k = request.final_top_k
    
    print(f"[Recall][Config] âš™ï¸ æ£€ç´¢é…ç½®å·²æ›´æ–°")
    
    # è¿”å›æ›´æ–°åçš„é…ç½®
    return RetrievalConfigResponse(
        retriever_type="ElevenLayer",
        l1_enabled=config.l1_enabled,
        l2_enabled=config.l2_enabled,
        l3_enabled=config.l3_enabled,
        l4_enabled=config.l4_enabled,
        l5_enabled=config.l5_enabled,
        l6_enabled=config.l6_enabled,
        l7_enabled=config.l7_enabled,
        l8_enabled=config.l8_enabled,
        l9_enabled=config.l9_enabled,
        l10_enabled=config.l10_enabled,
        l11_enabled=config.l11_enabled,
        l7_vector_top_k=config.l7_vector_top_k,
        final_top_k=config.final_top_k,
        weights={
            "inverted": config.weights.inverted,
            "entity": config.weights.entity,
            "graph": config.weights.graph,
            "ngram": config.weights.ngram,
            "vector": config.weights.vector,
            "temporal": config.weights.temporal,
        }
    )


# ==================== v4.0 å›¾è°±éå† API ====================

class GraphTraverseRequest(BaseModel):
    """å›¾è°±éå†è¯·æ±‚"""
    start_entity: str = Field(..., description="èµ·å§‹å®ä½“")
    max_depth: int = Field(default=2, ge=1, le=5, description="æœ€å¤§æ·±åº¦")
    relation_types: Optional[List[str]] = Field(None, description="å…³ç³»ç±»å‹è¿‡æ»¤")
    user_id: str = Field(default="default", description="ç”¨æˆ·ID")


@app.post("/v1/graph/traverse", tags=["Graph"])
async def traverse_graph(request: GraphTraverseRequest):
    """çŸ¥è¯†å›¾è°±éå†
    
    ä»æŒ‡å®šå®ä½“å¼€å§‹ï¼ŒæŒ‰å…³ç³»éå†çŸ¥è¯†å›¾è°±ã€‚
    è¿”å›éå†è·¯å¾„ä¸Šçš„æ‰€æœ‰å®ä½“å’Œå…³ç³»ã€‚
    """
    engine = get_engine()
    
    try:
        # ä½¿ç”¨æ—¶æ€å›¾è°±æˆ–æ™®é€šå›¾è°±
        if hasattr(engine, 'temporal_graph') and engine.temporal_graph is not None:
            result = engine.temporal_graph.traverse(
                start_entity=request.start_entity,
                max_depth=request.max_depth,
                relation_types=request.relation_types
            )
        elif hasattr(engine, 'knowledge_graph') and engine.knowledge_graph is not None:
            result = engine.knowledge_graph.traverse(
                start_entity=request.start_entity,
                max_depth=request.max_depth,
                relation_types=request.relation_types
            )
        else:
            return {
                "success": False,
                "error": "çŸ¥è¯†å›¾è°±æœªå¯ç”¨",
                "nodes": [],
                "edges": []
            }
        
        return {
            "success": True,
            "start_entity": request.start_entity,
            "nodes": result.get("nodes", []),
            "edges": result.get("edges", []),
            "depth_reached": result.get("depth_reached", 0)
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "nodes": [],
            "edges": []
        }


@app.get("/v1/graph/entity/{entity_name}/neighbors", tags=["Graph"])
async def get_entity_neighbors(
    entity_name: str,
    user_id: str = Query(default="default", description="ç”¨æˆ·ID"),
    include_relations: bool = Query(default=True, description="æ˜¯å¦åŒ…å«å…³ç³»è¯¦æƒ…")
):
    """è·å–å®ä½“çš„é‚»å±…èŠ‚ç‚¹
    
    è¿”å›ä¸æŒ‡å®šå®ä½“ç›´æ¥ç›¸è¿çš„æ‰€æœ‰å®ä½“ã€‚
    """
    engine = get_engine()
    
    try:
        related = engine.get_related_entities(entity_name)
        
        return {
            "success": True,
            "entity": entity_name,
            "neighbors": related,
            "count": len(related)
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "neighbors": []
        }


# ==================== å®ä½“ API ====================

@app.get("/v1/entities/{name}", tags=["Entities"])
async def get_entity(name: str):
    """è·å–å®ä½“ä¿¡æ¯"""
    engine = get_engine()
    entity = engine.get_entity(name)
    
    if not entity:
        raise HTTPException(status_code=404, detail="å®ä½“ä¸å­˜åœ¨")
    
    return entity


@app.get("/v1/entities/{name}/related", tags=["Entities"])
async def get_related_entities(name: str):
    """è·å–ç›¸å…³å®ä½“"""
    engine = get_engine()
    related = engine.get_related_entities(name)
    return {"entity": name, "related": related}


# ==================== ç®¡ç† API ====================

@app.get("/v1/stats", tags=["Admin"])
async def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    engine = get_engine()
    return engine.get_stats()


@app.post("/v1/indexes/rebuild-vector", tags=["Admin"])
async def rebuild_vector_index(user_id: Optional[str] = None):
    """é‡å»ºå‘é‡ç´¢å¼•
    
    ä»ç°æœ‰è®°å¿†æ•°æ®é‡æ–°ç”Ÿæˆå‘é‡ç´¢å¼•ã€‚
    ç”¨äºä¿®å¤ç»´åº¦ä¸åŒ¹é…ã€ç´¢å¼•æŸåç­‰é—®é¢˜ã€‚
    
    Args:
        user_id: å¯é€‰ï¼ŒæŒ‡å®šåªé‡å»ºæŸä¸ªç”¨æˆ·çš„ç´¢å¼•ã€‚ä¸ºç©ºæ—¶é‡å»ºæ‰€æœ‰ç”¨æˆ·ã€‚
        
    æ³¨æ„ï¼šé‡å»ºè¿‡ç¨‹ä¼šæ¶ˆè€—è¾ƒå¤šæ—¶é—´å’Œ API è°ƒç”¨ï¼ˆå¦‚æœä½¿ç”¨ API embeddingï¼‰ã€‚
    """
    engine = get_engine()
    print(f"[Recall] æ”¶åˆ°é‡å»ºå‘é‡ç´¢å¼•è¯·æ±‚: user_id={user_id}")
    result = engine.rebuild_vector_index(user_id)
    return result


@app.get("/v1/users", tags=["Admin"])
async def list_users():
    """åˆ—å‡ºæ‰€æœ‰ç”¨æˆ·ï¼ˆè§’è‰²ï¼‰
    
    è¿”å›æ‰€æœ‰æœ‰è®°å¿†æ•°æ®çš„è§’è‰²åˆ—è¡¨ï¼Œä»¥åŠæ¯ä¸ªè§’è‰²çš„è®°å¿†æ•°é‡ã€‚
    ç”¨äºç®¡ç†å’Œæ¸…ç†ä¸å†éœ€è¦çš„è§’è‰²æ•°æ®ã€‚
    """
    engine = get_engine()
    users = engine.storage.list_users()
    
    result = []
    for user_id in users:
        memories = engine.get_all(user_id=user_id, limit=10000)
        result.append({
            "user_id": user_id,
            "memory_count": len(memories)
        })
    
    return {
        "users": result,
        "total": len(result)
    }


@app.post("/v1/config/reload", tags=["Admin"])
async def reload_config():
    """çƒ­æ›´æ–°é…ç½®
    
    é‡æ–°åŠ è½½ recall_data/config/api_keys.env é…ç½®æ–‡ä»¶ã€‚
    ä¿®æ”¹ API Key åè°ƒç”¨æ­¤æ¥å£å³å¯ç”Ÿæ•ˆï¼Œæ— éœ€é‡å¯æœåŠ¡ã€‚
    
    ä½¿ç”¨æ–¹æ³•ï¼š
    1. ç¼–è¾‘ recall_data/config/api_keys.env æ–‡ä»¶
    2. è°ƒç”¨æ­¤æ¥å£: curl -X POST http://localhost:18888/v1/config/reload
    """
    try:
        engine = reload_engine()
        stats = engine.get_stats()
        
        # è·å–å½“å‰ embedding æ¨¡å¼
        embedding_info = "Lite æ¨¡å¼" if stats.get('lite') else "Local/Cloud æ¨¡å¼"
        
        return {
            "success": True,
            "message": "é…ç½®å·²é‡æ–°åŠ è½½",
            "embedding_mode": embedding_info,
            "config_file": str(get_config_file_path())
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é‡æ–°åŠ è½½å¤±è´¥: {str(e)}")


@app.post("/v1/maintenance/rebuild-index", tags=["Admin"])
async def rebuild_index():
    """æ‰‹åŠ¨é‡å»º VectorIndex ç´¢å¼•
    
    ä¸ºæ‰€æœ‰ä¼ç¬”å’Œæ¡ä»¶é‡å»ºè¯­ä¹‰ç´¢å¼•ã€‚é€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨è°ƒç”¨ï¼Œ
    ç³»ç»Ÿä¼šåœ¨é¦–æ¬¡å‡çº§æ—¶è‡ªåŠ¨é‡å»ºã€‚
    
    ä½¿ç”¨åœºæ™¯ï¼š
    - ç´¢å¼•æ•°æ®æŸåéœ€è¦é‡å»º
    - æ‰‹åŠ¨å¯¼å…¥äº†æ•°æ®æ–‡ä»¶
    - ä»å¤‡ä»½æ¢å¤åéœ€è¦é‡å»ºç´¢å¼•
    
    æ³¨æ„ï¼šæ­¤æ“ä½œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå–å†³äºæ•°æ®é‡å¤§å°ã€‚
    """
    try:
        engine = get_engine()
        
        if not engine._vector_index or not engine._vector_index.enabled:
            return {
                "success": False,
                "message": "VectorIndex æœªå¯ç”¨ï¼ˆLite æ¨¡å¼ä¸‹ä¸å¯ç”¨ï¼‰",
                "indexed_count": 0
            }
        
        # ä½¿ç”¨å…¬å¼€æ–¹æ³•å¼ºåˆ¶é‡å»ºç´¢å¼•
        indexed_count = engine.rebuild_vector_index(force=True)
        
        return {
            "success": True,
            "message": "ç´¢å¼•é‡å»ºå®Œæˆ",
            "indexed_count": indexed_count
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç´¢å¼•é‡å»ºå¤±è´¥: {str(e)}")


@app.get("/v1/config", tags=["Admin"])
async def get_config():
    """è·å–å½“å‰é…ç½®ä¿¡æ¯
    
    æ˜¾ç¤ºé…ç½®æ–‡ä»¶è·¯å¾„å’Œå½“å‰é…ç½®çŠ¶æ€ï¼ˆæ•æ„Ÿä¿¡æ¯è„±æ•ï¼‰
    """
    config_file = get_config_file_path()
    
    # æ£€æŸ¥å„ç§é…ç½®çŠ¶æ€
    def mask_key(key: str) -> str:
        """è„±æ•æ˜¾ç¤º API Key"""
        if not key:
            return "æœªé…ç½®"
        if len(key) > 8:
            return key[:4] + '****' + key[-4:]
        return '****'
    
    def safe_int(val: str, default: int) -> int:
        """å®‰å…¨è½¬æ¢æ•´æ•°"""
        try:
            return int(val) if val else default
        except (ValueError, TypeError):
            return default
    
    def safe_float(val: str, default: float) -> float:
        """å®‰å…¨è½¬æ¢æµ®ç‚¹æ•°"""
        try:
            return float(val) if val else default
        except (ValueError, TypeError):
            return default
    
    def safe_bool(val: str, default: bool) -> bool:
        """å®‰å…¨è½¬æ¢å¸ƒå°”å€¼"""
        if not val:
            return default
        return val.lower() in ('true', '1', 'yes', 'on')
    
    # Embedding é…ç½®
    embedding_key = os.environ.get('EMBEDDING_API_KEY', '')
    embedding_base = os.environ.get('EMBEDDING_API_BASE', '')
    embedding_model = os.environ.get('EMBEDDING_MODEL', '')
    embedding_dimension = os.environ.get('EMBEDDING_DIMENSION', '')
    embedding_mode = os.environ.get('RECALL_EMBEDDING_MODE', 'auto')
    
    # LLM é…ç½®
    llm_key = os.environ.get('LLM_API_KEY', '')
    llm_base = os.environ.get('LLM_API_BASE', '')
    llm_model = os.environ.get('LLM_MODEL', '')
    
    # å®¹é‡é™åˆ¶é…ç½®
    context_trigger_interval = safe_int(os.environ.get('CONTEXT_TRIGGER_INTERVAL', ''), 5)
    context_max_context_turns = safe_int(os.environ.get('CONTEXT_MAX_CONTEXT_TURNS', ''), 20)
    context_max_per_type = safe_int(os.environ.get('CONTEXT_MAX_PER_TYPE', ''), 30)
    context_max_total = safe_int(os.environ.get('CONTEXT_MAX_TOTAL', ''), 100)
    context_decay_days = safe_int(os.environ.get('CONTEXT_DECAY_DAYS', ''), 7)
    context_decay_rate = safe_float(os.environ.get('CONTEXT_DECAY_RATE', ''), 0.1)
    context_min_confidence = safe_float(os.environ.get('CONTEXT_MIN_CONFIDENCE', ''), 0.3)
    
    # ä¸Šä¸‹æ–‡æ„å»ºé…ç½®
    build_context_include_recent = safe_int(os.environ.get('BUILD_CONTEXT_INCLUDE_RECENT', ''), 10)
    proactive_reminder_enabled = safe_bool(os.environ.get('PROACTIVE_REMINDER_ENABLED', ''), True)
    proactive_reminder_turns = safe_int(os.environ.get('PROACTIVE_REMINDER_TURNS', ''), 50)
    
    foreshadowing_max_return = safe_int(os.environ.get('FORESHADOWING_MAX_RETURN', ''), 5)
    foreshadowing_max_active = safe_int(os.environ.get('FORESHADOWING_MAX_ACTIVE', ''), 50)
    
    # æ™ºèƒ½å»é‡é…ç½®
    dedup_embedding_enabled = safe_bool(os.environ.get('DEDUP_EMBEDDING_ENABLED', ''), True)
    dedup_high_threshold = safe_float(os.environ.get('DEDUP_HIGH_THRESHOLD', ''), 0.92)
    dedup_low_threshold = safe_float(os.environ.get('DEDUP_LOW_THRESHOLD', ''), 0.75)
    
    return {
        "config_file": str(config_file),
        "config_file_exists": config_file.exists(),
        "embedding": {
            "api_key": mask_key(embedding_key),
            "api_base": embedding_base or "æœªé…ç½®",
            "model": embedding_model or "æœªé…ç½®",
            "dimension": embedding_dimension or "æœªé…ç½®",
            "mode": embedding_mode,
            "status": "å·²é…ç½®" if (embedding_key and embedding_base) else "æœªé…ç½®"
        },
        "llm": {
            "api_key": mask_key(llm_key),
            "api_base": llm_base or "æœªé…ç½®",
            "model": llm_model or "æœªé…ç½®",
            "status": "å·²é…ç½®" if llm_key else "æœªé…ç½®"
        },
        "capacity_limits": {
            "context": {
                "trigger_interval": context_trigger_interval,
                "max_per_type": context_max_per_type,
                "max_total": context_max_total,
                "decay_days": context_decay_days,
                "decay_rate": context_decay_rate,
                "min_confidence": context_min_confidence
            },
            "build_context": {
                "max_context_turns": context_max_context_turns,
                "include_recent": build_context_include_recent,
                "proactive_reminder_enabled": proactive_reminder_enabled,
                "proactive_reminder_turns": proactive_reminder_turns
            },
            "foreshadowing": {
                "max_return": foreshadowing_max_return,
                "max_active": foreshadowing_max_active
            },
            "dedup": {
                "embedding_enabled": dedup_embedding_enabled,
                "high_threshold": dedup_high_threshold,
                "low_threshold": dedup_low_threshold
            }
        },
        "hint": "ç¼–è¾‘é…ç½®æ–‡ä»¶åè°ƒç”¨ POST /v1/config/reload çƒ­æ›´æ–°ï¼Œæµ‹è¯•è¿æ¥ GET /v1/config/test"
    }


@app.get("/v1/config/test", tags=["Admin"])
async def test_connection():
    """æµ‹è¯• Embedding API è¿æ¥
    
    æµ‹è¯•å½“å‰é…ç½®çš„ Embedding API æ˜¯å¦å¯ä»¥æ­£å¸¸è¿æ¥ã€‚
    ä¼šå®é™…è°ƒç”¨ API ç”Ÿæˆä¸€ä¸ªæµ‹è¯•å‘é‡æ¥éªŒè¯ã€‚
    
    ä½¿ç”¨æ–¹æ³•ï¼š
    curl http://localhost:18888/v1/config/test
    
    è¿”å›ï¼š
    - success: true/false
    - message: æµ‹è¯•ç»“æœæè¿°
    - backend: å½“å‰ä½¿ç”¨çš„åç«¯ç±»å‹
    - model: å½“å‰ä½¿ç”¨çš„æ¨¡å‹
    - dimension: å‘é‡ç»´åº¦
    - latency_ms: API è°ƒç”¨å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    """
    engine = get_engine()
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ Lite æ¨¡å¼
    config = engine.embedding_config
    if engine.lightweight or not config or config.backend.value == "none":
        return {
            "success": True,
            "message": "Lite æ¨¡å¼æ— éœ€æµ‹è¯• API è¿æ¥",
            "backend": "none",
            "model": None,
            "dimension": None,
            "latency_ms": 0
        }
    
    # ä»å¼•æ“è·å–å½“å‰é…ç½®
    backend_type = config.backend.value if config.backend else "unknown"
    model = config.api_model or config.local_model or "unknown"
    dimension = config.dimension
    
    # è·å– embedding åç«¯å¹¶æµ‹è¯•
    try:
        # å®é™…æµ‹è¯• embedding è°ƒç”¨
        start_time = time.time()
        test_text = "Hello, this is a test."
        
        # å°è¯•è·å– embedding
        if engine._vector_index and engine._vector_index.embedding_backend:
            embedding_backend = engine._vector_index.embedding_backend
            embedding = embedding_backend.encode(test_text)
            actual_dimension = len(embedding)
            elapsed_ms = (time.time() - start_time) * 1000
            
            return {
                "success": True,
                "message": f"API è¿æ¥æˆåŠŸï¼æ¨¡å‹ {model} å·¥ä½œæ­£å¸¸",
                "backend": backend_type,
                "model": model,
                "dimension": actual_dimension,
                "latency_ms": round(elapsed_ms, 2)
            }
        else:
            return {
                "success": False,
                "message": "Embedding åç«¯æœªåˆå§‹åŒ–ï¼ˆå¯èƒ½æ˜¯ Lite æ¨¡å¼æˆ–ç´¢å¼•æœªåŠ è½½ï¼‰",
                "backend": backend_type,
                "model": model,
                "dimension": dimension,
                "latency_ms": 0
            }
            
    except Exception as e:
        error_msg = str(e)
        
        # å‹å¥½çš„é”™è¯¯æç¤º
        if "API key" in error_msg.lower() or "unauthorized" in error_msg.lower() or "401" in error_msg:
            friendly_msg = "API Key æ— æ•ˆæˆ–æœªé…ç½®"
        elif "connection" in error_msg.lower() or "network" in error_msg.lower():
            friendly_msg = "ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ– API åœ°å€"
        elif "model" in error_msg.lower() or "404" in error_msg:
            friendly_msg = "æ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨"
        else:
            friendly_msg = f"è¿æ¥å¤±è´¥: {error_msg}"
        
        # å°è¯•è·å–å½“å‰é…ç½®ä¿¡æ¯ç”¨äºé”™è¯¯å“åº”
        try:
            cfg = engine.embedding_config
            current_backend = cfg.backend.value if cfg and cfg.backend else 'unknown'
            current_model = (cfg.api_model or cfg.local_model) if cfg else None
        except:
            current_backend = os.environ.get('RECALL_EMBEDDING_MODE', 'auto')
            current_model = None
        
        return {
            "success": False,
            "message": friendly_msg,
            "error": error_msg,
            "backend": current_backend,
            "model": current_model,
            "dimension": None,
            "latency_ms": 0
        }


@app.get("/v1/config/test/llm", tags=["Admin"])
async def test_llm_connection():
    """æµ‹è¯• LLM API è¿æ¥
    
    æµ‹è¯•å½“å‰é…ç½®çš„ LLM API æ˜¯å¦å¯ä»¥æ­£å¸¸è¿æ¥ã€‚
    ä¼šå®é™…è°ƒç”¨ API ç”Ÿæˆä¸€ä¸ªç®€çŸ­å›å¤æ¥éªŒè¯ã€‚
    
    ä½¿ç”¨æ–¹æ³•ï¼š
    curl http://localhost:18888/v1/config/test/llm
    
    è¿”å›ï¼š
    - success: true/false
    - message: æµ‹è¯•ç»“æœæè¿°
    - model: å½“å‰é…ç½®çš„æ¨¡å‹
    - latency_ms: API è°ƒç”¨å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    """
    # è·å– LLM é…ç½®
    llm_api_key = os.environ.get('LLM_API_KEY', '')
    llm_api_base = os.environ.get('LLM_API_BASE', '')
    llm_model = os.environ.get('LLM_MODEL', 'gpt-3.5-turbo')
    
    # å¦‚æœæ²¡æœ‰ LLM_API_KEYï¼Œå°è¯•ä½¿ç”¨ OPENAI_API_KEY
    if not llm_api_key:
        llm_api_key = os.environ.get('OPENAI_API_KEY', '')
    
    if not llm_api_key:
        return {
            "success": False,
            "message": "LLM API Key æœªé…ç½®",
            "model": llm_model,
            "api_base": llm_api_base or "é»˜è®¤",
            "latency_ms": 0,
            "hint": "è¯·åœ¨ api_keys.env ä¸­è®¾ç½® LLM_API_KEY æˆ– OPENAI_API_KEY"
        }
    
    try:
        from .utils.llm_client import LLMClient
        
        start_time = time.time()
        
        # åˆ›å»º LLM å®¢æˆ·ç«¯
        client = LLMClient(
            model=llm_model,
            api_key=llm_api_key,
            api_base=llm_api_base if llm_api_base else None,
            timeout=15.0,
            max_retries=1
        )
        
        # å‘é€ç®€å•çš„æµ‹è¯•è¯·æ±‚
        response = client.chat(
            messages=[{"role": "user", "content": "Say 'Hello' in one word."}],
            max_tokens=10,
            temperature=0
        )
        
        elapsed_ms = (time.time() - start_time) * 1000
        
        return {
            "success": True,
            "message": f"LLM è¿æ¥æˆåŠŸï¼æ¨¡å‹ {response.model} å·¥ä½œæ­£å¸¸",
            "model": response.model,
            "api_base": llm_api_base or "é»˜è®¤",
            "response": response.content[:50] if response.content else "",
            "latency_ms": round(elapsed_ms, 2),
            "usage": response.usage
        }
        
    except Exception as e:
        error_msg = str(e)
        
        # å‹å¥½çš„é”™è¯¯æç¤º
        if "API key" in error_msg.lower() or "unauthorized" in error_msg.lower() or "401" in error_msg:
            friendly_msg = "API Key æ— æ•ˆæˆ–æœªæˆæƒ"
        elif "connection" in error_msg.lower() or "network" in error_msg.lower():
            friendly_msg = "ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ– API åœ°å€"
        elif "model" in error_msg.lower() or "404" in error_msg:
            friendly_msg = f"æ¨¡å‹ {llm_model} ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨"
        elif "timeout" in error_msg.lower():
            friendly_msg = "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        else:
            friendly_msg = f"è¿æ¥å¤±è´¥: {error_msg}"
        
        return {
            "success": False,
            "message": friendly_msg,
            "error": error_msg,
            "model": llm_model,
            "api_base": llm_api_base or "é»˜è®¤",
            "latency_ms": 0
        }


@app.get("/v1/config/detect-dimension", tags=["Admin"])
async def detect_embedding_dimension(api_key: Optional[str] = None, api_base: Optional[str] = None, model: Optional[str] = None):
    """è‡ªåŠ¨æ£€æµ‹ Embedding æ¨¡å‹çš„å‘é‡ç»´åº¦
    
    è°ƒç”¨ Embedding API ç”Ÿæˆä¸€ä¸ªæµ‹è¯•å‘é‡ï¼Œè¿”å›å…¶å®é™…ç»´åº¦ã€‚
    å¦‚æœæœªæä¾›å‚æ•°ï¼Œåˆ™ä½¿ç”¨å½“å‰é…ç½®çš„ APIã€‚
    
    Args:
        api_key: å¯é€‰ï¼Œä¸´æ—¶ä½¿ç”¨çš„ API Key
        api_base: å¯é€‰ï¼Œä¸´æ—¶ä½¿ç”¨çš„ API Base URL
        model: å¯é€‰ï¼Œä¸´æ—¶ä½¿ç”¨çš„æ¨¡å‹åç§°
        
    Returns:
        dimension: æ£€æµ‹åˆ°çš„å‘é‡ç»´åº¦
        model: ä½¿ç”¨çš„æ¨¡å‹
    """
    # ä¼˜å…ˆä½¿ç”¨å‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®
    embedding_key = api_key or os.environ.get('EMBEDDING_API_KEY', '')
    embedding_base = api_base or os.environ.get('EMBEDDING_API_BASE', '')
    embedding_model = model or os.environ.get('EMBEDDING_MODEL', 'text-embedding-3-small')
    
    if not embedding_key:
        return {
            "success": False,
            "message": "è¯·å…ˆé…ç½® Embedding API Key",
            "dimension": None
        }
    
    try:
        from openai import OpenAI
        
        client_kwargs = {"api_key": embedding_key, "timeout": 30}
        if embedding_base:
            client_kwargs["base_url"] = embedding_base
        
        client = OpenAI(**client_kwargs)
        
        # ç”Ÿæˆæµ‹è¯•å‘é‡
        start_time = time.time()
        response = client.embeddings.create(
            model=embedding_model,
            input="Hello, this is a test for dimension detection."
        )
        elapsed_ms = (time.time() - start_time) * 1000
        
        # è·å–å®é™…ç»´åº¦
        if response.data and len(response.data) > 0:
            actual_dimension = len(response.data[0].embedding)
            return {
                "success": True,
                "message": f"æ£€æµ‹åˆ°å‘é‡ç»´åº¦: {actual_dimension}",
                "dimension": actual_dimension,
                "model": embedding_model,
                "api_base": embedding_base or "é»˜è®¤",
                "latency_ms": round(elapsed_ms, 2)
            }
        else:
            return {
                "success": False,
                "message": "API è¿”å›ç©ºç»“æœ",
                "dimension": None
            }
            
    except Exception as e:
        return {
            "success": False,
            "message": f"æ£€æµ‹å¤±è´¥: {str(e)}",
            "dimension": None
        }


@app.get("/v1/config/models/embedding", tags=["Admin"])
async def get_embedding_models(api_key: Optional[str] = None, api_base: Optional[str] = None):
    """è·å–å¯ç”¨çš„ Embedding æ¨¡å‹åˆ—è¡¨
    
    ä»æŒ‡å®šçš„ API è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ã€‚
    å¦‚æœæœªæä¾›å‚æ•°ï¼Œåˆ™ä½¿ç”¨å½“å‰é…ç½®çš„ APIã€‚
    
    Args:
        api_key: å¯é€‰ï¼Œä¸´æ—¶ä½¿ç”¨çš„ API Key
        api_base: å¯é€‰ï¼Œä¸´æ—¶ä½¿ç”¨çš„ API Base URL
    """
    # ä¼˜å…ˆä½¿ç”¨å‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®
    embedding_key = api_key or os.environ.get('EMBEDDING_API_KEY', '')
    embedding_base = api_base or os.environ.get('EMBEDDING_API_BASE', '')
    
    if not embedding_key:
        return {
            "success": False,
            "message": "è¯·å…ˆå¡«å†™ Embedding API Key",
            "models": []
        }
    
    try:
        from openai import OpenAI
        import httpx
        
        client_kwargs = {"api_key": embedding_key, "timeout": 30}
        if embedding_base:
            client_kwargs["base_url"] = embedding_base
        
        client = OpenAI(**client_kwargs)
        
        try:
            models_response = client.models.list()
        except Exception as list_err:
            # å¦‚æœ /models ç«¯ç‚¹ä¸æ”¯æŒï¼Œå°è¯•ç›´æ¥è¯·æ±‚
            base_url = embedding_base or "https://api.openai.com/v1"
            models_url = f"{base_url.rstrip('/')}/models"
            
            try:
                async with httpx.AsyncClient(timeout=30) as http_client:
                    resp = await http_client.get(
                        models_url,
                        headers={"Authorization": f"Bearer {embedding_key}"}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        models_data = data.get('data', [])
                        if models_data:
                            embedding_models = []
                            for model in models_data:
                                model_id = model.get('id', '').lower()
                                if any(kw in model_id for kw in ['embed', 'bge', 'bce', 'e5', 'minilm', 'nomic']):
                                    embedding_models.append({
                                        "id": model.get('id'),
                                        "owned_by": model.get('owned_by', 'unknown')
                                    })
                            if not embedding_models:
                                embedding_models = [
                                    {"id": m.get('id'), "owned_by": m.get('owned_by', 'unknown')}
                                    for m in models_data
                                ]
                            return {
                                "success": True,
                                "message": f"è·å–åˆ° {len(embedding_models)} ä¸ªæ¨¡å‹",
                                "models": embedding_models,
                                "api_base": embedding_base or "é»˜è®¤"
                            }
                    # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œè¿”å›è¯¦ç»†é”™è¯¯
                    return {
                        "success": False,
                        "message": f"è¯¥ API ä¸æ”¯æŒè·å–æ¨¡å‹åˆ—è¡¨ (HTTP {resp.status_code})ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥æ¨¡å‹åç§°",
                        "models": [],
                        "hint": "é€‰æ‹©'è‡ªå®šä¹‰æ¨¡å‹'å¹¶æ‰‹åŠ¨è¾“å…¥æ¨¡å‹åç§°"
                    }
            except Exception as http_err:
                return {
                    "success": False,
                    "message": f"è¯¥ API ä¸æ”¯æŒ /models ç«¯ç‚¹ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥æ¨¡å‹åç§°",
                    "models": [],
                    "hint": "é€‰æ‹©'è‡ªå®šä¹‰æ¨¡å‹'å¹¶æ‰‹åŠ¨è¾“å…¥æ¨¡å‹åç§°",
                    "error_detail": str(http_err)
                }
        
        # è¿‡æ»¤å‡º embedding ç›¸å…³çš„æ¨¡å‹
        embedding_models = []
        for model in models_response.data:
            model_id = model.id.lower()
            # åŒ¹é… embedding æ¨¡å‹çš„å¸¸è§å‘½å
            if any(kw in model_id for kw in ['embed', 'bge', 'bce', 'e5', 'minilm', 'nomic']):
                embedding_models.append({
                    "id": model.id,
                    "owned_by": getattr(model, 'owned_by', 'unknown')
                })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„ embedding æ¨¡å‹ï¼Œè¿”å›æ‰€æœ‰æ¨¡å‹è®©ç”¨æˆ·é€‰æ‹©
        if not embedding_models:
            embedding_models = [
                {"id": model.id, "owned_by": getattr(model, 'owned_by', 'unknown')}
                for model in models_response.data
            ]
        
        return {
            "success": True,
            "message": f"è·å–åˆ° {len(embedding_models)} ä¸ªæ¨¡å‹",
            "models": embedding_models,
            "api_base": embedding_base or "é»˜è®¤"
        }
        
    except Exception as e:
        import traceback
        return {
            "success": False,
            "message": f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}",
            "models": [],
            "error_detail": traceback.format_exc()
        }


@app.get("/v1/config/models/llm", tags=["Admin"])
async def get_llm_models(api_key: Optional[str] = None, api_base: Optional[str] = None):
    """è·å–å¯ç”¨çš„ LLM æ¨¡å‹åˆ—è¡¨
    
    ä»æŒ‡å®šçš„ API è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ã€‚
    å¦‚æœæœªæä¾›å‚æ•°ï¼Œåˆ™ä½¿ç”¨å½“å‰é…ç½®çš„ APIã€‚
    
    Args:
        api_key: å¯é€‰ï¼Œä¸´æ—¶ä½¿ç”¨çš„ API Key
        api_base: å¯é€‰ï¼Œä¸´æ—¶ä½¿ç”¨çš„ API Base URL
    """
    # ä¼˜å…ˆä½¿ç”¨å‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®
    llm_key = api_key or os.environ.get('LLM_API_KEY', '')
    llm_base = api_base or os.environ.get('LLM_API_BASE', '')
    
    if not llm_key:
        return {
            "success": False,
            "message": "è¯·å…ˆå¡«å†™ LLM API Key",
            "models": []
        }
    
    try:
        from openai import OpenAI
        import httpx
        
        client_kwargs = {"api_key": llm_key, "timeout": 30}
        if llm_base:
            client_kwargs["base_url"] = llm_base
        
        client = OpenAI(**client_kwargs)
        
        try:
            models_response = client.models.list()
        except Exception as list_err:
            # å¦‚æœ /models ç«¯ç‚¹ä¸æ”¯æŒï¼Œå°è¯•ç›´æ¥è¯·æ±‚
            base_url = llm_base or "https://api.openai.com/v1"
            models_url = f"{base_url.rstrip('/')}/models"
            
            try:
                async with httpx.AsyncClient(timeout=30) as http_client:
                    resp = await http_client.get(
                        models_url,
                        headers={"Authorization": f"Bearer {llm_key}"}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                        models_data = data.get('data', [])
                        if models_data:
                            llm_models = []
                            for model in models_data:
                                model_id = model.get('id', '').lower()
                                # æ’é™¤ embedding æ¨¡å‹
                                if not any(kw in model_id for kw in ['embed', 'bge', 'bce', 'e5-', 'minilm']):
                                    llm_models.append({
                                        "id": model.get('id'),
                                        "owned_by": model.get('owned_by', 'unknown')
                                    })
                            return {
                                "success": True,
                                "message": f"è·å–åˆ° {len(llm_models)} ä¸ªæ¨¡å‹",
                                "models": llm_models,
                                "api_base": llm_base or "é»˜è®¤"
                            }
                    # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œè¿”å›è¯¦ç»†é”™è¯¯
                    return {
                        "success": False,
                        "message": f"è¯¥ API ä¸æ”¯æŒè·å–æ¨¡å‹åˆ—è¡¨ (HTTP {resp.status_code})ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥æ¨¡å‹åç§°",
                        "models": [],
                        "hint": "é€‰æ‹©'è‡ªå®šä¹‰æ¨¡å‹'å¹¶æ‰‹åŠ¨è¾“å…¥æ¨¡å‹åç§°"
                    }
            except Exception as http_err:
                return {
                    "success": False,
                    "message": f"è¯¥ API ä¸æ”¯æŒ /models ç«¯ç‚¹ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥æ¨¡å‹åç§°",
                    "models": [],
                    "hint": "é€‰æ‹©'è‡ªå®šä¹‰æ¨¡å‹'å¹¶æ‰‹åŠ¨è¾“å…¥æ¨¡å‹åç§°",
                    "error_detail": str(http_err)
                }
        
        # è¿‡æ»¤å‡º LLM/Chat ç›¸å…³çš„æ¨¡å‹ï¼ˆæ’é™¤ embedding æ¨¡å‹ï¼‰
        llm_models = []
        for model in models_response.data:
            model_id = model.id.lower()
            # æ’é™¤ embedding æ¨¡å‹
            if not any(kw in model_id for kw in ['embed', 'bge', 'bce', 'e5-', 'minilm']):
                llm_models.append({
                    "id": model.id,
                    "owned_by": getattr(model, 'owned_by', 'unknown')
                })
        
        return {
            "success": True,
            "message": f"è·å–åˆ° {len(llm_models)} ä¸ªæ¨¡å‹",
            "models": llm_models,
            "api_base": llm_base or "é»˜è®¤"
        }
        
    except Exception as e:
        import traceback
        return {
            "success": False,
            "message": f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}",
            "models": [],
            "error_detail": traceback.format_exc()
        }


class ConfigUpdateRequest(BaseModel):
    """é…ç½®æ›´æ–°è¯·æ±‚ï¼ˆç»Ÿä¸€ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼ï¼‰"""
    # Embedding é…ç½®
    embedding_api_key: Optional[str] = Field(default=None, description="Embedding API Key")
    embedding_api_base: Optional[str] = Field(default=None, description="Embedding API åœ°å€")
    embedding_model: Optional[str] = Field(default=None, description="Embedding æ¨¡å‹")
    embedding_dimension: Optional[int] = Field(default=None, description="å‘é‡ç»´åº¦")
    recall_embedding_mode: Optional[str] = Field(default=None, description="Embedding æ¨¡å¼")
    embedding_rate_limit: Optional[int] = Field(default=None, description="API é€Ÿç‡é™åˆ¶ï¼ˆæ¯æ—¶é—´çª—å£æœ€å¤§è¯·æ±‚æ•°ï¼‰")
    embedding_rate_window: Optional[int] = Field(default=None, description="é€Ÿç‡é™åˆ¶æ—¶é—´çª—å£ï¼ˆç§’ï¼‰")
    # LLM é…ç½®
    llm_api_key: Optional[str] = Field(default=None, description="LLM API Key")
    llm_api_base: Optional[str] = Field(default=None, description="LLM API åœ°å€")
    llm_model: Optional[str] = Field(default=None, description="LLM æ¨¡å‹")
    # æŒä¹…æ¡ä»¶å®¹é‡é…ç½®
    context_trigger_interval: Optional[int] = Field(default=None, description="æ¡ä»¶æå–è§¦å‘é—´éš”ï¼ˆæ¯Nè½®ï¼‰")
    context_max_per_type: Optional[int] = Field(default=None, description="æ¯ç±»å‹æ¡ä»¶ä¸Šé™")
    context_max_total: Optional[int] = Field(default=None, description="æ¡ä»¶æ€»æ•°ä¸Šé™")
    context_decay_days: Optional[int] = Field(default=None, description="è¡°å‡å¼€å§‹å¤©æ•°")
    context_decay_rate: Optional[float] = Field(default=None, description="æ¯æ¬¡è¡°å‡æ¯”ä¾‹ (0-1)")
    context_min_confidence: Optional[float] = Field(default=None, description="æœ€ä½ç½®ä¿¡åº¦ (0-1)")
    # ä¼ç¬”ç³»ç»Ÿå®¹é‡é…ç½®
    foreshadowing_max_return: Optional[int] = Field(default=None, description="ä¼ç¬”å¬å›æ•°é‡")
    foreshadowing_max_active: Optional[int] = Field(default=None, description="æ´»è·ƒä¼ç¬”æ•°é‡ä¸Šé™")
    # æ™ºèƒ½å»é‡é…ç½®
    dedup_embedding_enabled: Optional[bool] = Field(default=None, description="å¯ç”¨è¯­ä¹‰å»é‡")
    dedup_high_threshold: Optional[float] = Field(default=None, description="é«˜ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)")
    dedup_low_threshold: Optional[float] = Field(default=None, description="ä½ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)")
    # ä¸Šä¸‹æ–‡æ„å»ºé…ç½®ï¼ˆ100%ä¸é—å¿˜ä¿è¯ï¼‰
    context_max_context_turns: Optional[int] = Field(default=None, description="å¯¹è¯æå–æœ€å¤§è½®æ¬¡")
    build_context_include_recent: Optional[int] = Field(default=None, description="build_contexté»˜è®¤æœ€è¿‘å¯¹è¯è½®æ¬¡")
    proactive_reminder_enabled: Optional[bool] = Field(default=None, description="å¯ç”¨ä¸»åŠ¨æé†’")
    proactive_reminder_turns: Optional[int] = Field(default=None, description="ä¸»åŠ¨æé†’è§¦å‘è½®æ¬¡")


@app.put("/v1/config", tags=["Admin"])
async def update_config(request: ConfigUpdateRequest):
    """æ›´æ–°é…ç½®æ–‡ä»¶
    
    æ›´æ–° api_keys.env ä¸­çš„é…ç½®é¡¹ã€‚åªä¼šæ›´æ–°è¯·æ±‚ä¸­åŒ…å«çš„éç©ºå­—æ®µã€‚
    æ›´æ–°åä¼šè‡ªåŠ¨é‡æ–°åŠ è½½é…ç½®ã€‚
    
    ä½¿ç”¨æ–¹æ³•ï¼š
    curl -X PUT http://localhost:18888/v1/config \\
         -H "Content-Type: application/json" \\
         -d '{"embedding_api_key": "your-api-key", "llm_api_key": "your-llm-key"}'
    """
    config_file = get_config_file_path()
    
    # è¯»å–å½“å‰é…ç½®
    current_config = {}
    if config_file.exists():
        with open(config_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, _, value = line.partition('=')
                    current_config[key.strip()] = value.strip()
    
    # æ˜ å°„è¯·æ±‚å­—æ®µåˆ°é…ç½®é”®ï¼ˆç»Ÿä¸€ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼ï¼‰
    field_to_key = {
        'embedding_api_key': 'EMBEDDING_API_KEY',
        'embedding_api_base': 'EMBEDDING_API_BASE',
        'embedding_model': 'EMBEDDING_MODEL',
        'embedding_dimension': 'EMBEDDING_DIMENSION',
        'recall_embedding_mode': 'RECALL_EMBEDDING_MODE',
        'embedding_rate_limit': 'EMBEDDING_RATE_LIMIT',
        'embedding_rate_window': 'EMBEDDING_RATE_WINDOW',
        'llm_api_key': 'LLM_API_KEY',
        'llm_api_base': 'LLM_API_BASE',
        'llm_model': 'LLM_MODEL',
        # æŒä¹…æ¡ä»¶å®¹é‡é…ç½®
        'context_trigger_interval': 'CONTEXT_TRIGGER_INTERVAL',
        'context_max_per_type': 'CONTEXT_MAX_PER_TYPE',
        'context_max_total': 'CONTEXT_MAX_TOTAL',
        'context_decay_days': 'CONTEXT_DECAY_DAYS',
        'context_decay_rate': 'CONTEXT_DECAY_RATE',
        'context_min_confidence': 'CONTEXT_MIN_CONFIDENCE',
        # ä¼ç¬”ç³»ç»Ÿå®¹é‡é…ç½®
        'foreshadowing_max_return': 'FORESHADOWING_MAX_RETURN',
        'foreshadowing_max_active': 'FORESHADOWING_MAX_ACTIVE',
        # æ™ºèƒ½å»é‡é…ç½®
        'dedup_embedding_enabled': 'DEDUP_EMBEDDING_ENABLED',
        'dedup_high_threshold': 'DEDUP_HIGH_THRESHOLD',
        'dedup_low_threshold': 'DEDUP_LOW_THRESHOLD',
        # ä¸Šä¸‹æ–‡æ„å»ºé…ç½®ï¼ˆ100%ä¸é—å¿˜ä¿è¯ï¼‰
        'context_max_context_turns': 'CONTEXT_MAX_CONTEXT_TURNS',
        'build_context_include_recent': 'BUILD_CONTEXT_INCLUDE_RECENT',
        'proactive_reminder_enabled': 'PROACTIVE_REMINDER_ENABLED',
        'proactive_reminder_turns': 'PROACTIVE_REMINDER_TURNS',
    }
    
    # æ›´æ–°é…ç½®
    updated_fields = []
    request_dict = request.model_dump(exclude_none=True)
    
    for field, config_key in field_to_key.items():
        if field in request_dict:
            value = request_dict[field]
            if value is not None:
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                str_value = str(value) if not isinstance(value, str) else value
                current_config[config_key] = str_value
                # åŒæ—¶æ›´æ–°ç¯å¢ƒå˜é‡
                os.environ[config_key] = str_value
                updated_fields.append(config_key)
    
    if not updated_fields:
        return {
            "success": False,
            "message": "æ²¡æœ‰æä¾›éœ€è¦æ›´æ–°çš„é…ç½®é¡¹"
        }
    
    # å†™å›é…ç½®æ–‡ä»¶ï¼ˆä¿ç•™æ³¨é‡Šå’Œæ ¼å¼ï¼‰
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        config_file.parent.mkdir(parents=True, exist_ok=True)
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå…ˆåˆ›å»ºåŒ…å«å®Œæ•´æ¨¡æ¿çš„æ–‡ä»¶
        if not config_file.exists():
            config_file.write_text(get_default_config_content(), encoding='utf-8')
            print(f"[Config] å·²åˆ›å»ºé…ç½®æ–‡ä»¶: {config_file}")
        
        # è¯»å–åŸæ–‡ä»¶ä¿ç•™æ³¨é‡Š
        lines = []
        existing_keys = set()
        
        with open(config_file, 'r', encoding='utf-8') as f:
            for line in f:
                original_line = line.rstrip('\n\r')
                stripped = original_line.strip()
                
                if stripped and not stripped.startswith('#') and '=' in stripped:
                    key = stripped.split('=')[0].strip()
                    if key in current_config:
                        # æ›´æ–°è¿™è¡Œçš„å€¼
                        lines.append(f"{key}={current_config[key]}")
                        existing_keys.add(key)
                    else:
                        lines.append(original_line)
                else:
                    lines.append(original_line)
        
        # æ·»åŠ æ–°çš„é…ç½®é¡¹
        for key, value in current_config.items():
            if key not in existing_keys and key in SUPPORTED_CONFIG_KEYS:
                lines.append(f"{key}={value}")
        
        # å†™å…¥æ–‡ä»¶
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
            if not lines[-1].endswith('\n'):
                f.write('\n')
        
        # é‡æ–°åŠ è½½å¼•æ“é…ç½®
        try:
            reload_engine()
        except Exception as reload_err:
            # é…ç½®å·²ä¿å­˜ï¼Œä½†é‡æ–°åŠ è½½å¤±è´¥
            return {
                "success": True,
                "message": f"é…ç½®å·²ä¿å­˜ï¼Œä½†é‡æ–°åŠ è½½å¤±è´¥: {str(reload_err)}",
                "updated_fields": updated_fields,
                "hint": "è¯·æ‰‹åŠ¨é‡å¯æœåŠ¡æˆ–è°ƒç”¨ POST /v1/config/reload"
            }
        
        return {
            "success": True,
            "message": "é…ç½®å·²æ›´æ–°å¹¶é‡æ–°åŠ è½½",
            "updated_fields": updated_fields
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¿å­˜é…ç½®å¤±è´¥: {str(e)}")


@app.get("/v1/config/full", tags=["Admin"])
async def get_full_config():
    """è·å–å®Œæ•´é…ç½®ä¿¡æ¯
    
    è¿”å› Embedding å’Œ LLM çš„å®Œæ•´é…ç½®çŠ¶æ€ï¼ŒåŒ…æ‹¬è„±æ•åçš„ API Keyã€‚
    ä¾›ä»»ä½•å®¢æˆ·ç«¯ï¼ˆæ’ä»¶ã€Web UIã€CLI ç­‰ï¼‰ä½¿ç”¨ã€‚
    """
    def mask_key(key: str) -> str:
        """è„±æ•æ˜¾ç¤º API Key"""
        if not key:
            return ""
        if len(key) > 12:
            return key[:4] + '*' * 8 + key[-4:]
        elif len(key) > 4:
            return key[:2] + '*' * (len(key) - 2)
        return '****'
    
    def get_key_status(key: str) -> str:
        """è·å– API Key çŠ¶æ€"""
        if not key:
            return "æœªé…ç½®"
        return "å·²é…ç½®"
    
    # Embedding é…ç½®ï¼ˆç»Ÿä¸€ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼ï¼‰
    embedding_key = os.environ.get('EMBEDDING_API_KEY', '')
    
    embedding_config = {
        "api_key": mask_key(embedding_key),
        "api_key_status": get_key_status(embedding_key),
        "api_base": os.environ.get('EMBEDDING_API_BASE', ''),
        "model": os.environ.get('EMBEDDING_MODEL', ''),
        "dimension": os.environ.get('EMBEDDING_DIMENSION', ''),
        "mode": os.environ.get('RECALL_EMBEDDING_MODE', ''),
        "rate_limit": os.environ.get('EMBEDDING_RATE_LIMIT', ''),
        "rate_window": os.environ.get('EMBEDDING_RATE_WINDOW', ''),
    }
    
    # LLM é…ç½®
    llm_key = os.environ.get('LLM_API_KEY', '')
    llm_config = {
        "api_key": mask_key(llm_key),
        "api_key_status": get_key_status(llm_key),
        "api_base": os.environ.get('LLM_API_BASE', ''),
        "model": os.environ.get('LLM_MODEL', 'gpt-3.5-turbo'),
    }
    
    return {
        "embedding": embedding_config,
        "llm": llm_config,
        "config_file": str(get_config_file_path()),
    }


@app.post("/v1/consolidate", tags=["Admin"])
async def consolidate(user_id: str = Query(default="default")):
    """æ‰§è¡Œè®°å¿†æ•´åˆ"""
    engine = get_engine()
    engine.consolidate(user_id=user_id)
    return {"success": True, "message": "æ•´åˆå®Œæˆ"}


@app.post("/v1/reset", tags=["Admin"])
async def reset(
    user_id: Optional[str] = Query(default=None),
    confirm: bool = Query(default=False)
):
    """é‡ç½®è®°å¿†ï¼ˆå±é™©æ“ä½œï¼‰"""
    if not confirm:
        raise HTTPException(status_code=400, detail="éœ€è¦ confirm=true ç¡®è®¤")
    
    engine = get_engine()
    engine.reset(user_id=user_id)
    return {"success": True, "message": "é‡ç½®å®Œæˆ"}


# ==================== mem0 å…¼å®¹ API ====================
# æä¾›ä¸ mem0 API æ ¼å¼å…¼å®¹çš„æ¥å£

@app.post("/v1/memory/", tags=["mem0 Compatible"])
async def mem0_add(
    messages: List[Dict[str, str]] = Body(...),
    user_id: str = Body(default="default"),
    metadata: Optional[Dict[str, Any]] = Body(default=None)
):
    """mem0 å…¼å®¹ - æ·»åŠ è®°å¿†"""
    engine = get_engine()
    
    results = []
    for msg in messages:
        content = msg.get('content', '')
        if content:
            result = engine.add(content, user_id=user_id, metadata=metadata)
            results.append({"id": result.id, "success": result.success})
    
    return {"results": results}


@app.get("/v1/memory/", tags=["mem0 Compatible"])
async def mem0_get_all(
    user_id: str = Query(default="default"),
    limit: int = Query(default=100)
):
    """mem0 å…¼å®¹ - è·å–æ‰€æœ‰è®°å¿†"""
    engine = get_engine()
    memories = engine.get_all(user_id=user_id, limit=limit)
    
    # è½¬æ¢ä¸º mem0 æ ¼å¼
    return {
        "memories": [
            {
                "id": m.get('id'),
                "memory": m.get('content', m.get('memory', '')),
                "user_id": user_id,
                "metadata": m.get('metadata', {}),
                "created_at": m.get('created_at')
            }
            for m in memories
        ]
    }


@app.post("/v1/memory/search/", tags=["mem0 Compatible"])
async def mem0_search(
    query: str = Body(...),
    user_id: str = Body(default="default"),
    limit: int = Body(default=10)
):
    """mem0 å…¼å®¹ - æœç´¢è®°å¿†"""
    engine = get_engine()
    results = engine.search(query, user_id=user_id, top_k=limit)
    
    return {
        "memories": [
            {
                "id": r.id,
                "memory": r.content,
                "score": r.score,
                "user_id": user_id,
                "metadata": r.metadata
            }
            for r in results
        ]
    }


@app.delete("/v1/memory/{memory_id}/", tags=["mem0 Compatible"])
async def mem0_delete(memory_id: str, user_id: str = Query(default="default")):
    """mem0 å…¼å®¹ - åˆ é™¤è®°å¿†"""
    engine = get_engine()
    success = engine.delete(memory_id, user_id=user_id)
    return {"success": success}

# Recall v3 - å®Œæ•´å®æ–½æ–¹æ¡ˆ

> **æœ¬æ–‡æ¡£ç›®æ ‡**ï¼šä»»ä½•AIéƒ½èƒ½100%æŒ‰ç…§æ­¤æ–¹æ¡ˆå®ç°æ‰€æœ‰åŠŸèƒ½ï¼Œä¸é—æ¼ä»»ä½•ç»†èŠ‚ã€‚
> 
> **æ ¸å¿ƒæ¶æ„**ï¼š**çº¯æœ¬åœ°æ’ä»¶** + **ç”¨æˆ·è‡ªå·±çš„ AI API key**
> - æ‰€æœ‰æ•°æ®å­˜å‚¨åœ¨ç”¨æˆ·æœ¬åœ°ï¼Œä¸ä¾èµ–ä»»ä½•äº‘ç«¯æœåŠ¡
> - ç”¨æˆ·ä½¿ç”¨è‡ªå·±çš„ OpenAI/Claude/å…¶ä»– API key è°ƒç”¨å¤§æ¨¡å‹
> - Recall åªè´Ÿè´£è®°å¿†å­˜å‚¨å’Œæ£€ç´¢ï¼Œä¸æ¶‰åŠä»»ä½•è¿œç¨‹æœåŠ¡
> 
> âš ï¸ **è¯šå®å£°æ˜**ï¼š
> - é¦–æ¬¡å¯åŠ¨éœ€è¦ä¸‹è½½ spaCy æ¨¡å‹ï¼ˆ~50MBï¼‰å’Œ sentence-transformers æ¨¡å‹ï¼ˆ~400MBï¼‰ï¼Œå†·å¯åŠ¨çº¦éœ€ 10-15ç§’
> - **æ ‡å‡†æ¨¡å¼**ï¼šè¿è¡Œæ—¶å†…å­˜çº¦ 500-600MBï¼ˆæ¨è 2GB+ å¯ç”¨å†…å­˜ï¼‰
> - **è½»é‡æ¨¡å¼**ï¼šè¿è¡Œæ—¶å†…å­˜çº¦ 80-100MBï¼ˆé€‚åˆä½é…ç”µè„‘ï¼Œç¦ç”¨å‘é‡æ£€ç´¢ï¼‰
> - æ€§èƒ½æ•°å€¼ä¸ºåˆç†ä¼°ç®—ï¼Œå®é™…éœ€è¦åŸºå‡†æµ‹è¯•éªŒè¯
>
> **ğŸ’» ç³»ç»Ÿè¦æ±‚ï¼ˆå¾ˆä½ï¼‰**ï¼š
> | é…ç½® | æ ‡å‡†æ¨¡å¼ | è½»é‡æ¨¡å¼ |
> |------|---------|----------|
> | å†…å­˜ | 2GB+ å¯ç”¨ | 512MB+ å¯ç”¨ |
> | ç£ç›˜ | 1GB ç©ºé—² | 100MB ç©ºé—² |
> | CPU | ä»»æ„ | ä»»æ„ |
> | GPU | **ä¸éœ€è¦** | **ä¸éœ€è¦** |
> | ç³»ç»Ÿ | Win/Mac/Linux | Win/Mac/Linux |
> 
> **å·²å®ç°çš„åŠŸèƒ½**ï¼š
> - âœ… å®Œæ•´çš„æœ¬åœ°å­˜å‚¨å±‚ï¼ˆ4å±‚æ¶æ„ï¼‰
> - âœ… 8å±‚æ£€ç´¢ç³»ç»Ÿï¼ˆ100%ä¸é—å¿˜ï¼‰
> - âœ… ä¼ç¬”è¿½è¸ªç³»ç»Ÿï¼ˆæ‰‹åŠ¨ç®¡ç†ï¼‰
> - âœ… ä¼ç¬”è‡ªåŠ¨æ£€æµ‹ï¼ˆLLM è¾…åŠ©åˆ†æï¼‰
> - âœ… çŸ¥è¯†å›¾è°±ï¼ˆè½»é‡æœ¬åœ°ç‰ˆï¼Œæ— éœ€Neo4jï¼‰
> - âœ… è®°å¿†æ™ºèƒ½æ€»ç»“ï¼ˆå¯¹æ ‡ mem0ï¼‰
> - âœ… å¤šç”¨æˆ·/å¤šè§’è‰²æ”¯æŒ
> - âœ… SillyTavern æ’ä»¶
> - âœ… å‘½ä»¤è¡Œå·¥å…·
> - âœ… HTTP API æ¥å£
> - âœ… mem0 å…¼å®¹å±‚ï¼ˆæ— ç¼è¿ç§»ï¼‰
> - âŒ CodeIndexerï¼ˆä»£ç ç´¢å¼•ï¼Œå¯é€‰ï¼Œv3.1ï¼‰

## ã€‡ã€3æ­¥å®‰è£…ï¼Œå¼€ç®±å³ç”¨

> **ğŸ¯ è®¾è®¡åŸåˆ™ï¼šå³æ’å³ç”¨ã€æ— ç—•å¸è½½**
> - æ‰€æœ‰æ•°æ®å­˜å‚¨åœ¨**é¡¹ç›®ç›®å½•**ä¸­ï¼ˆ`./recall_data/`ï¼‰
> - å¸è½½åªéœ€**åˆ é™¤æ•´ä¸ªé¡¹ç›®æ–‡ä»¶å¤¹**ï¼Œæ— éœ€ä»»ä½•é¢å¤–æ“ä½œ
> - ä¸åœ¨ç”¨æˆ·ç›®å½•ã€ç³»ç»Ÿç›®å½•æˆ–ä»»ä½•å¤–éƒ¨ä½ç½®åˆ›å»ºæ–‡ä»¶
> - ä¸ä¿®æ”¹ç³»ç»Ÿç¯å¢ƒå˜é‡ã€æ³¨å†Œè¡¨æˆ–ç³»ç»ŸæœåŠ¡
> - ä¸ä¿®æ”¹å…¶ä»–åº”ç”¨çš„é…ç½®æ–‡ä»¶

### æ–¹å¼ä¸€ï¼šSillyTavern ç”¨æˆ·ï¼ˆå°ç™½æ¨èï¼‰
```
1. æ‰“å¼€ SillyTavern â†’ æ‰©å±• â†’ æœç´¢ "Recall"
2. ç‚¹å‡»å®‰è£…
3. å®Œæˆï¼ï¼ˆæœ¬åœ°æ¨¡å¼ï¼Œéœ€è¦API keyï¼‰

æ³¨æ„ï¼šSTæ’ä»¶è¿æ¥æœ¬åœ°Pythonåç«¯ï¼Œéœ€è¦å…ˆå®‰è£…Pythonç‰ˆæœ¬
```

### æ–¹å¼äºŒï¼šå‘½ä»¤è¡Œå®‰è£…ï¼ˆæ¨èä½¿ç”¨è™šæ‹Ÿç¯å¢ƒï¼‰
```bash
# æ¨èï¼šä½¿ç”¨è™šæ‹Ÿç¯å¢ƒéš”ç¦»ï¼Œä¸æ±¡æŸ“å…¨å±€ Python ç¯å¢ƒ
python -m venv recall-env
# Windows:
recall-env\Scripts\activate
# Linux/Mac:
source recall-env/bin/activate

pip install recall-ai
recall init          # è¾“å…¥ä½ çš„ API key
recall chat          # å¼€å§‹ä½¿ç”¨
```

> ğŸ’¡ é¦–æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ä¸‹è½½ NLP æ¨¡å‹ï¼ˆçº¦500MBï¼‰ï¼Œä¹‹åä¸å†éœ€è¦ã€‚
> æ¨¡å‹å­˜å‚¨åœ¨é¡¹ç›®ç›®å½•çš„ `./recall_data/models/` ä¸­ï¼Œåˆ é™¤é¡¹ç›®æ–‡ä»¶å¤¹å³å¯å®Œå…¨å¸è½½ã€‚

### æ–¹å¼ä¸‰ï¼šç›´æ¥ä½¿ç”¨
```bash
pip install recall-ai
# Windows:
set OPENAI_API_KEY=sk-xxx
# Linux/Mac:
export OPENAI_API_KEY=sk-xxx
recall init --mode local
```

### æ–¹å¼å››ï¼šè½»é‡æ¨¡å¼ï¼ˆä½é…ç”µè„‘/å†…å­˜ä¸è¶³ï¼‰
```bash
pip install recall-ai
recall init --lightweight   # è½»é‡æ¨¡å¼ï¼Œå†…å­˜å ç”¨ä»… ~80MB
recall chat
```

> ğŸ’¡ è½»é‡æ¨¡å¼ç¦ç”¨å‘é‡è¯­ä¹‰æ£€ç´¢ï¼Œä½†å…³é”®è¯åŒ¹é…ã€ä¼ç¬”è¿½è¸ªã€è§„èŒƒæ£€æŸ¥ç­‰æ ¸å¿ƒåŠŸèƒ½å®Œå…¨ä¿ç•™ã€‚
> å¯¹äº 90% çš„ä½¿ç”¨åœºæ™¯ï¼Œè½»é‡æ¨¡å¼å·²ç»è¶³å¤Ÿã€‚

### ğŸ—‘ï¸ å®Œæ•´å¸è½½æ–¹æ³•
```bash
# æ–¹æ³•ä¸€ï¼šç›´æ¥åˆ é™¤é¡¹ç›®æ–‡ä»¶å¤¹ï¼ˆæ¨èï¼Œæœ€ç®€å•ï¼‰
# åˆ é™¤æ•´ä¸ªé¡¹ç›®ç›®å½•å³å¯ï¼Œæ‰€æœ‰æ•°æ®ã€æ¨¡å‹ã€é…ç½®éƒ½åœ¨é‡Œé¢

# æ–¹æ³•äºŒï¼šå¦‚æœæ˜¯ pip å…¨å±€å®‰è£…
pip uninstall recall-ai
# ç„¶ååˆ é™¤ä½ å­˜æ”¾æ•°æ®çš„å·¥ä½œç›®å½•ä¸­çš„ recall_data/ æ–‡ä»¶å¤¹

# æ–¹æ³•ä¸‰ï¼šå¦‚æœä½¿ç”¨äº†è™šæ‹Ÿç¯å¢ƒï¼Œç›´æ¥åˆ é™¤è™šæ‹Ÿç¯å¢ƒç›®å½•å³å¯
```

> âœ… **å¸è½½ä¿è¯**ï¼šåˆ é™¤é¡¹ç›®æ–‡ä»¶å¤¹åï¼Œç³»ç»Ÿå®Œå…¨æ¢å¤åŸçŠ¶ï¼Œä¸ç•™ä»»ä½•ç—•è¿¹ã€‚
> æ‰€æœ‰æ•°æ®ã€æ¨¡å‹ã€é…ç½®å‡åœ¨é¡¹ç›®ç›®å½•çš„ `recall_data/` å†…ï¼Œä¸ä¼šåœ¨ç”¨æˆ·ç›®å½•æˆ–ç³»ç»Ÿç›®å½•ç•™ä¸‹ä»»ä½•æ–‡ä»¶ã€‚

---

## ã€‡ç‚¹äº”ã€æŠ€æœ¯é€‰å‹ä¸ä¾èµ–ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

### æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼ˆå³æ’å³ç”¨/ç¯å¢ƒéš”ç¦»ï¼‰

> **ğŸ¯ ç¯å¢ƒéš”ç¦»è¦æ±‚**ï¼š
> 1. **å•ä¸€æ•°æ®ç›®å½•**ï¼šæ‰€æœ‰æ•°æ®ã€ç¼“å­˜ã€æ¨¡å‹å‡å­˜å‚¨åœ¨é¡¹ç›®ç›®å½•çš„ `./recall_data/` ä¸‹
> 2. **æ— ç³»ç»Ÿçº§ä¿®æ”¹**ï¼šä¸ä¿®æ”¹æ³¨å†Œè¡¨ã€ä¸å®‰è£…ç³»ç»ŸæœåŠ¡ã€ä¸ä¿®æ”¹ PATH
> 3. **ä¾èµ–è‡ªåŒ…å«**ï¼šNLP æ¨¡å‹ä¸‹è½½åˆ°é¡¹ç›®ç›®å½•ï¼Œä¸æ±¡æŸ“å…¨å±€ç¼“å­˜
> 4. **é…ç½®æ–‡ä»¶éš”ç¦»**ï¼šä¸ä¿®æ”¹å…¶ä»–åº”ç”¨ï¼ˆå¦‚ SillyTavernï¼‰çš„åŸæœ‰é…ç½®
> 5. **ä¼˜é›…é™çº§**ï¼šä¾èµ–ä¸å¯ç”¨æ—¶æä¾›æ¸…æ™°é”™è¯¯ä¿¡æ¯ï¼Œä¸å´©æºƒ

### æ•°æ®ç›®å½•ç»“æ„ï¼ˆå…¨éƒ¨æ•°æ®åœ¨é¡¹ç›®ç›®å½•å†…ï¼‰

```
ä½ çš„é¡¹ç›®ç›®å½•/                         # é¡¹ç›®æ ¹ç›®å½•ï¼ˆåˆ é™¤æ­¤ç›®å½•å³å®Œå…¨å¸è½½ï¼‰
â”œâ”€â”€ recall_data/                     # Recall æ•°æ®æ ¹ç›®å½•
â”‚   â”œâ”€â”€ config.json                  # ç”¨æˆ·é…ç½®ï¼ˆAPI keyç­‰ï¼‰
â”‚   â”œâ”€â”€ data/                        # è®°å¿†æ•°æ®
â”‚   â”‚   â””â”€â”€ {user_id}/{character_id}/ # æŒ‰ç”¨æˆ·/è§’è‰²éš”ç¦»
â”‚   â”‚       â”œâ”€â”€ manifest.json
â”‚   â”‚       â”œâ”€â”€ L0_core/
â”‚   â”‚       â”œâ”€â”€ L1_consolidated/
â”‚   â”‚       â”œâ”€â”€ L2_working/
â”‚   â”‚       â”œâ”€â”€ L3_archive/
â”‚   â”‚       â””â”€â”€ indexes/
â”‚   â”œâ”€â”€ models/                      # NLP æ¨¡å‹ç¼“å­˜ï¼ˆå®Œå…¨éš”ç¦»ï¼‰
â”‚   â”‚   â”œâ”€â”€ sentence-transformers/   # Embedding æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ spacy/                   # spaCy æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ huggingface/             # HuggingFace ç¼“å­˜
â”‚   â”‚   â””â”€â”€ torch/                   # PyTorch ç¼“å­˜
â”‚   â”œâ”€â”€ cache/                       # ä¸´æ—¶ç¼“å­˜
â”‚   â””â”€â”€ logs/                        # æ—¥å¿—æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
â”œâ”€â”€ venv/                            # è™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ï¼‰
â””â”€â”€ ...                              # å…¶ä»–é¡¹ç›®æ–‡ä»¶
```

> âš ï¸ **é‡è¦**ï¼šæ‰€æœ‰æ•°æ®éƒ½åœ¨ `recall_data/` ç›®å½•å†…ï¼Œä¸ä¼šåœ¨ç”¨æˆ·ç›®å½•ï¼ˆ~ï¼‰ã€
> ç³»ç»Ÿç›®å½•æˆ–ä»»ä½•å¤–éƒ¨ä½ç½®åˆ›å»ºä»»ä½•æ–‡ä»¶ã€‚åˆ é™¤é¡¹ç›®æ–‡ä»¶å¤¹ = å®Œå…¨å¸è½½ã€‚

### æ ¸å¿ƒä¾èµ–æ¸…å•

```toml
# pyproject.toml
[project]
name = "recall-ai"
version = "3.0.0"
requires-python = ">=3.10"

dependencies = [
    # æ ¸å¿ƒæ¡†æ¶
    "pydantic>=2.0",           # æ•°æ®æ¨¡å‹éªŒè¯
    "sqlalchemy>=2.0",         # æ•°æ®åº“ORMï¼ˆå¯é€‰ï¼‰
    
    # NLPå¤„ç†
    "spacy>=3.5",              # å®ä½“è¯†åˆ«
    "jieba>=0.42",             # ä¸­æ–‡åˆ†è¯
    
    # å‘é‡æ£€ç´¢ï¼ˆæ ‡å‡†æ¨¡å¼éœ€è¦ï¼‰
    "sentence-transformers>=2.2",  # Embeddingæ¨¡å‹ï¼ˆä¼šè‡ªåŠ¨å®‰è£…torchï¼‰
    "faiss-cpu>=1.7",          # å‘é‡ç´¢å¼•
    
    # LLMè°ƒç”¨
    "litellm>=1.0",            # ç»Ÿä¸€LLMæ¥å£
    "openai>=1.0",             # OpenAI SDK
    "httpx>=0.24",             # å¼‚æ­¥HTTP
    
    # WebæœåŠ¡
    "fastapi>=0.100",          # HTTP APIæ¡†æ¶
    "uvicorn>=0.22",           # ASGIæœåŠ¡å™¨
    
    # å·¥å…·åº“
    "click>=8.0",              # CLIæ¡†æ¶
    "rich>=13.0",              # ç»ˆç«¯ç¾åŒ–
    "numpy>=1.24",             # æ•°å€¼è®¡ç®—
    "pybloom-live>=4.0",       # å¸ƒéš†è¿‡æ»¤å™¨
]

[project.optional-dependencies]
lightweight = []               # è½»é‡æ¨¡å¼æ— é¢å¤–ä¾èµ–
dev = ["pytest>=7.0", "black", "ruff"]

[project.scripts]
recall = "recall.cli:main"
```

### é¡¹ç›®ç›®å½•ç»“æ„ï¼ˆå¿…é¡»æŒ‰æ­¤ç»“æ„åˆ›å»ºï¼‰

```
recall/
â”œâ”€â”€ recall/                          # æ ¸å¿ƒåŒ…
â”‚   â”œâ”€â”€ __init__.py                  # ç‰ˆæœ¬ä¿¡æ¯ã€ä¸»å…¥å£
â”‚   â”œâ”€â”€ engine.py                    # RecallEngine ä¸»ç±»
â”‚   â”œâ”€â”€ config.py                    # é…ç½®ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/                     # å­˜å‚¨å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                  # å­˜å‚¨åŸºç±»
â”‚   â”‚   â”œâ”€â”€ layer0_core.py           # L0 æ ¸å¿ƒè®¾å®š
â”‚   â”‚   â”œâ”€â”€ layer1_consolidated.py   # L1 é•¿æœŸè®°å¿†
â”‚   â”‚   â”œâ”€â”€ layer2_working.py        # L2 å·¥ä½œè®°å¿†
â”‚   â”‚   â”œâ”€â”€ layer3_archive.py        # L3 åŸæ–‡å­˜æ¡£
â”‚   â”‚   â””â”€â”€ volume_manager.py        # åˆ†å·ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ index/                       # ç´¢å¼•å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ entity_index.py          # å®ä½“ç´¢å¼•
â”‚   â”‚   â”œâ”€â”€ inverted_index.py        # å€’æ’ç´¢å¼•
â”‚   â”‚   â”œâ”€â”€ vector_index.py          # å‘é‡ç´¢å¼•
â”‚   â”‚   â”œâ”€â”€ ngram_index.py           # N-gramç´¢å¼•
â”‚   â”‚   â””â”€â”€ code_index.py            # ä»£ç ç´¢å¼•
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/                   # æ£€ç´¢å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ eight_layer.py           # 8å±‚æ£€ç´¢å¼•æ“
â”‚   â”‚   â””â”€â”€ context_builder.py       # ä¸Šä¸‹æ–‡ç»„è£…
â”‚   â”‚
â”‚   â”œâ”€â”€ processor/                   # å¤„ç†å™¨
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ entity_extractor.py      # å®ä½“æå–
â”‚   â”‚   â”œâ”€â”€ foreshadowing.py         # ä¼ç¬”è¿½è¸ª
â”‚   â”‚   â”œâ”€â”€ consistency.py           # ä¸€è‡´æ€§æ ¡éªŒ
â”‚   â”‚   â””â”€â”€ code_analyzer.py         # ä»£ç åˆ†æ
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                      # æ•°æ®æ¨¡å‹ï¼ˆPydanticï¼‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ entity.py                # å®ä½“æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ event.py                 # äº‹ä»¶æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ foreshadowing.py         # ä¼ç¬”æ¨¡å‹
â”‚   â”‚   â””â”€â”€ turn.py                  # è½®æ¬¡æ¨¡å‹
â”‚   â”‚
â”‚   â””â”€â”€ utils/                       # å·¥å…·
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ embedding.py             # å‘é‡åŒ–
â”‚       â”œâ”€â”€ tokenizer.py             # åˆ†è¯
â”‚       â””â”€â”€ llm_client.py            # LLMè°ƒç”¨
â”‚
â”œâ”€â”€ plugins/                         # æ’ä»¶
â”‚   â””â”€â”€ sillytavern/                 # STæ’ä»¶
â”‚       â”œâ”€â”€ manifest.json
â”‚       â”œâ”€â”€ index.js
â”‚       â””â”€â”€ style.css
â”‚
â”œâ”€â”€ tests/                           # æµ‹è¯•
â”‚   â”œâ”€â”€ test_storage.py
â”‚   â”œâ”€â”€ test_retrieval.py
â”‚   â”œâ”€â”€ test_foreshadowing.py
â”‚   â””â”€â”€ test_integration.py
â”‚
â”œâ”€â”€ cli.py                           # CLIå…¥å£
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ä¸€ã€æ¶æ„æ€»è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Recall v3 å®Œæ•´æ¶æ„                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      å­˜å‚¨å±‚ï¼ˆ4å±‚ + åˆ†å·ï¼‰                         â”‚   â”‚
â”‚  â”‚  L0: æ ¸å¿ƒè®¾å®š â”‚ L1: é•¿æœŸè®°å¿† â”‚ L2: å·¥ä½œè®°å¿† â”‚ L3: åŸæ–‡å­˜æ¡£        â”‚   â”‚
â”‚  â”‚              â”‚              â”‚              â”‚ + åˆ†å·æ”¯æŒ           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      æ£€ç´¢å±‚ï¼ˆ8å±‚é˜²å¾¡ï¼‰                            â”‚   â”‚
â”‚  â”‚  ç²¾ç¡®â†’åˆ«åâ†’è§¦å‘è¯â†’å…³ç³»â†’æ—¶é—´â†’å‘é‡â†’N-gramâ†’è¿½é—®                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      ä¸“é¡¹è¿½è¸ªç³»ç»Ÿ                                 â”‚   â”‚
â”‚  â”‚  ä¼ç¬”è¿½è¸ª â”‚ ä¸€è‡´æ€§æ ¡éªŒ â”‚ ä»£ç ç´¢å¼• â”‚ ä¾èµ–è¿½è¸ª â”‚ é£æ ¼æ¨æ–­           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      é›¶é…ç½®è‡ªåŠ¨åŒ–å±‚                               â”‚   â”‚
â”‚  â”‚  è‡ªåŠ¨åœºæ™¯æ£€æµ‹ â”‚ è‡ªåŠ¨å‚æ•°è°ƒä¼˜ â”‚ è‡ªåŠ¨ç´¢å¼•ç»´æŠ¤                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## äºŒã€å­˜å‚¨å±‚ï¼š4å±‚ + åˆ†å·ï¼ˆæ”¯æŒ2äº¿å­—ï¼‰

### 2.1 åˆ†å·æ¶æ„

```
é¡¹ç›®æ ¹ç›®å½•/
â”œâ”€â”€ recall_data/
â”‚   â”œâ”€â”€ manifest.json          # å…¨å±€å…ƒæ•°æ®
â”‚   â”œâ”€â”€ L0_core/               # æ ¸å¿ƒè®¾å®šï¼ˆå•æ–‡ä»¶ï¼Œâ‰¤10KBï¼‰
â”‚   â”‚   â””â”€â”€ core.json
â”‚   â”‚
â”‚   â”œâ”€â”€ L1_consolidated/       # é•¿æœŸè®°å¿†ï¼ˆæŒ‰å®ä½“åˆ†ç‰‡ï¼‰
â”‚   â”‚   â”œâ”€â”€ entities_0001.json # æ¯ç‰‡æœ€å¤š1000å®ä½“
â”‚   â”‚   â”œâ”€â”€ entities_0002.json
â”‚   â”‚   â””â”€â”€ index.json         # åˆ†ç‰‡ç´¢å¼•
â”‚   â”‚
â”‚   â”œâ”€â”€ L2_working/            # å·¥ä½œè®°å¿†ï¼ˆå†…å­˜ä¸ºä¸»ï¼Œå®šæœŸæŒä¹…åŒ–ï¼‰
â”‚   â”‚   â””â”€â”€ session_{id}.json
â”‚   â”‚
â”‚   â”œâ”€â”€ L3_archive/            # åŸæ–‡å­˜æ¡£ï¼ˆåˆ†å·ï¼‰
â”‚   â”‚   â”œâ”€â”€ volume_0001/       # æ¯å·10ä¸‡è½® æˆ– 50MB
â”‚   â”‚   â”‚   â”œâ”€â”€ turns_00001_10000.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ turns_10001_20000.jsonl
â”‚   â”‚   â”‚   â””â”€â”€ volume_index.json
â”‚   â”‚   â”œâ”€â”€ volume_0002/
â”‚   â”‚   â””â”€â”€ global_index.json  # è·¨å·ç´¢å¼•
â”‚   â”‚
â”‚   â””â”€â”€ indexes/               # æ‰€æœ‰ç´¢å¼•ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰
â”‚       â”œâ”€â”€ entity_name.idx    # å®ä½“åç´¢å¼•
â”‚       â”œâ”€â”€ keyword_inverted.idx
â”‚       â”œâ”€â”€ ngram_3.idx        # 3-gramç´¢å¼•
â”‚       â”œâ”€â”€ vector.faiss       # å‘é‡ç´¢å¼•
â”‚       â””â”€â”€ code/              # ä»£ç ä¸“ç”¨ç´¢å¼•
â”‚           â”œâ”€â”€ symbols.idx    # ç¬¦å·ç´¢å¼•
â”‚           â”œâ”€â”€ imports.idx    # ä¾èµ–ç´¢å¼•
â”‚           â””â”€â”€ style.json     # é£æ ¼è§„èŒƒ
```

### 2.2 åˆ†å·ç­–ç•¥

```python
class VolumeManager:
    """åˆ†å·ç®¡ç†å™¨ - æ”¯æŒ2äº¿å­—è§„æ¨¡"""
    
    # é›¶é…ç½®é»˜è®¤å€¼ï¼ˆç»è¿‡ä¼˜åŒ–ï¼Œç”¨æˆ·æ— éœ€ä¿®æ”¹ï¼‰
    DEFAULT_CONFIG = {
        'turns_per_file': 10000,      # æ¯æ–‡ä»¶1ä¸‡è½®
        'max_volume_size_mb': 50,      # æ¯å·50MB
        'turns_per_volume': 100000,    # æ¯å·10ä¸‡è½®
        'preload_volumes': 2,          # é¢„åŠ è½½æœ€è¿‘2å·
        'index_granularity': 1000,     # ç´¢å¼•ç²’åº¦ï¼šæ¯1000è½®å»ºä¸€ä¸ªæ£€æŸ¥ç‚¹
    }
    
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.config = self.DEFAULT_CONFIG.copy()
        self.loaded_volumes = {}  # volume_id -> VolumeData
        self.file_locks = {}      # å¹¶å‘æ§åˆ¶
        self._init_storage()
    
    def _init_storage(self):
        """åˆå§‹åŒ–å­˜å‚¨ç›®å½•"""
        os.makedirs(f"{self.data_path}/L3_archive", exist_ok=True)
        self.manifest = self._load_or_create_manifest()
    
    def get_turn(self, turn_number: int) -> dict:
        """O(1) å®šä½ä»»æ„è½®æ¬¡"""
        volume_id = turn_number // self.config['turns_per_volume']
        file_id = (turn_number % self.config['turns_per_volume']) // self.config['turns_per_file']
        offset = turn_number % self.config['turns_per_file']
        
        # å¦‚æœå·æœªåŠ è½½ï¼ŒæŒ‰éœ€åŠ è½½
        if volume_id not in self.loaded_volumes:
            self._load_volume(volume_id)
        
        return self.loaded_volumes[volume_id].get_turn(file_id, offset)
    
    def _load_volume(self, volume_id: int):
        """åŠ è½½æŒ‡å®šå·åˆ°å†…å­˜"""
        volume_path = f"{self.data_path}/L3_archive/volume_{volume_id:04d}"
        
        if not os.path.exists(volume_path):
            # å·ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºå·
            self.loaded_volumes[volume_id] = VolumeData(volume_id)
            return
        
        # åŠ è½½å·ç´¢å¼•
        index_path = f"{volume_path}/volume_index.json"
        with open(index_path, 'r', encoding='utf-8') as f:
            volume_index = json.load(f)
        
        # åªåŠ è½½ç´¢å¼•ï¼Œæ•°æ®æ–‡ä»¶æŒ‰éœ€è¯»å–ï¼ˆèŠ‚çœå†…å­˜ï¼‰
        self.loaded_volumes[volume_id] = VolumeData(
            volume_id=volume_id,
            index=volume_index,
            base_path=volume_path,
            lazy_load=True  # æ‡’åŠ è½½æ¨¡å¼
        )
    
    def preload_recent(self, num_volumes: int = None):
        """é¢„åŠ è½½æœ€è¿‘çš„å·ï¼Œç¡®ä¿å¸¸ç”¨æ•°æ®åœ¨å†…å­˜"""
        if num_volumes is None:
            num_volumes = self.config['preload_volumes']
        
        latest_volume = self.manifest.get('latest_volume', 0)
        for i in range(num_volumes):
            vol_id = latest_volume - i
            if vol_id >= 0 and vol_id not in self.loaded_volumes:
                self._load_volume(vol_id)
                # æœ€è¿‘çš„å·å®Œå…¨åŠ è½½åˆ°å†…å­˜
                if i == 0:
                    self.loaded_volumes[vol_id].load_all_to_memory()
    
    def append_turn(self, turn_data: dict) -> int:
        """è¿½åŠ æ–°è½®æ¬¡ï¼Œè¿”å›è½®æ¬¡å·"""
        turn_number = self.manifest.get('total_turns', 0)
        volume_id = turn_number // self.config['turns_per_volume']
        
        # ç¡®ä¿å·å·²åŠ è½½
        if volume_id not in self.loaded_volumes:
            self._load_volume(volume_id)
        
        # ä½¿ç”¨æ–‡ä»¶é”ä¿è¯å¹¶å‘å®‰å…¨
        with self._get_lock(volume_id):
            self.loaded_volumes[volume_id].append(turn_data)
            self.manifest['total_turns'] = turn_number + 1
            self._save_manifest()
        
        return turn_number
    
    def _get_lock(self, volume_id: int):
        """è·å–å·çº§åˆ«çš„é”"""
        if volume_id not in self.file_locks:
            import threading
            self.file_locks[volume_id] = threading.Lock()
        return self.file_locks[volume_id]
    
    def _load_or_create_manifest(self) -> dict:
        """åŠ è½½æˆ–åˆ›å»ºå…¨å±€manifest"""
        manifest_path = f"{self.data_path}/manifest.json"
        if os.path.exists(manifest_path):
            with open(manifest_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {'total_turns': 0, 'latest_volume': 0, 'created_at': datetime.now().isoformat()}
    
    def _save_manifest(self):
        """ä¿å­˜manifest"""
        manifest_path = f"{self.data_path}/manifest.json"
        with open(manifest_path, 'w', encoding='utf-8') as f:
            json.dump(self.manifest, f, ensure_ascii=False, indent=2)
    
    def get_next_turn_number(self) -> int:
        """è·å–ä¸‹ä¸€ä¸ªè½®æ¬¡å·"""
        return self.manifest.get('total_turns', 0)
    
    def get_total_turns(self) -> int:
        """è·å–æ€»è½®æ¬¡æ•°"""
        return self.manifest.get('total_turns', 0)


class VolumeData:
    """å•ä¸ªå·çš„æ•°æ®ç®¡ç†"""
    
    def __init__(self, volume_id: int, index: dict = None, base_path: str = None, lazy_load: bool = False):
        self.volume_id = volume_id
        self.index = index or {'files': {}, 'turn_count': 0}
        self.base_path = base_path
        self.lazy_load = lazy_load
        self.cached_turns = {}  # turn_number -> turn_data
    
    def get_turn(self, file_id: int, offset: int) -> dict:
        """è·å–æŒ‡å®šè½®æ¬¡"""
        turn_number = self.volume_id * 100000 + file_id * 10000 + offset
        
        if turn_number in self.cached_turns:
            return self.cached_turns[turn_number]
        
        if self.lazy_load and self.base_path:
            # ä»æ–‡ä»¶è¯»å–
            file_path = f"{self.base_path}/turns_{file_id*10000+1:05d}_{(file_id+1)*10000:05d}.jsonl"
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    if offset < len(lines):
                        return json.loads(lines[offset])
        
        return None
    
    def append(self, turn_data: dict):
        """è¿½åŠ è½®æ¬¡"""
        turn_number = turn_data.get('turn', self.index['turn_count'])
        self.cached_turns[turn_number] = turn_data
        self.index['turn_count'] += 1
        
        # å®šæœŸæŒä¹…åŒ–
        if self.index['turn_count'] % 100 == 0:
            self._persist()
    
    def _persist(self):
        """æŒä¹…åŒ–åˆ°ç£ç›˜"""
        if not self.base_path:
            return
        
        os.makedirs(self.base_path, exist_ok=True)
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„å†™å…¥
        by_file = {}
        for turn_num, data in self.cached_turns.items():
            file_id = (turn_num % 100000) // 10000
            if file_id not in by_file:
                by_file[file_id] = []
            by_file[file_id].append(data)
        
        for file_id, turns in by_file.items():
            file_path = f"{self.base_path}/turns_{file_id*10000+1:05d}_{(file_id+1)*10000:05d}.jsonl"
            with open(file_path, 'a', encoding='utf-8') as f:
                for turn in turns:
                    f.write(json.dumps(turn, ensure_ascii=False) + '\n')
        
        # ä¿å­˜å·ç´¢å¼•
        index_path = f"{self.base_path}/volume_index.json"
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(self.index, f, ensure_ascii=False, indent=2)
    
    def load_all_to_memory(self):
        """å°†æ•´ä¸ªå·åŠ è½½åˆ°å†…å­˜ï¼ˆç”¨äºçƒ­å·ï¼‰"""
        if not self.base_path:
            return
        
        for file_name in os.listdir(self.base_path):
            if file_name.endswith('.jsonl'):
                file_path = f"{self.base_path}/{file_name}"
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        turn = json.loads(line)
                        self.cached_turns[turn['turn']] = turn
        
        self.lazy_load = False  # å·²å®Œå…¨åŠ è½½
```

### 2.3 å„å±‚è¯¦ç»†è®¾è®¡

#### å®Œæ•´æ•°æ®æ¨¡å‹å®šä¹‰ï¼ˆPydanticï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰

> **æ³¨æ„**ï¼šæœ¬æ–‡æ¡£ä¸­éƒ¨åˆ†æ•°æ®æ¨¡å‹ï¼ˆå¦‚ Foreshadowingã€Relationï¼‰åœ¨ä¸åŒç« èŠ‚æœ‰ä¸¤ç§å®šä¹‰ï¼š
> - **Pydanticç‰ˆ**ï¼šç”¨äºAPIåºåˆ—åŒ–å’Œæ•°æ®éªŒè¯ï¼ˆrecall/models/ï¼‰
> - **dataclassç‰ˆ**ï¼šç”¨äºå†…éƒ¨å¤„ç†é€»è¾‘ï¼ˆrecall/processor/ï¼‰
> 
> å®ç°æ—¶å¯ç»Ÿä¸€ä½¿ç”¨Pydanticç‰ˆæœ¬ï¼Œæˆ–æ ¹æ®åœºæ™¯é€‰æ‹©ã€‚ä¸¤è€…å­—æ®µåŸºæœ¬ä¸€è‡´ã€‚

```python
# recall/models/base.py
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
from datetime import datetime
from enum import Enum

class EntityType(str, Enum):
    CHARACTER = "CHARACTER"
    ITEM = "ITEM"
    LOCATION = "LOCATION"
    CONCEPT = "CONCEPT"
    CODE_SYMBOL = "CODE_SYMBOL"

class EventType(str, Enum):
    STATE_CHANGE = "STATE_CHANGE"
    RELATIONSHIP = "RELATIONSHIP"
    ITEM_TRANSFER = "ITEM_TRANSFER"
    FORESHADOWING = "FORESHADOWING"
    PLOT_POINT = "PLOT_POINT"
    CODE_CHANGE = "CODE_CHANGE"

class ForeshadowingStatus(str, Enum):
    UNRESOLVED = "UNRESOLVED"
    POSSIBLY_TRIGGERED = "POSSIBLY_TRIGGERED"
    RESOLVED = "RESOLVED"

# recall/models/entity.py
class Relation(BaseModel):
    target_id: str
    relation_type: str  # e.g., "æ‹äºº", "æ•Œäºº", "æ‹¥æœ‰", "ä½äº"
    established_turn: int
    is_current: bool = True

class Entity(BaseModel):
    id: str
    name: str
    aliases: List[str] = []
    entity_type: EntityType
    current_state: Dict[str, Any] = {}
    confidence: float = 1.0
    verification_count: int = 1
    source_turns: List[int] = []
    last_verified: datetime = Field(default_factory=datetime.now)
    relations: List[Relation] = []
    embedding: Optional[List[float]] = None  # è¯­ä¹‰å‘é‡

# recall/models/turn.py
class Turn(BaseModel):
    turn: int
    timestamp: datetime
    user: str
    assistant: str
    metadata: Dict[str, Any] = {}
    entities_mentioned: List[str] = []
    events_detected: List[str] = []
    ngrams_3: List[str] = []
    keywords: List[str] = []

# recall/models/foreshadowing.py
class Foreshadowing(BaseModel):
    id: str
    created_turn: int
    content: str
    summary: str
    trigger_keywords: List[str]
    trigger_combinations: List[List[str]]
    trigger_entities: List[str]
    content_embedding: Optional[List[float]] = None
    status: ForeshadowingStatus = ForeshadowingStatus.UNRESOLVED
    resolution_turn: Optional[int] = None
    resolution_content: Optional[str] = None
    remind_after_turns: int = 100
    last_reminded: Optional[int] = None
    importance: str = "MEDIUM"  # HIGH, MEDIUM, LOW

# recall/models/event.py
class Event(BaseModel):
    id: str
    turn: int
    event_type: EventType
    summary: str
    detail: str
    entities_involved: List[str]
    keywords: List[str]
    priority: str = "P2"  # P0, P1, P2, P3
    embedding: Optional[List[float]] = None
```

#### L0: æ ¸å¿ƒè®¾å®šï¼ˆæ°¸ä¸æ›´æ–°ï¼Œæ¯æ¬¡å¿…æ³¨å…¥ï¼‰

```python
# recall/storage/layer0_core.py
"""L0æ ¸å¿ƒè®¾å®š - å®Œæ•´å®ç°"""

import os
import json
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Any, Optional

@dataclass
class CoreSettings:
    """æ ¸å¿ƒè®¾å®š - ç”¨æˆ·ä¸€æ¬¡é…ç½®ï¼Œæ°¸ä¹…ç”Ÿæ•ˆ"""
    
    # RPåœºæ™¯
    character_card: str = ""          # è§’è‰²å¡ï¼ˆâ‰¤2000å­—ï¼‰
    world_setting: str = ""           # ä¸–ç•Œè§‚ï¼ˆâ‰¤1000å­—ï¼‰
    writing_style: str = ""           # å†™ä½œé£æ ¼è¦æ±‚
    
    # ä»£ç åœºæ™¯
    code_standards: str = ""          # ä»£ç è§„èŒƒ
    project_structure: str = ""       # é¡¹ç›®ç»“æ„è¯´æ˜
    naming_conventions: str = ""      # å‘½åè§„èŒƒ
    
    # é€šç”¨
    user_preferences: Dict[str, Any] = field(default_factory=dict)
    absolute_rules: List[str] = field(default_factory=list)  # ç»å¯¹ä¸èƒ½è¿åçš„è§„åˆ™
    
    @classmethod
    def load(cls, data_path: str) -> 'CoreSettings':
        """ä»æ–‡ä»¶åŠ è½½æ ¸å¿ƒè®¾å®š"""
        config_file = os.path.join(data_path, 'L0_core', 'core.json')
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return cls(**data)
        return cls()  # è¿”å›é»˜è®¤ç©ºè®¾å®š
    
    def save(self, data_path: str):
        """ä¿å­˜æ ¸å¿ƒè®¾å®š"""
        config_dir = os.path.join(data_path, 'L0_core')
        os.makedirs(config_dir, exist_ok=True)
        config_file = os.path.join(config_dir, 'core.json')
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(self), f, ensure_ascii=False, indent=2)
    
    def get_injection_text(self, scenario: str) -> str:
        """æ ¹æ®åœºæ™¯è¿”å›éœ€è¦æ³¨å…¥çš„æ ¸å¿ƒè®¾å®š"""
        if scenario == 'roleplay':
            parts = [self.character_card, self.world_setting, self.writing_style]
            return '\n\n'.join(p for p in parts if p)
        elif scenario == 'coding':
            parts = [self.code_standards, self.naming_conventions]
            return '\n\n'.join(p for p in parts if p)
        else:
            return self._get_universal_rules()
    
    def _get_universal_rules(self) -> str:
        """è·å–é€šç”¨è§„åˆ™"""
        if not self.absolute_rules:
            return ""
        return "ã€å¿…é¡»éµå®ˆçš„è§„åˆ™ã€‘\n" + "\n".join(f"- {r}" for r in self.absolute_rules)
```

#### L1: é•¿æœŸè®°å¿†ï¼ˆè·¨ä¼šè¯æŒä¹…ï¼‰

```python
# recall/storage/layer1_consolidated.py
"""L1é•¿æœŸè®°å¿† - å®Œæ•´å®ç°"""

import os
import json
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Any, Optional
from datetime import datetime

@dataclass
class ConsolidatedEntity:
    """é•¿æœŸè®°å¿†å®ä½“ - ç»è¿‡éªŒè¯çš„æŒä¹…çŸ¥è¯†"""
    
    id: str
    name: str
    aliases: List[str] = field(default_factory=list)
    entity_type: str = "UNKNOWN"  # CHARACTER, ITEM, LOCATION, CONCEPT, CODE_SYMBOL
    
    # å½“å‰çŠ¶æ€
    current_state: Dict[str, Any] = field(default_factory=dict)
    
    # éªŒè¯ä¿¡æ¯
    confidence: float = 0.5           # ç½®ä¿¡åº¦ (0-1)
    verification_count: int = 0       # è¢«éªŒè¯æ¬¡æ•°
    source_turns: List[int] = field(default_factory=list)     # åŸå§‹æ¥æº
    last_verified: str = ""           # ISOæ ¼å¼æ—¶é—´æˆ³
    
    # å…³ç³»
    relations: List[Dict] = field(default_factory=list)


class ConsolidatedMemory:
    """L1é•¿æœŸè®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.storage_dir = os.path.join(data_path, 'L1_consolidated')
        self.entities: Dict[str, ConsolidatedEntity] = {}
        self._load()
    
    def _load(self):
        """åŠ è½½æ‰€æœ‰é•¿æœŸè®°å¿†"""
        if not os.path.exists(self.storage_dir):
            os.makedirs(self.storage_dir, exist_ok=True)
            return
        
        for file in os.listdir(self.storage_dir):
            if file.startswith('entities_') and file.endswith('.json'):
                file_path = os.path.join(self.storage_dir, file)
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for item in data:
                        entity = ConsolidatedEntity(**item)
                        self.entities[entity.id] = entity
    
    def _save(self):
        """ä¿å­˜é•¿æœŸè®°å¿†ï¼ˆåˆ†ç‰‡å­˜å‚¨ï¼‰"""
        os.makedirs(self.storage_dir, exist_ok=True)
        
        # æ¯1000ä¸ªå®ä½“ä¸€ä¸ªæ–‡ä»¶
        entities_list = list(self.entities.values())
        chunk_size = 1000
        
        for i in range(0, len(entities_list), chunk_size):
            chunk = entities_list[i:i+chunk_size]
            file_path = os.path.join(self.storage_dir, f'entities_{i//chunk_size+1:04d}.json')
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump([asdict(e) for e in chunk], f, ensure_ascii=False, indent=2)
    
    def add_or_update(self, entity: ConsolidatedEntity):
        """æ·»åŠ æˆ–æ›´æ–°å®ä½“"""
        if entity.id in self.entities:
            existing = self.entities[entity.id]
            existing.verification_count += 1
            existing.confidence = min(1.0, existing.confidence + 0.1)
            existing.last_verified = datetime.now().isoformat()
            # åˆå¹¶çŠ¶æ€
            existing.current_state.update(entity.current_state)
        else:
            self.entities[entity.id] = entity
        self._save()
    
    def get(self, entity_id: str) -> Optional[ConsolidatedEntity]:
        """è·å–å®ä½“"""
        return self.entities.get(entity_id)
    
    def search_by_name(self, name: str) -> List[ConsolidatedEntity]:
        """æŒ‰åç§°æœç´¢"""
        name_lower = name.lower()
        results = []
        for entity in self.entities.values():
            if name_lower in entity.name.lower():
                results.append(entity)
            elif any(name_lower in alias.lower() for alias in entity.aliases):
                results.append(entity)
        return results
    
    def get_all_entity_names(self) -> List[str]:
        """è·å–æ‰€æœ‰å®ä½“åç§°ï¼ˆåŒ…æ‹¬åˆ«åï¼‰"""
        names = []
        for entity in self.entities.values():
            names.append(entity.name)
            names.extend(entity.aliases)
        return names
```

#### L2: å·¥ä½œè®°å¿†ï¼ˆä¼šè¯å†…æ´»è·ƒï¼‰

```python
# recall/storage/layer2_working.py
"""L2å·¥ä½œè®°å¿† - å®Œæ•´å®ç°"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass

@dataclass
class WorkingEntity:
    """å·¥ä½œè®°å¿†ä¸­çš„å®ä½“"""
    name: str
    entity_type: str
    last_accessed: int  # turn number
    access_count: int
    data: Dict[str, Any]

class WorkingMemory:
    """å·¥ä½œè®°å¿† - å½“å‰ä¼šè¯çš„æ´»è·ƒä¸Šä¸‹æ–‡"""
    
    def __init__(self, capacity: int = 200):
        self.capacity = capacity
        self.entities: Dict[str, WorkingEntity] = {}      # name -> entity
        self.events: List[Dict] = []        # æœ€è¿‘äº‹ä»¶
        self.focus_stack: List[str] = []    # å½“å‰ç„¦ç‚¹æ ˆï¼ˆå®ä½“ååˆ—è¡¨ï¼‰
        self.current_turn: int = 0
    
    def update_with_delta_rule(self, new_info):
        """Delta Rule: æ–°ä¿¡æ¯å¯ä»¥è¦†ç›–ç›¸å…³æ—§ä¿¡æ¯"""
        # new_info å¯ä»¥æ˜¯ ExtractedEntity æˆ– dict
        if hasattr(new_info, 'name'):
            name = new_info.name
            entity_type = getattr(new_info, 'entity_type', 'UNKNOWN')
            data = {'source': getattr(new_info, 'source_text', '')}
        else:
            name = new_info.get('name', str(new_info))
            entity_type = new_info.get('entity_type', 'UNKNOWN')
            data = new_info
        
        if name in self.entities:
            # æ›´æ–°å·²æœ‰å®ä½“
            existing = self.entities[name]
            existing.last_accessed = self.current_turn
            existing.access_count += 1
            existing.data.update(data if isinstance(data, dict) else {})
        else:
            # å®¹é‡æ»¡åˆ™æ·˜æ±°
            if len(self.entities) >= self.capacity:
                self._evict_one()
            
            # æ·»åŠ æ–°å®ä½“
            self.entities[name] = WorkingEntity(
                name=name,
                entity_type=entity_type,
                last_accessed=self.current_turn,
                access_count=1,
                data=data if isinstance(data, dict) else {'value': data}
            )
        
        # æ›´æ–°ç„¦ç‚¹æ ˆ
        if name in self.focus_stack:
            self.focus_stack.remove(name)
        self.focus_stack.append(name)
        if len(self.focus_stack) > 20:
            self.focus_stack.pop(0)
    
    def _evict_one(self):
        """æ·˜æ±°ä¸€ä¸ªæœ€ä¸æ´»è·ƒçš„å®ä½“"""
        if not self.entities:
            return
        
        # æ‰¾åˆ°æœ€ä¹…æœªè®¿é—®ä¸”è®¿é—®æ¬¡æ•°æœ€å°‘çš„
        min_score = float('inf')
        to_evict = None
        
        for name, entity in self.entities.items():
            # åˆ†æ•° = è®¿é—®æ¬¡æ•° / (å½“å‰è½®æ¬¡ - æœ€åè®¿é—®è½®æ¬¡ + 1)
            recency = self.current_turn - entity.last_accessed + 1
            score = entity.access_count / recency
            if score < min_score:
                min_score = score
                to_evict = name
        
        if to_evict:
            del self.entities[to_evict]
            if to_evict in self.focus_stack:
                self.focus_stack.remove(to_evict)
    
    def get_active_entities(self, limit: int = 50) -> List[WorkingEntity]:
        """è·å–æœ€æ´»è·ƒçš„å®ä½“"""
        sorted_entities = sorted(
            self.entities.values(),
            key=lambda e: (e.access_count, e.last_accessed),
            reverse=True
        )
        return sorted_entities[:limit]
    
    def increment_turn(self):
        """å¢åŠ è½®æ¬¡è®¡æ•°"""
        self.current_turn += 1
```

---

## äºŒç‚¹ä¸‰ã€çŸ¥è¯†å›¾è°±å±‚ï¼ˆè¶…è¶Š cognee çš„æ ¸å¿ƒèƒ½åŠ›ï¼‰

> **ä¸ç«å“å¯¹æ¯”**ï¼šcognee ä½¿ç”¨ Neo4j å­˜å‚¨çŸ¥è¯†å›¾è°±ï¼Œä½†éœ€è¦é¢å¤–éƒ¨ç½²æ•°æ®åº“ã€‚
> Recall ä½¿ç”¨è½»é‡çº§çš„æœ¬åœ°å›¾ç»“æ„ï¼Œæ— éœ€é¢å¤–ä¾èµ–ï¼ŒåŒæ—¶æä¾›ç­‰æ•ˆèƒ½åŠ›ã€‚

### 2.3.1 å®ä½“å…³ç³»å›¾

```python
# recall/graph/knowledge_graph.py
"""çŸ¥è¯†å›¾è°± - å®ä½“å…³ç³»çš„ç»“æ„åŒ–å­˜å‚¨"""

import json
import os
from dataclasses import dataclass, field, asdict
from typing import Dict, List, Set, Optional, Tuple
from collections import defaultdict

@dataclass
class Relation:
    """å®ä½“é—´çš„å…³ç³»"""
    source_id: str           # æºå®ä½“ID
    target_id: str           # ç›®æ ‡å®ä½“ID
    relation_type: str       # å…³ç³»ç±»å‹
    properties: Dict = field(default_factory=dict)  # å…³ç³»å±æ€§
    created_turn: int = 0    # åˆ›å»ºè½®æ¬¡
    confidence: float = 0.5  # ç½®ä¿¡åº¦
    source_text: str = ""    # åŸæ–‡ä¾æ®

class KnowledgeGraph:
    """è½»é‡çº§çŸ¥è¯†å›¾è°± - æ— éœ€ Neo4j"""
    
    # é¢„å®šä¹‰çš„å…³ç³»ç±»å‹ï¼ˆé’ˆå¯¹ RP åœºæ™¯ä¼˜åŒ–ï¼‰
    RELATION_TYPES = {
        # äººç‰©å…³ç³»
        'IS_FRIEND_OF': 'æ˜¯æœ‹å‹',
        'IS_ENEMY_OF': 'æ˜¯æ•Œäºº',
        'IS_FAMILY_OF': 'æ˜¯å®¶äºº',
        'LOVES': 'çˆ±æ…•',
        'HATES': 'æ†æ¨',
        'KNOWS': 'è®¤è¯†',
        'WORKS_FOR': 'ä¸º...å·¥ä½œ',
        'MENTORS': 'æŒ‡å¯¼',
        
        # ç©ºé—´å…³ç³»
        'LOCATED_IN': 'ä½äº',
        'TRAVELS_TO': 'å‰å¾€',
        'OWNS': 'æ‹¥æœ‰',
        'LIVES_IN': 'å±…ä½äº',
        
        # äº‹ä»¶å…³ç³»
        'PARTICIPATED_IN': 'å‚ä¸äº†',
        'CAUSED': 'å¯¼è‡´äº†',
        'WITNESSED': 'ç›®å‡»äº†',
        
        # ç‰©å“å…³ç³»
        'CARRIES': 'æºå¸¦',
        'USES': 'ä½¿ç”¨',
        'GAVE_TO': 'ç»™äºˆ',
        'RECEIVED_FROM': 'æ”¶åˆ°æ¥è‡ª',
    }
    
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.graph_file = os.path.join(data_path, 'knowledge_graph.json')
        
        # é‚»æ¥è¡¨å­˜å‚¨
        self.outgoing: Dict[str, List[Relation]] = defaultdict(list)  # source â†’ relations
        self.incoming: Dict[str, List[Relation]] = defaultdict(list)  # target â†’ relations
        self.relation_index: Dict[str, List[Relation]] = defaultdict(list)  # type â†’ relations
        
        self._load()
    
    def _load(self):
        """åŠ è½½å›¾è°±"""
        if os.path.exists(self.graph_file):
            with open(self.graph_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for item in data.get('relations', []):
                    rel = Relation(**item)
                    self._index_relation(rel)
    
    def _save(self):
        """ä¿å­˜å›¾è°±"""
        os.makedirs(os.path.dirname(self.graph_file), exist_ok=True)
        
        # æ”¶é›†æ‰€æœ‰å…³ç³»
        all_relations = []
        seen = set()
        for relations in self.outgoing.values():
            for rel in relations:
                key = (rel.source_id, rel.target_id, rel.relation_type)
                if key not in seen:
                    seen.add(key)
                    all_relations.append(asdict(rel))
        
        with open(self.graph_file, 'w', encoding='utf-8') as f:
            json.dump({'relations': all_relations}, f, ensure_ascii=False, indent=2)
    
    def _index_relation(self, rel: Relation):
        """ç´¢å¼•ä¸€ä¸ªå…³ç³»"""
        self.outgoing[rel.source_id].append(rel)
        self.incoming[rel.target_id].append(rel)
        self.relation_index[rel.relation_type].append(rel)
    
    def add_relation(self, source_id: str, target_id: str, relation_type: str,
                     properties: Dict = None, turn: int = 0, source_text: str = ""):
        """æ·»åŠ å…³ç³»"""
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        for rel in self.outgoing[source_id]:
            if rel.target_id == target_id and rel.relation_type == relation_type:
                # æ›´æ–°ç½®ä¿¡åº¦
                rel.confidence = min(1.0, rel.confidence + 0.1)
                self._save()
                return rel
        
        rel = Relation(
            source_id=source_id,
            target_id=target_id,
            relation_type=relation_type,
            properties=properties or {},
            created_turn=turn,
            confidence=0.5,
            source_text=source_text
        )
        self._index_relation(rel)
        self._save()
        return rel
    
    def get_neighbors(self, entity_id: str, relation_type: str = None, 
                      direction: str = 'both') -> List[Tuple[str, Relation]]:
        """è·å–é‚»å±…å®ä½“
        
        Args:
            entity_id: å®ä½“ID
            relation_type: å¯é€‰ï¼Œè¿‡æ»¤å…³ç³»ç±»å‹
            direction: 'out'=å‡ºè¾¹, 'in'=å…¥è¾¹, 'both'=åŒå‘
        
        Returns:
            [(é‚»å±…ID, å…³ç³»å¯¹è±¡), ...]
        """
        neighbors = []
        
        if direction in ('out', 'both'):
            for rel in self.outgoing.get(entity_id, []):
                if relation_type is None or rel.relation_type == relation_type:
                    neighbors.append((rel.target_id, rel))
        
        if direction in ('in', 'both'):
            for rel in self.incoming.get(entity_id, []):
                if relation_type is None or rel.relation_type == relation_type:
                    neighbors.append((rel.source_id, rel))
        
        return neighbors
    
    def find_path(self, source_id: str, target_id: str, max_depth: int = 3) -> Optional[List]:
        """æŸ¥æ‰¾ä¸¤ä¸ªå®ä½“é—´çš„è·¯å¾„ï¼ˆBFSï¼‰"""
        if source_id == target_id:
            return [source_id]
        
        visited = {source_id}
        queue = [(source_id, [source_id])]
        
        while queue:
            current, path = queue.pop(0)
            
            if len(path) > max_depth:
                continue
            
            for neighbor_id, rel in self.get_neighbors(current, direction='out'):
                if neighbor_id == target_id:
                    return path + [neighbor_id]
                
                if neighbor_id not in visited:
                    visited.add(neighbor_id)
                    queue.append((neighbor_id, path + [neighbor_id]))
        
        return None
    
    def get_subgraph(self, entity_id: str, depth: int = 2) -> Dict:
        """è·å–ä»¥æŸå®ä½“ä¸ºä¸­å¿ƒçš„å­å›¾"""
        visited = set()
        nodes = []
        edges = []
        queue = [(entity_id, 0)]
        
        while queue:
            current, current_depth = queue.pop(0)
            
            if current in visited or current_depth > depth:
                continue
            
            visited.add(current)
            nodes.append(current)
            
            for neighbor_id, rel in self.get_neighbors(current):
                edges.append({
                    'source': rel.source_id,
                    'target': rel.target_id,
                    'type': rel.relation_type
                })
                if neighbor_id not in visited:
                    queue.append((neighbor_id, current_depth + 1))
        
        return {'nodes': nodes, 'edges': edges}
    
    def query(self, pattern: str) -> List[Dict]:
        """ç®€å•çš„å›¾æŸ¥è¯¢ï¼ˆç±»ä¼¼ Cypher ä½†æ›´ç®€å•ï¼‰
        
        ç¤ºä¾‹: "PERSON -LOVES-> PERSON"
        """
        # è§£ææ¨¡å¼
        import re
        match = re.match(r'(\w+)\s*-(\w+)->\s*(\w+)', pattern)
        if not match:
            return []
        
        source_type, rel_type, target_type = match.groups()
        
        results = []
        for rel in self.relation_index.get(rel_type, []):
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ£€æŸ¥å®ä½“ç±»å‹
            results.append({
                'source': rel.source_id,
                'relation': rel_type,
                'target': rel.target_id,
                'confidence': rel.confidence
            })
        
        return results


### 2.3.2 å…³ç³»è‡ªåŠ¨æå–

```python
# recall/graph/relation_extractor.py
"""å…³ç³»æå–å™¨ - ä»å¯¹è¯ä¸­è‡ªåŠ¨å‘ç°å®ä½“å…³ç³»"""

import re
from typing import List, Tuple

class RelationExtractor:
    """ä»æ–‡æœ¬ä¸­è‡ªåŠ¨æå–å®ä½“å…³ç³»"""
    
    # å…³ç³»æ¨¡å¼ï¼ˆæ­£åˆ™åŒ¹é…ï¼‰
    PATTERNS = [
        # ä¸­æ–‡æ¨¡å¼
        (r'(.{2,10})æ˜¯(.{2,10})çš„(æœ‹å‹|æ•Œäºº|å®¶äºº|è€å¸ˆ|å­¦ç”Ÿ|ä¸Šå¸|ä¸‹å±)', 
         lambda m: (m.group(1), 'IS_' + {'æœ‹å‹':'FRIEND', 'æ•Œäºº':'ENEMY', 'å®¶äºº':'FAMILY', 
                    'è€å¸ˆ':'MENTOR', 'å­¦ç”Ÿ':'STUDENT', 'ä¸Šå¸':'BOSS', 'ä¸‹å±':'SUBORDINATE'}[m.group(3)] + '_OF', m.group(2))),
        
        (r'(.{2,10})çˆ±ä¸Šäº†(.{2,10})', lambda m: (m.group(1), 'LOVES', m.group(2))),
        (r'(.{2,10})å–œæ¬¢(.{2,10})', lambda m: (m.group(1), 'LIKES', m.group(2))),
        (r'(.{2,10})è®¨åŒ(.{2,10})', lambda m: (m.group(1), 'HATES', m.group(2))),
        (r'(.{2,10})ä½åœ¨(.{2,10})', lambda m: (m.group(1), 'LIVES_IN', m.group(2))),
        (r'(.{2,10})å»äº†(.{2,10})', lambda m: (m.group(1), 'TRAVELS_TO', m.group(2))),
        (r'(.{2,10})æ‹¥æœ‰(.{2,10})', lambda m: (m.group(1), 'OWNS', m.group(2))),
        (r'(.{2,10})ç»™(.{2,10})äº†(.{2,10})', lambda m: (m.group(1), 'GAVE_TO', m.group(2))),
        
        # è‹±æ–‡æ¨¡å¼
        (r'(\w+) is (?:a )?friend of (\w+)', lambda m: (m.group(1), 'IS_FRIEND_OF', m.group(2))),
        (r'(\w+) loves (\w+)', lambda m: (m.group(1), 'LOVES', m.group(2))),
        (r'(\w+) lives in (\w+)', lambda m: (m.group(1), 'LIVES_IN', m.group(2))),
    ]
    
    def __init__(self, entity_extractor):
        self.entity_extractor = entity_extractor
    
    def extract(self, text: str, turn: int = 0) -> List[Tuple[str, str, str, str]]:
        """
        ä»æ–‡æœ¬ä¸­æå–å…³ç³»
        
        Returns:
            [(source, relation_type, target, source_text), ...]
        """
        relations = []
        
        # 1. åŸºäºæ¨¡å¼åŒ¹é…
        for pattern, extractor in self.PATTERNS:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                try:
                    source, rel_type, target = extractor(match)
                    relations.append((source.strip(), rel_type, target.strip(), match.group(0)))
                except:
                    continue
        
        # 2. åŸºäºå…±ç°ï¼ˆåŒä¸€å¥è¯ä¸­å‡ºç°çš„å®ä½“å¯èƒ½æœ‰å…³ç³»ï¼‰
        sentences = re.split(r'[ã€‚.!?ï¼ï¼Ÿ]', text)
        entities = self.entity_extractor.extract(text)
        
        for sentence in sentences:
            sentence_entities = [e for e in entities if e.name in sentence]
            # å¦‚æœåŒä¸€å¥è¯æœ‰å¤šä¸ªå®ä½“ï¼Œå»ºç«‹å¼±å…³ç³»
            if len(sentence_entities) >= 2:
                for i, e1 in enumerate(sentence_entities[:-1]):
                    for e2 in sentence_entities[i+1:]:
                        relations.append((e1.name, 'MENTIONED_WITH', e2.name, sentence))
        
        return relations
```

---

## äºŒç‚¹å››ã€å¤šç”¨æˆ·/å¤šä¼šè¯æ”¯æŒï¼ˆè¶…è¶Š mem0ï¼‰

> **ä¸ mem0 å¯¹æ¯”**ï¼šmem0 æ”¯æŒ user_id å’Œ session_idï¼ŒRecall ä¹Ÿæ”¯æŒï¼Œ
> å¹¶é¢å¤–å¢åŠ äº† **è§’è‰²éš”ç¦»**ï¼ˆä¸åŒ RP è§’è‰²çš„è®°å¿†ä¸æ··æ·†ï¼‰ã€‚

```python
# recall/storage/multi_tenant.py
"""å¤šç”¨æˆ·/å¤šä¼šè¯æ”¯æŒ"""

import os
from typing import Optional
from dataclasses import dataclass

@dataclass
class MemoryScope:
    """è®°å¿†ä½œç”¨åŸŸ"""
    user_id: str = "default"      # ç”¨æˆ·ID
    session_id: str = "default"   # ä¼šè¯IDï¼ˆå¯é€‰ï¼‰
    character_id: str = "default" # è§’è‰²IDï¼ˆRPåœºæ™¯ï¼‰
    
    def to_path(self) -> str:
        """è½¬æ¢ä¸ºå­˜å‚¨è·¯å¾„"""
        return f"{self.user_id}/{self.character_id}/{self.session_id}"


class MultiTenantStorage:
    """å¤šç§Ÿæˆ·å­˜å‚¨ç®¡ç†"""
    
    def __init__(self, base_path: str):
        self.base_path = base_path
    
    def get_data_path(self, scope: MemoryScope) -> str:
        """è·å–ç‰¹å®šä½œç”¨åŸŸçš„æ•°æ®è·¯å¾„"""
        path = os.path.join(self.base_path, scope.to_path())
        os.makedirs(path, exist_ok=True)
        return path
    
    def list_users(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰ç”¨æˆ·"""
        if not os.path.exists(self.base_path):
            return []
        return [d for d in os.listdir(self.base_path) 
                if os.path.isdir(os.path.join(self.base_path, d))]
    
    def list_characters(self, user_id: str) -> list:
        """åˆ—å‡ºç”¨æˆ·çš„æ‰€æœ‰è§’è‰²"""
        user_path = os.path.join(self.base_path, user_id)
        if not os.path.exists(user_path):
            return []
        return [d for d in os.listdir(user_path) 
                if os.path.isdir(os.path.join(user_path, d))]
    
    def delete_session(self, scope: MemoryScope):
        """åˆ é™¤ç‰¹å®šä¼šè¯çš„è®°å¿†"""
        import shutil
        path = self.get_data_path(scope)
        if os.path.exists(path):
            shutil.rmtree(path)
    
    def export_memories(self, scope: MemoryScope) -> dict:
        """å¯¼å‡ºæŸä½œç”¨åŸŸçš„æ‰€æœ‰è®°å¿†ï¼ˆç”¨äºå¤‡ä»½/è¿ç§»ï¼‰"""
        import json
        path = self.get_data_path(scope)
        
        export_data = {'scope': scope.__dict__, 'files': {}}
        for root, dirs, files in os.walk(path):
            for file in files:
                if file.endswith('.json'):
                    file_path = os.path.join(root, file)
                    rel_path = os.path.relpath(file_path, path)
                    with open(file_path, 'r', encoding='utf-8') as f:
                        export_data['files'][rel_path] = json.load(f)
        
        return export_data
    
    def import_memories(self, export_data: dict, target_scope: MemoryScope = None):
        """å¯¼å…¥è®°å¿†"""
        import json
        
        scope = target_scope or MemoryScope(**export_data['scope'])
        path = self.get_data_path(scope)
        
        for rel_path, content in export_data['files'].items():
            file_path = os.path.join(path, rel_path)
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(content, f, ensure_ascii=False, indent=2)
```

#### L3: åŸæ–‡å­˜æ¡£ï¼ˆå®Œæ•´ä¿å­˜ï¼Œæ°¸ä¸å‹ç¼©ï¼‰

```python
# recall/storage/archive.py
from datetime import datetime
from typing import List

class ArchiveStorage:
    """åŸæ–‡å­˜æ¡£ - ä¸€å­—ä¸å·®ï¼Œæ”¯æŒåˆ†å·"""
    
    def __init__(self, volume_manager, ngram_index, inverted_index):
        self.volume_manager = volume_manager
        self.ngram_index = ngram_index
        self.inverted_index = inverted_index
    
    def store_turn(self, turn_number: int, user_input: str, ai_output: str, metadata: dict):
        """å­˜å‚¨å®Œæ•´å¯¹è¯è½®æ¬¡"""
        combined = user_input + ' ' + ai_output
        record = {
            'turn': turn_number,
            'timestamp': datetime.now().isoformat(),
            'user': user_input,
            'assistant': ai_output,
            'metadata': metadata,
        }
        
        self.volume_manager.append_turn(record)
        self._update_indexes(turn_number, combined)
    
    def _update_indexes(self, turn_number: int, content: str):
        """æ›´æ–°ç´¢å¼•"""
        # æå–å…³é”®è¯å¹¶æ·»åŠ åˆ°å€’æ’ç´¢å¼•
        keywords = self._extract_keywords(content)
        self.inverted_index.add_batch(keywords, turn_number)
        
        # æ·»åŠ åˆ° N-gram ç´¢å¼•
        self.ngram_index.add(turn_number, content)
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰"""
        import re
        # ä¸­æ–‡è¯ç»„
        chinese = re.findall(r'[\u4e00-\u9fa5]{2,6}', text)
        # è‹±æ–‡å•è¯
        english = re.findall(r'[a-zA-Z]{3,}', text.lower())
        # è¿‡æ»¤åœç”¨è¯
        stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'æœ‰', 'è¿™', 'é‚£', 'the', 'a', 'an', 'is', 'are'}
        return [w for w in chinese + english if w not in stopwords]
    
    def search_raw(self, query: str) -> List[dict]:
        """åŸæ–‡æœç´¢ - ç»ˆæå…œåº•ï¼Œ100%ä¸æ¼"""
        results = []
        
        # å…ˆç”¨ç´¢å¼•å¿«é€Ÿå®šä½å€™é€‰
        candidate_turns = self.ngram_index.search(query)
        
        # å¯¹å€™é€‰åšç²¾ç¡®åŒ¹é…éªŒè¯
        for turn in candidate_turns:
            record = self.volume_manager.get_turn(turn)
            if query in record['user'] or query in record['assistant']:
                results.append(record)
        
        return results
```

---

## äºŒç‚¹äº”ã€ç´¢å¼•å±‚å®Œæ•´å®ç°

### 2.5.1 å®ä½“ç´¢å¼•

```python
# recall/index/entity_index.py
"""å®ä½“ç´¢å¼• - æ”¯æŒåç§°å’Œåˆ«åçš„å¿«é€ŸæŸ¥æ‰¾"""

import json
import os
from typing import Dict, List, Optional, Set
from dataclasses import dataclass, asdict

@dataclass
class IndexedEntity:
    """ç´¢å¼•ä¸­çš„å®ä½“"""
    id: str
    name: str
    aliases: List[str]
    entity_type: str
    turn_references: List[int]  # å‡ºç°è¿‡çš„è½®æ¬¡
    
class EntityIndex:
    """å®ä½“ç´¢å¼•"""
    
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.index_file = os.path.join(data_path, 'indexes', 'entity_index.json')
        
        # å†…å­˜ç´¢å¼•
        self.entities: Dict[str, IndexedEntity] = {}   # id â†’ entity
        self.name_index: Dict[str, str] = {}           # name/alias â†’ id
        
        self._load()
    
    def _load(self):
        """åŠ è½½ç´¢å¼•"""
        if os.path.exists(self.index_file):
            with open(self.index_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for item in data:
                    entity = IndexedEntity(**item)
                    self.entities[entity.id] = entity
                    self.name_index[entity.name.lower()] = entity.id
                    for alias in entity.aliases:
                        self.name_index[alias.lower()] = entity.id
    
    def _save(self):
        """ä¿å­˜ç´¢å¼•"""
        os.makedirs(os.path.dirname(self.index_file), exist_ok=True)
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump([asdict(e) for e in self.entities.values()], f, ensure_ascii=False)
    
    def add(self, entity: IndexedEntity):
        """æ·»åŠ å®ä½“"""
        if entity.id in self.entities:
            # åˆå¹¶å¼•ç”¨
            existing = self.entities[entity.id]
            existing.turn_references = list(set(existing.turn_references + entity.turn_references))
            existing.aliases = list(set(existing.aliases + entity.aliases))
        else:
            self.entities[entity.id] = entity
        
        # æ›´æ–°åç§°ç´¢å¼•
        self.name_index[entity.name.lower()] = entity.id
        for alias in entity.aliases:
            self.name_index[alias.lower()] = entity.id
        
        self._save()
    
    def get_by_name(self, name: str) -> Optional[IndexedEntity]:
        """é€šè¿‡åç§°æˆ–åˆ«åæŸ¥æ‰¾"""
        entity_id = self.name_index.get(name.lower())
        if entity_id:
            return self.entities.get(entity_id)
        return None
    
    def get_by_id(self, entity_id: str) -> Optional[IndexedEntity]:
        """é€šè¿‡IDæŸ¥æ‰¾"""
        return self.entities.get(entity_id)
    
    def search(self, query: str) -> List[IndexedEntity]:
        """æ¨¡ç³Šæœç´¢"""
        query_lower = query.lower()
        results = []
        
        for name, entity_id in self.name_index.items():
            if query_lower in name:
                entity = self.entities[entity_id]
                if entity not in results:
                    results.append(entity)
        
        return results
    
    def all_entities(self) -> List[IndexedEntity]:
        """è¿”å›æ‰€æœ‰å®ä½“"""
        return list(self.entities.values())
    
    def get_top_entities(self, limit: int = 100) -> List[IndexedEntity]:
        """è·å–æœ€å¸¸å¼•ç”¨çš„å®ä½“ï¼ˆç”¨äºé¢„çƒ­ç¼“å­˜ï¼‰"""
        sorted_entities = sorted(
            self.entities.values(),
            key=lambda e: len(e.turn_references),
            reverse=True
        )
        return sorted_entities[:limit]


### 2.5.2 å€’æ’ç´¢å¼•

```python
# recall/index/inverted_index.py
"""å€’æ’ç´¢å¼• - å…³é”®è¯åˆ°è½®æ¬¡çš„æ˜ å°„"""

import json
import os
from typing import Dict, List, Set
from collections import defaultdict

class InvertedIndex:
    """å€’æ’ç´¢å¼•"""
    
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.index_file = os.path.join(data_path, 'indexes', 'inverted_index.json')
        
        # keyword â†’ set of turn_ids
        self.index: Dict[str, Set[int]] = defaultdict(set)
        
        self._load()
    
    def _load(self):
        """åŠ è½½ç´¢å¼•"""
        if os.path.exists(self.index_file):
            with open(self.index_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for keyword, turns in data.items():
                    self.index[keyword] = set(turns)
    
    def _save(self):
        """ä¿å­˜ç´¢å¼•ï¼ˆå¢é‡ï¼‰"""
        os.makedirs(os.path.dirname(self.index_file), exist_ok=True)
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump({k: list(v) for k, v in self.index.items()}, f, ensure_ascii=False)
    
    def add(self, keyword: str, turn_id: int):
        """æ·»åŠ ç´¢å¼•é¡¹"""
        keyword = keyword.lower()
        self.index[keyword].add(turn_id)
        # æ‰¹é‡ä¿å­˜ä¼˜åŒ–ï¼šæ¯100æ¬¡æ·»åŠ ä¿å­˜ä¸€æ¬¡
        if sum(len(v) for v in self.index.values()) % 100 == 0:
            self._save()
    
    def add_batch(self, keywords: List[str], turn_id: int):
        """æ‰¹é‡æ·»åŠ """
        for kw in keywords:
            self.index[kw.lower()].add(turn_id)
        self._save()
    
    def search(self, keyword: str) -> List[int]:
        """æœç´¢åŒ…å«å…³é”®è¯çš„è½®æ¬¡"""
        return sorted(self.index.get(keyword.lower(), set()))
    
    def search_all(self, keywords: List[str]) -> List[int]:
        """æœç´¢åŒ…å«æ‰€æœ‰å…³é”®è¯çš„è½®æ¬¡ï¼ˆANDé€»è¾‘ï¼‰"""
        if not keywords:
            return []
        
        result_sets = [self.index.get(kw.lower(), set()) for kw in keywords]
        intersection = set.intersection(*result_sets) if result_sets else set()
        return sorted(intersection)
    
    def search_any(self, keywords: List[str]) -> List[int]:
        """æœç´¢åŒ…å«ä»»ä¸€å…³é”®è¯çš„è½®æ¬¡ï¼ˆORé€»è¾‘ï¼‰"""
        result = set()
        for kw in keywords:
            result.update(self.index.get(kw.lower(), set()))
        return sorted(result)
```

### 2.5.3 å‘é‡ç´¢å¼•

```python
# recall/index/vector_index.py
"""å‘é‡ç´¢å¼• - è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢"""

import os
import numpy as np
from typing import List, Tuple, Optional

class VectorIndex:
    """å‘é‡ç´¢å¼• - ä½¿ç”¨FAISSå®ç°é«˜æ•ˆç›¸ä¼¼åº¦æœç´¢"""
    
    def __init__(self, data_path: str, model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2'):
        self.data_path = data_path
        self.index_file = os.path.join(data_path, 'indexes', 'vector_index.faiss')
        self.mapping_file = os.path.join(data_path, 'indexes', 'vector_mapping.json')
        
        # è‡ªå®šä¹‰æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆéš”ç¦»åˆ°é¡¹ç›®ç›®å½• ./recall_data/models/ï¼‰
        # ä½¿ç”¨ RecallInit è·å–ç»Ÿä¸€çš„æ•°æ®æ ¹ç›®å½•
        from ..init import RecallInit
        model_cache_dir = os.path.join(RecallInit.get_data_root(), 'models', 'sentence-transformers')
        os.makedirs(model_cache_dir, exist_ok=True)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®© sentence-transformers ä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜ç›®å½•
        os.environ['SENTENCE_TRANSFORMERS_HOME'] = model_cache_dir
        
        # åŠ è½½embeddingæ¨¡å‹
        from sentence_transformers import SentenceTransformer
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
        
        # åˆå§‹åŒ–FAISSç´¢å¼•
        import faiss
        self.index = None
        self.turn_mapping: List[int] = []  # FAISSå†…éƒ¨ID â†’ turn_id
        
        self._load()
    
    def _load(self):
        """åŠ è½½ç´¢å¼•"""
        import faiss
        
        if os.path.exists(self.index_file):
            self.index = faiss.read_index(self.index_file)
            
            import json
            with open(self.mapping_file, 'r') as f:
                self.turn_mapping = json.load(f)
        else:
            # åˆ›å»ºæ–°ç´¢å¼• (Inner Product for cosine similarity with normalized vectors)
            self.index = faiss.IndexFlatIP(self.dimension)
    
    def _save(self):
        """ä¿å­˜ç´¢å¼•"""
        import faiss
        import json
        
        os.makedirs(os.path.dirname(self.index_file), exist_ok=True)
        faiss.write_index(self.index, self.index_file)
        
        with open(self.mapping_file, 'w') as f:
            json.dump(self.turn_mapping, f)
    
    def encode(self, text: str) -> np.ndarray:
        """æ–‡æœ¬è½¬å‘é‡"""
        embedding = self.model.encode(text, normalize_embeddings=True)
        return embedding.astype('float32')
    
    def add(self, turn_id: int, embedding: np.ndarray):
        """æ·»åŠ å‘é‡"""
        if embedding.ndim == 1:
            embedding = embedding.reshape(1, -1)
        
        self.index.add(embedding)
        self.turn_mapping.append(turn_id)
        
        # æ¯100æ¬¡æ·»åŠ ä¿å­˜ä¸€æ¬¡
        if len(self.turn_mapping) % 100 == 0:
            self._save()
    
    def add_text(self, turn_id: int, text: str):
        """ç›´æ¥æ·»åŠ æ–‡æœ¬"""
        embedding = self.encode(text)
        self.add(turn_id, embedding)
    
    def search(self, query: str, top_k: int = 20) -> List[Tuple[int, float]]:
        """æœç´¢æœ€ç›¸ä¼¼çš„è½®æ¬¡"""
        if self.index.ntotal == 0:
            return []
        
        query_embedding = self.encode(query).reshape(1, -1)
        
        # FAISSæœç´¢
        distances, indices = self.index.search(query_embedding, min(top_k, self.index.ntotal))
        
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx >= 0 and idx < len(self.turn_mapping):
                turn_id = self.turn_mapping[idx]
                results.append((turn_id, float(dist)))
        
        return results
    
    def search_by_embedding(self, embedding: np.ndarray, top_k: int = 20) -> List[Tuple[int, float]]:
        """é€šè¿‡å‘é‡æœç´¢"""
        if self.index.ntotal == 0:
            return []
        
        if embedding.ndim == 1:
            embedding = embedding.reshape(1, -1)
        
        distances, indices = self.index.search(embedding, min(top_k, self.index.ntotal))
        
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx >= 0 and idx < len(self.turn_mapping):
                turn_id = self.turn_mapping[idx]
                results.append((turn_id, float(dist)))
        
        return results
```

### 2.5.4 å®ä½“æå–å™¨

```python
# recall/processor/entity_extractor.py
"""å®ä½“æå–å™¨ - NLPé©±åŠ¨çš„å®ä½“è¯†åˆ«"""

import re
from typing import List, Set
from dataclasses import dataclass

@dataclass
class ExtractedEntity:
    """æå–çš„å®ä½“"""
    name: str
    entity_type: str  # PERSON, LOCATION, ITEM, ORG, CODE_SYMBOL
    confidence: float
    source_text: str

class EntityExtractor:
    """å®ä½“æå–å™¨"""
    
    def __init__(self):
        # åŠ è½½spaCyæ¨¡å‹ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰
        self.nlp = self._load_spacy_model()
        
        # åŠ è½½jiebaç”¨äºä¸­æ–‡åˆ†è¯
        import jieba
        self.jieba = jieba
        
        # åœç”¨è¯
        self.stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'æœ‰', 'è¿™', 'é‚£', 'å°±', 'éƒ½', 
                         'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
                         'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would'}
    
    def _load_spacy_model(self):
        """åŠ è½½ spaCy æ¨¡å‹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬åœ°ç›®å½•ï¼ˆä¸æ±¡æŸ“å…¨å±€ï¼‰"""
        import spacy
        import subprocess
        import sys
        import os
        
        # è‡ªå®šä¹‰æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆéš”ç¦»åˆ°é¡¹ç›®ç›®å½• ./recall_data/models/ï¼‰
        # ä½¿ç”¨ RecallInit è·å–ç»Ÿä¸€çš„æ•°æ®æ ¹ç›®å½•
        from ..init import RecallInit
        model_cache_dir = os.path.join(RecallInit.get_data_root(), 'models', 'spacy')
        os.makedirs(model_cache_dir, exist_ok=True)
        
        # ä¼˜å…ˆå°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
        for model_name in ['zh_core_web_sm', 'en_core_web_sm']:
            local_model_path = os.path.join(model_cache_dir, model_name)
            
            # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰æ¨¡å‹
            if os.path.exists(local_model_path):
                try:
                    return spacy.load(local_model_path)
                except Exception:
                    pass
            
            # å°è¯•ä»å…¨å±€åŠ è½½ï¼ˆå¦‚æœç”¨æˆ·å·²å®‰è£…ï¼‰
            try:
                return spacy.load(model_name)
            except OSError:
                pass
            
            # ä¸‹è½½åˆ°æœ¬åœ°ç›®å½•
            print(f"[Recall] é¦–æ¬¡è¿è¡Œï¼Œæ­£åœ¨ä¸‹è½½ NLP æ¨¡å‹ {model_name}...")
            print(f"[Recall] æ¨¡å‹å°†ä¿å­˜åˆ°ï¼š{model_cache_dir}")
            try:
                # ä½¿ç”¨ spacy ä¸‹è½½åˆ°æŒ‡å®šç›®å½•
                subprocess.check_call([
                    sys.executable, '-m', 'spacy', 'download', model_name,
                    '--target', model_cache_dir
                ])
                return spacy.load(local_model_path)
            except Exception as e:
                print(f"[Recall] ä¸‹è½½ {model_name} å¤±è´¥: {e}")
                continue
        
        # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨ç©ºç™½æ¨¡å‹ï¼ˆåŸºç¡€åŠŸèƒ½ä»å¯ç”¨ï¼‰
        print("[Recall] è­¦å‘Šï¼šæ— æ³•åŠ è½½ NLP æ¨¡å‹ï¼Œå®ä½“è¯†åˆ«åŠŸèƒ½å°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
        return spacy.blank('zh')  # ç©ºç™½æ¨¡å‹ï¼Œåªæœ‰åˆ†è¯ï¼Œæ²¡æœ‰NER
    
    def extract(self, text: str) -> List[ExtractedEntity]:
        """æå–å®ä½“"""
        entities = []
        
        # 1. ä½¿ç”¨spaCyæå–å‘½åå®ä½“
        doc = self.nlp(text[:10000])  # é™åˆ¶é•¿åº¦é¿å…OOM
        for ent in doc.ents:
            entity_type = self._map_spacy_label(ent.label_)
            if entity_type:
                entities.append(ExtractedEntity(
                    name=ent.text,
                    entity_type=entity_type,
                    confidence=0.8,
                    source_text=ent.sent.text if ent.sent else text[:100]
                ))
        
        # 2. ä¸­æ–‡ä¸“åæå–ï¼ˆå¼•å·å†…å®¹ã€ä¹¦åå·å†…å®¹ï¼‰
        quoted = re.findall(r'[ã€Œã€"\'](.*?)[ã€ã€"\']', text)
        for name in quoted:
            if 2 <= len(name) <= 20:
                entities.append(ExtractedEntity(
                    name=name,
                    entity_type='ITEM' if len(name) <= 4 else 'MISC',
                    confidence=0.6,
                    source_text=text[:100]
                ))
        
        # 3. ä»£ç ç¬¦å·æå–
        code_symbols = re.findall(r'\b([A-Z][a-zA-Z0-9_]+)\b', text)  # CamelCase
        code_symbols += re.findall(r'\b([a-z_][a-zA-Z0-9_]{2,})\b', text)  # snake_case
        for symbol in set(code_symbols):
            if not symbol.lower() in self.stopwords:
                entities.append(ExtractedEntity(
                    name=symbol,
                    entity_type='CODE_SYMBOL',
                    confidence=0.5,
                    source_text=text[:100]
                ))
        
        # å»é‡
        seen = set()
        unique_entities = []
        for e in entities:
            if e.name.lower() not in seen:
                seen.add(e.name.lower())
                unique_entities.append(e)
        
        return unique_entities
    
    def extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        keywords = []
        
        # jiebaåˆ†è¯æå–
        words = self.jieba.cut(text)
        for word in words:
            if len(word) >= 2 and word not in self.stopwords:
                keywords.append(word)
        
        # è‹±æ–‡å…³é”®è¯
        english_words = re.findall(r'[a-zA-Z]{3,}', text)
        keywords.extend([w.lower() for w in english_words if w.lower() not in self.stopwords])
        
        return list(set(keywords))
    
    def _map_spacy_label(self, label: str) -> str:
        """æ˜ å°„spaCyæ ‡ç­¾åˆ°æˆ‘ä»¬çš„ç±»å‹"""
        mapping = {
            'PERSON': 'PERSON',
            'PER': 'PERSON',
            'GPE': 'LOCATION',
            'LOC': 'LOCATION',
            'ORG': 'ORG',
            'PRODUCT': 'ITEM',
            'WORK_OF_ART': 'ITEM',
        }
        return mapping.get(label, None)
```

---

## ä¸‰ã€æ£€ç´¢å±‚ï¼š8å±‚é˜²å¾¡ï¼ˆ100%ä¸é—å¿˜ï¼‰

```
ç”¨æˆ·æŸ¥è¯¢
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          8 å±‚æ£€ç´¢é˜²å¾¡                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ç¬¬1å±‚ï¼šç²¾ç¡®åŒ¹é…                                                   â”‚   â”‚
â”‚  â”‚ "ç¥ç§˜è€äºº" â†’ ç›´æ¥å‘½ä¸­å®ä½“                                         â”‚   â”‚
â”‚  â”‚ å¤æ‚åº¦ï¼šO(1)  â”‚  å‡†ç¡®ç‡ï¼š100%ï¼ˆå¦‚æœåå­—å®Œå…¨åŒ¹é…ï¼‰                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ç¬¬2å±‚ï¼šåˆ«ååŒ¹é…                                                   â”‚   â”‚
â”‚  â”‚ "è€å¤´" â†’ åŒ¹é…åˆ«å â†’ å‘½ä¸­"ç¥ç§˜è€äºº"                                 â”‚   â”‚
â”‚  â”‚ å¤æ‚åº¦ï¼šO(1)  â”‚  è¦†ç›–ï¼šæ˜µç§°ã€ç®€ç§°ã€è¯¯æ‹¼                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ç¬¬3å±‚ï¼šè§¦å‘ç‰¹å¾ç»„åˆ                                               â”‚   â”‚
â”‚  â”‚ "æœˆäº®" + "å˜çº¢" â†’ è§¦å‘ç»„åˆ â†’ å‘½ä¸­"è¡€æœˆé¢„è¨€"                        â”‚   â”‚
â”‚  â”‚ ç”¨é€”ï¼šæ•æ‰é—´æ¥æè¿°ã€éšæ™¦æŒ‡ä»£                                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ç¬¬4å±‚ï¼šå…³ç³»å›¾è°±æ‰©å±•                                               â”‚   â”‚
â”‚  â”‚ "ç¥ç§˜è€äºº" â†’ å…³è” â†’ ["é“¶è‰²é’¥åŒ™", "è¡€æœˆé¢„è¨€", "æ£®æ—å°å±‹"]           â”‚   â”‚
â”‚  â”‚ ç”¨é€”ï¼šæ‰¾åˆ°é—´æ¥ç›¸å…³çš„å®ä½“                                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ç¬¬5å±‚ï¼šæ—¶é—´èŒƒå›´æ‰«æ                                               â”‚   â”‚
â”‚  â”‚ "æœ€å¼€å§‹" â†’ æ‰«æç¬¬1-100è½®                                          â”‚   â”‚
â”‚  â”‚ "æ˜¨å¤©" â†’ æ‰«æå¯¹åº”æ—¶é—´æˆ³çš„è½®æ¬¡                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ç¬¬6å±‚ï¼šå‘é‡è¯­ä¹‰æ£€ç´¢                                               â”‚   â”‚
â”‚  â”‚ è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ï¼Œæ•æ‰åŒä¹‰è¡¨è¾¾ã€é—´æ¥æè¿°                              â”‚   â”‚
â”‚  â”‚ "é‚£ä¸ªç»™æˆ‘ä¸œè¥¿çš„äºº" â†’ è¯­ä¹‰æ¥è¿‘"ç¥ç§˜è€äººç»™ä¸»è§’é’¥åŒ™"                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ç¬¬7å±‚ï¼šN-gram åŸæ–‡åŒ¹é…ï¼ˆç»ˆæå…œåº•ï¼‰                                 â”‚   â”‚
â”‚  â”‚ çº¯å­—ç¬¦ä¸²åŒ¹é…ï¼Œä¸ä¾èµ–ä»»ä½•æ™ºèƒ½æå–                                   â”‚   â”‚
â”‚  â”‚ å³ä½¿æ‰€æœ‰æ™ºèƒ½å¤„ç†éƒ½å¤±è´¥ï¼Œåªè¦åŸæ–‡å­˜åœ¨å°±èƒ½æ‰¾åˆ°                        â”‚   â”‚
â”‚  â”‚ âš ï¸ è¿™å±‚æ˜¯ 100% ä¸é—å¿˜çš„ä¿è¯                                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ç¬¬8å±‚ï¼šå¼•å¯¼å¼è¿½é—®                                                 â”‚   â”‚
â”‚  â”‚ å¦‚æœä»¥ä¸Šéƒ½æ²¡å‘½ä¸­ï¼Œä¸»åŠ¨è¯¢é—®ç”¨æˆ·æ›´å¤šçº¿ç´¢                              â”‚   â”‚
â”‚  â”‚ "ä½ èƒ½æä¾›æ›´å¤šç»†èŠ‚å—ï¼Ÿæ¯”å¦‚å¤§æ¦‚æ˜¯ä»€ä¹ˆæ—¶å€™çš„äº‹ï¼Ÿ"                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8å±‚æ£€ç´¢å®ç°

```python
# recall/retrieval/eight_layer.py
"""8å±‚æ£€ç´¢å¼•æ“ - å®Œæ•´å®ç°"""

import re
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    results: List[Dict] = field(default_factory=list)
    clarification: Optional[str] = None
    log: List[Tuple[str, int]] = field(default_factory=list)
    elapsed_ms: float = 0

class EightLayerRetrieval:
    """8å±‚æ£€ç´¢é˜²å¾¡ - ç¡®ä¿100%å¬å›"""
    
    def __init__(self, engine, lightweight: bool = False):
        self.engine = engine
        self.lightweight = lightweight  # è½»é‡æ¨¡å¼è·³è¿‡å‘é‡æ£€ç´¢
    
    def retrieve(self, query: str, context: dict) -> RetrievalResult:
        all_results = []
        retrieval_log = []  # è®°å½•æ¯å±‚å‘½ä¸­æƒ…å†µï¼Œä¾¿äºè°ƒè¯•
        
        # === ç¬¬1å±‚ï¼šç²¾ç¡®åŒ¹é… ===
        exact_hits = self.exact_match(query)
        all_results.extend(exact_hits)
        retrieval_log.append(('exact', len(exact_hits)))
        
        # === ç¬¬2å±‚ï¼šåˆ«ååŒ¹é… ===
        alias_hits = self.alias_match(query)
        all_results.extend(alias_hits)
        retrieval_log.append(('alias', len(alias_hits)))
        
        # === ç¬¬3å±‚ï¼šè§¦å‘ç‰¹å¾ç»„åˆ ===
        trigger_hits = self.trigger_combination_match(query)
        all_results.extend(trigger_hits)
        retrieval_log.append(('trigger', len(trigger_hits)))
        
        # === ç¬¬4å±‚ï¼šå…³ç³»å›¾è°±æ‰©å±• ===
        related_entities = self.expand_relations(all_results)
        all_results.extend(related_entities)
        retrieval_log.append(('relation', len(related_entities)))
        
        # === ç¬¬5å±‚ï¼šæ—¶é—´èŒƒå›´æ‰«æ ===
        time_range = self.parse_time_expression(query, context)
        if time_range:
            time_hits = self.scan_time_range(time_range)
            all_results.extend(time_hits)
            retrieval_log.append(('time', len(time_hits)))
        
        # === ç¬¬6å±‚ï¼šå‘é‡è¯­ä¹‰æ£€ç´¢ï¼ˆè½»é‡æ¨¡å¼è·³è¿‡ï¼‰===
        if not self.lightweight:
            semantic_hits = self.vector_search(query, top_k=20)
            all_results.extend(semantic_hits)
            retrieval_log.append(('vector', len(semantic_hits)))
        else:
            retrieval_log.append(('vector', 0))  # è½»é‡æ¨¡å¼è·³è¿‡
        
        # === ç¬¬7å±‚ï¼šN-gram åŸæ–‡åŒ¹é…ï¼ˆç»ˆæå…œåº•ï¼‰ ===
        if self.need_ngram_fallback(all_results, query):
            ngram_hits = self.ngram_raw_search(query)
            all_results.extend(ngram_hits)
            retrieval_log.append(('ngram', len(ngram_hits)))
        
        # å»é‡ + æ’åº
        all_results = self.deduplicate_and_rank(all_results)
        
        # === ç¬¬8å±‚ï¼šå¼•å¯¼å¼è¿½é—® ===
        clarification = None
        if len(all_results) == 0:
            clarification = self.generate_clarification_question(query)
        
        return RetrievalResult(
            results=all_results,
            clarification=clarification,
            log=retrieval_log
        )
    
    def exact_match(self, query: str) -> List[Dict]:
        """ç¬¬1å±‚ï¼šç²¾ç¡®åŒ¹é…"""
        entity = self.engine.entity_index.get_by_name(query)
        if entity:
            return [{'type': 'entity', 'data': entity, 'score': 1.0, 'layer': 'exact'}]
        return []
    
    def alias_match(self, query: str) -> List[Dict]:
        """ç¬¬2å±‚ï¼šåˆ«ååŒ¹é…"""
        entities = self.engine.entity_index.search(query)
        return [{'type': 'entity', 'data': e, 'score': 0.9, 'layer': 'alias'} for e in entities[:10]]
    
    def trigger_combination_match(self, query: str) -> List[Dict]:
        """ç¬¬3å±‚ï¼šè§¦å‘è¯ç»„åˆåŒ¹é…"""
        keywords = self.engine.entity_extractor.extract_keywords(query)
        turn_ids = self.engine.inverted_index.search_any(keywords)
        results = []
        for turn_id in turn_ids[:20]:
            turn = self.engine.volume_manager.get_turn(turn_id)
            if turn:
                results.append({'type': 'turn', 'data': turn, 'score': 0.7, 'layer': 'trigger'})
        return results
    
    def expand_relations(self, current_results: List[Dict]) -> List[Dict]:
        """ç¬¬4å±‚ï¼šå…³ç³»æ‰©å±•"""
        expanded = []
        seen_ids = set()
        
        for result in current_results:
            if result['type'] == 'entity':
                entity = result['data']
                if hasattr(entity, 'relations'):
                    for rel in entity.relations:
                        related_id = rel.get('target_id') if isinstance(rel, dict) else getattr(rel, 'target_id', None)
                        if related_id and related_id not in seen_ids:
                            related = self.engine.entity_index.get_by_id(related_id)
                            if related:
                                expanded.append({'type': 'entity', 'data': related, 'score': 0.6, 'layer': 'relation'})
                                seen_ids.add(related_id)
        return expanded[:10]
    
    def parse_time_expression(self, query: str, context: dict) -> Optional[Tuple[int, int]]:
        """è§£ææ—¶é—´è¡¨è¾¾å¼"""
        current_turn = context.get('current_turn', 0)
        
        if 'æœ€å¼€å§‹' in query or 'ä¸€å¼€å§‹' in query:
            return (0, min(100, current_turn))
        if 'æœ€è¿‘' in query:
            return (max(0, current_turn - 50), current_turn)
        if 'æ˜¨å¤©' in query or 'å‰å‡ å¤©' in query:
            return (max(0, current_turn - 200), max(0, current_turn - 50))
        
        # åŒ¹é… "ç¬¬Xè½®" æˆ– "Xè½®å‰"
        match = re.search(r'ç¬¬(\d+)è½®', query)
        if match:
            turn = int(match.group(1))
            return (max(0, turn - 5), turn + 5)
        
        return None
    
    def scan_time_range(self, time_range: Tuple[int, int]) -> List[Dict]:
        """ç¬¬5å±‚ï¼šæ—¶é—´èŒƒå›´æ‰«æ"""
        start, end = time_range
        results = []
        for turn_id in range(start, min(end + 1, start + 100)):  # é™åˆ¶æ‰«ææ•°é‡
            turn = self.engine.volume_manager.get_turn(turn_id)
            if turn:
                results.append({'type': 'turn', 'data': turn, 'score': 0.5, 'layer': 'time'})
        return results
    
    def vector_search(self, query: str, top_k: int = 20) -> List[Dict]:
        """ç¬¬6å±‚ï¼šå‘é‡è¯­ä¹‰æ£€ç´¢"""
        results = []
        search_results = self.engine.vector_index.search(query, top_k)
        for turn_id, score in search_results:
            turn = self.engine.volume_manager.get_turn(turn_id)
            if turn:
                results.append({'type': 'turn', 'data': turn, 'score': float(score), 'layer': 'vector'})
        return results
    
    def ngram_raw_search(self, query: str) -> List[Dict]:
        """ç¬¬7å±‚ï¼šN-gramå…œåº•æœç´¢"""
        turn_ids = self.engine.ngram_index.search(query)
        results = []
        for turn_id in turn_ids[:50]:
            turn = self.engine.volume_manager.get_turn(turn_id)
            if turn:
                # ç²¾ç¡®éªŒè¯
                content = str(turn.get('user', '')) + str(turn.get('assistant', ''))
                if query in content:
                    results.append({'type': 'turn', 'data': turn, 'score': 0.3, 'layer': 'ngram'})
        return results
    
    def generate_clarification_question(self, query: str) -> str:
        """ç¬¬8å±‚ï¼šç”Ÿæˆè¿½é—®"""
        return f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äºã€Œ{query}ã€çš„ç›¸å…³è®°å¿†ã€‚ä½ èƒ½æä¾›æ›´å¤šç»†èŠ‚å—ï¼Ÿæ¯”å¦‚å¤§æ¦‚æ˜¯ä»€ä¹ˆæ—¶å€™çš„äº‹ï¼Ÿ"
    
    def need_ngram_fallback(self, current_results, query) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘N-gramå…œåº•"""
        if not current_results:
            return True
        if '"' in query or '"' in query:
            return True
        # è·å–åˆ†æ•°
        scores = [r.get('score', 0) for r in current_results]
        if max(scores) < 0.5:
            return True
        return False
    
    def deduplicate_and_rank(self, results: List[Dict]) -> List[Dict]:
        """å»é‡å¹¶æ’åº"""
        seen = set()
        unique = []
        for r in results:
            # ç”Ÿæˆå”¯ä¸€é”®
            if r['type'] == 'turn':
                key = ('turn', r['data'].get('turn', id(r)))
            else:
                key = ('entity', getattr(r['data'], 'id', id(r)))
            
            if key not in seen:
                seen.add(key)
                unique.append(r)
        
        # æŒ‰åˆ†æ•°æ’åº
        unique.sort(key=lambda x: x.get('score', 0), reverse=True)
        return unique
```

### 3.5 ä¸Šä¸‹æ–‡æ„å»ºå™¨

```python
# recall/retrieval/context_builder.py
"""ä¸Šä¸‹æ–‡æ„å»ºå™¨ - ç»„è£…æœ€ç»ˆå‘é€ç»™LLMçš„ä¸Šä¸‹æ–‡"""

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional

@dataclass
class ContextResult:
    """ä¸Šä¸‹æ–‡ç»“æœ"""
    text: str                    # ç»„è£…å¥½çš„ä¸Šä¸‹æ–‡
    token_count: int = 0
    memories_used: List[Dict] = field(default_factory=list)
    foreshadowing_included: List = field(default_factory=list)
    needs_clarification: bool = False
    clarification_suggestions: List[str] = field(default_factory=list)

class ContextBuilder:
    """ä¸Šä¸‹æ–‡æ„å»ºå™¨"""
    
    def __init__(self, engine):
        self.engine = engine
    
    def build(
        self,
        user_input: str,
        retrieved: 'RetrievalResult',
        max_tokens: int = 8000
    ) -> ContextResult:
        """æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡"""
        parts = []
        memories_used = []
        token_count = 0
        
        # 1. L0æ ¸å¿ƒè®¾å®šï¼ˆå¿…é¡»åŒ…å«ï¼‰
        scenario = self.engine.scenario_detector.detect(user_input, [])
        l0_text = self.engine.core_settings.get_injection_text(scenario)
        if l0_text:
            parts.append(f"ã€æ ¸å¿ƒè®¾å®šã€‘\n{l0_text}")
            token_count += self._estimate_tokens(l0_text)
        
        # 2. æ´»è·ƒä¼ç¬”ä¸Šä¸‹æ–‡
        fsh_context = self.engine.foreshadowing_tracker.get_context_for_prompt(
            user_id=context.get('user_id')
        )
        if fsh_context:
            parts.append(fsh_context)
            token_count += self._estimate_tokens(fsh_context)
        
        # 3. æ£€ç´¢åˆ°çš„ç›¸å…³è®°å¿†ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰
        remaining_budget = max_tokens - token_count - 500  # ç•™500ç»™ç”¨æˆ·è¾“å…¥
        memory_text = self._format_memories(retrieved.results, remaining_budget)
        if memory_text:
            parts.append(f"ã€ç›¸å…³è®°å¿†ã€‘\n{memory_text}")
            memories_used = retrieved.results[:20]
            token_count += self._estimate_tokens(memory_text)
        
        # 4. å·¥ä½œè®°å¿†ä¸­çš„æ´»è·ƒå®ä½“
        active_entities = self.engine.working.get_active_entities(limit=10)
        if active_entities:
            entity_text = self._format_active_entities(active_entities)
            parts.append(f"ã€å½“å‰ç„¦ç‚¹ã€‘\n{entity_text}")
            token_count += self._estimate_tokens(entity_text)
        
        # 5. è§„èŒƒæé†’
        rules = self.engine.core_settings.absolute_rules
        if rules:
            rules_text = "ã€è¯·åŠ¡å¿…éµå®ˆã€‘\n" + "\n".join(f"- {r}" for r in rules[:5])
            parts.append(rules_text)
        
        # ç»„è£…
        full_text = "\n\n".join(parts)
        
        return ContextResult(
            text=full_text,
            token_count=token_count,
            memories_used=memories_used,
            foreshadowing_included=relevant_fs.split('\n') if relevant_fs else [],
            needs_clarification=retrieved.clarification is not None,
            clarification_suggestions=[retrieved.clarification] if retrieved.clarification else [],
        )
    
    def _format_memories(self, results: List[Dict], max_tokens: int) -> str:
        """æ ¼å¼åŒ–è®°å¿†ï¼Œä¸è¶…è¿‡tokené¢„ç®—"""
        lines = []
        current_tokens = 0
        
        for r in results:
            if r['type'] == 'turn':
                turn = r['data']
                line = f"[ç¬¬{turn.get('turn', '?')}è½®] {turn.get('user', '')[:100]}... â†’ {turn.get('assistant', '')[:100]}..."
            else:
                entity = r['data']
                name = getattr(entity, 'name', str(entity))
                line = f"[å®ä½“] {name}"
            
            line_tokens = self._estimate_tokens(line)
            if current_tokens + line_tokens > max_tokens:
                break
            
            lines.append(line)
            current_tokens += line_tokens
        
        return "\n".join(lines)
    
    def _format_active_entities(self, entities) -> str:
        """æ ¼å¼åŒ–æ´»è·ƒå®ä½“"""
        return ", ".join(e.name for e in entities[:10])
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°ï¼ˆç®€å•æ–¹æ³•ï¼šä¸­æ–‡1å­—=1.5tokenï¼Œè‹±æ–‡1è¯=1tokenï¼‰"""
        # ç®€åŒ–ä¼°ç®—
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        other_chars = len(text) - chinese_chars
        return int(chinese_chars * 1.5 + other_chars * 0.3)
```

---

## ä¸‰ç‚¹å…­ã€è®°å¿†æ™ºèƒ½æ€»ç»“ï¼ˆå¯¹æ ‡ mem0 çš„æ ¸å¿ƒèƒ½åŠ›ï¼‰

> **è¿™æ˜¯ mem0 çš„æ ¸å¿ƒåŠŸèƒ½**ï¼šè‡ªåŠ¨ä»å¯¹è¯ä¸­æå–å…³é”®è®°å¿†ï¼Œå½¢æˆç»“æ„åŒ–æ€»ç»“ã€‚
> Recall åŒæ ·æ”¯æŒï¼Œä½†æœ‰ä¸€ä¸ªå…³é”®åŒºåˆ«ï¼š**åŸæ–‡æ°¸ä¸ä¸¢å¼ƒ**ã€‚

```python
# recall/processor/memory_summarizer.py
"""è®°å¿†æ™ºèƒ½æ€»ç»“ - ä»å¯¹è¯ä¸­è‡ªåŠ¨æå–å…³é”®è®°å¿†"""

import json
from typing import List, Dict, Optional
from dataclasses import dataclass, field

@dataclass
class MemoryItem:
    """ä¸€æ¡è®°å¿†"""
    id: str
    content: str              # è®°å¿†å†…å®¹
    category: str             # ç±»åˆ«ï¼šFACT, PREFERENCE, EVENT, RELATION
    entities: List[str]       # æ¶‰åŠçš„å®ä½“
    source_turn: int          # æ¥æºè½®æ¬¡
    confidence: float = 0.5
    created_at: str = ""

class MemorySummarizer:
    """è®°å¿†æ€»ç»“å™¨ - ç±»ä¼¼ mem0 çš„ add() åŠŸèƒ½"""
    
    # æå–æç¤ºè¯ï¼ˆä¸­æ–‡ä¼˜åŒ–ï¼‰
    EXTRACTION_PROMPT = '''è¯·ä»ä»¥ä¸‹å¯¹è¯ä¸­æå–å…³é”®è®°å¿†ã€‚

å¯¹è¯å†…å®¹ï¼š
ç”¨æˆ·ï¼š{user}
AIï¼š{assistant}

è¯·æå–ä»¥ä¸‹ç±»å‹çš„è®°å¿†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼š
1. FACT - äº‹å®ä¿¡æ¯ï¼ˆå¦‚ï¼šè§’è‰²çš„å¹´é¾„ã€èŒä¸šã€èƒ½åŠ›ï¼‰
2. PREFERENCE - åå¥½ï¼ˆå¦‚ï¼šå–œæ¬¢ä»€ä¹ˆã€è®¨åŒä»€ä¹ˆï¼‰
3. EVENT - å‘ç”Ÿçš„äº‹ä»¶ï¼ˆå¦‚ï¼šå»äº†æŸåœ°ã€åšäº†æŸäº‹ï¼‰
4. RELATION - å…³ç³»ï¼ˆå¦‚ï¼šAæ˜¯Bçš„æœ‹å‹ï¼‰

ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ¯æ¡è®°å¿†ä¸€è¡Œï¼š
[
  {"content": "è®°å¿†å†…å®¹", "category": "FACT", "entities": ["å®ä½“1", "å®ä½“2"]},
  ...
]

å¦‚æœæ²¡æœ‰å€¼å¾—è®°å¿†çš„å†…å®¹ï¼Œè¿”å›ç©ºæ•°ç»„ï¼š[]
'''
    
    def __init__(self, llm_client=None, data_path: str = None):
        self.llm_client = llm_client
        self.data_path = data_path
        self.memories: List[MemoryItem] = []
        self._load()
    
    def _load(self):
        """åŠ è½½å·²æœ‰è®°å¿†"""
        if self.data_path:
            import os
            memory_file = os.path.join(self.data_path, 'memories.json')
            if os.path.exists(memory_file):
                with open(memory_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.memories = [MemoryItem(**item) for item in data]
    
    def _save(self):
        """ä¿å­˜è®°å¿†"""
        if self.data_path:
            import os
            from dataclasses import asdict
            memory_file = os.path.join(self.data_path, 'memories.json')
            os.makedirs(os.path.dirname(memory_file), exist_ok=True)
            with open(memory_file, 'w', encoding='utf-8') as f:
                json.dump([asdict(m) for m in self.memories], f, ensure_ascii=False, indent=2)
    
    def add(self, user_input: str, assistant_output: str, turn: int, 
            use_llm: bool = True) -> List[MemoryItem]:
        """
        ä»å¯¹è¯ä¸­æå–è®°å¿†ï¼ˆç±»ä¼¼ mem0.add()ï¼‰
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            assistant_output: AIè¾“å‡º
            turn: å½“å‰è½®æ¬¡
            use_llm: æ˜¯å¦ä½¿ç”¨LLMæå–ï¼ˆå¦åˆ™ä½¿ç”¨è§„åˆ™ï¼‰
        
        Returns:
            æå–çš„è®°å¿†åˆ—è¡¨
        """
        if use_llm and self.llm_client:
            return self._extract_with_llm(user_input, assistant_output, turn)
        else:
            return self._extract_with_rules(user_input, assistant_output, turn)
    
    def _extract_with_llm(self, user: str, assistant: str, turn: int) -> List[MemoryItem]:
        """ä½¿ç”¨LLMæå–è®°å¿†"""
        import uuid
        from datetime import datetime
        
        prompt = self.EXTRACTION_PROMPT.format(user=user, assistant=assistant)
        
        try:
            response = self.llm_client.chat.completions.create(
                model="gpt-4o-mini",  # ç”¨å°æ¨¡å‹èŠ‚çœæˆæœ¬
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
            )
            
            content = response.choices[0].message.content
            # è§£æJSON
            import re
            json_match = re.search(r'\[.*\]', content, re.DOTALL)
            if json_match:
                items = json.loads(json_match.group())
                
                new_memories = []
                for item in items:
                    memory = MemoryItem(
                        id=str(uuid.uuid4()),
                        content=item.get('content', ''),
                        category=item.get('category', 'FACT'),
                        entities=item.get('entities', []),
                        source_turn=turn,
                        confidence=0.8,
                        created_at=datetime.now().isoformat()
                    )
                    
                    # å»é‡æ£€æŸ¥
                    if not self._is_duplicate(memory):
                        self.memories.append(memory)
                        new_memories.append(memory)
                
                self._save()
                return new_memories
        
        except Exception as e:
            print(f"[Recall] LLMè®°å¿†æå–å¤±è´¥: {e}")
        
        # é™çº§åˆ°è§„åˆ™æå–
        return self._extract_with_rules(user, assistant, turn)
    
    def _extract_with_rules(self, user: str, assistant: str, turn: int) -> List[MemoryItem]:
        """ä½¿ç”¨è§„åˆ™æå–è®°å¿†ï¼ˆä¸ä¾èµ–LLMï¼‰"""
        import uuid
        import re
        from datetime import datetime
        
        new_memories = []
        combined = user + " " + assistant
        
        # è§„åˆ™1ï¼šæå–"æ˜¯"å­—å¥ï¼ˆäº‹å®ï¼‰
        is_patterns = re.findall(r'(.{2,10})æ˜¯(.{2,20})', combined)
        for subj, obj in is_patterns:
            memory = MemoryItem(
                id=str(uuid.uuid4()),
                content=f"{subj}æ˜¯{obj}",
                category='FACT',
                entities=[subj.strip()],
                source_turn=turn,
                confidence=0.5,
                created_at=datetime.now().isoformat()
            )
            if not self._is_duplicate(memory):
                self.memories.append(memory)
                new_memories.append(memory)
        
        # è§„åˆ™2ï¼šæå–"å–œæ¬¢/è®¨åŒ"ï¼ˆåå¥½ï¼‰
        pref_patterns = re.findall(r'(.{2,10})(å–œæ¬¢|è®¨åŒ|çˆ±|æ¨)(.{2,20})', combined)
        for subj, verb, obj in pref_patterns:
            memory = MemoryItem(
                id=str(uuid.uuid4()),
                content=f"{subj}{verb}{obj}",
                category='PREFERENCE',
                entities=[subj.strip(), obj.strip()],
                source_turn=turn,
                confidence=0.6,
                created_at=datetime.now().isoformat()
            )
            if not self._is_duplicate(memory):
                self.memories.append(memory)
                new_memories.append(memory)
        
        self._save()
        return new_memories
    
    def _is_duplicate(self, new_memory: MemoryItem) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤"""
        for existing in self.memories:
            # ç®€å•çš„å†…å®¹ç›¸ä¼¼åº¦æ£€æŸ¥
            if new_memory.content == existing.content:
                return True
            if (new_memory.category == existing.category and 
                set(new_memory.entities) == set(existing.entities)):
                return True
        return False
    
    def search(self, query: str, limit: int = 10) -> List[MemoryItem]:
        """æœç´¢è®°å¿†ï¼ˆç±»ä¼¼ mem0.search()ï¼‰"""
        results = []
        query_lower = query.lower()
        
        for memory in self.memories:
            score = 0
            
            # å†…å®¹åŒ¹é…
            if query_lower in memory.content.lower():
                score += 2
            
            # å®ä½“åŒ¹é…
            for entity in memory.entities:
                if query_lower in entity.lower():
                    score += 1
            
            if score > 0:
                results.append((score, memory))
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x[0], reverse=True)
        return [m for _, m in results[:limit]]
    
    def get_all(self, user_id: str = None) -> List[MemoryItem]:
        """è·å–æ‰€æœ‰è®°å¿†ï¼ˆç±»ä¼¼ mem0.get_all()ï¼‰"""
        return self.memories
    
    def delete(self, memory_id: str):
        """åˆ é™¤è®°å¿†"""
        self.memories = [m for m in self.memories if m.id != memory_id]
        self._save()
```

### ä¸ mem0 çš„ API å…¼å®¹

```python
# recall/compat/mem0_compat.py
"""mem0 å…¼å®¹å±‚ - è®©ç†Ÿæ‚‰ mem0 çš„ç”¨æˆ·æ— ç¼è¿ç§»"""

class Memory:
    """mem0 å…¼å®¹æ¥å£"""
    
    def __init__(self, **kwargs):
        from ..processor.memory_summarizer import MemorySummarizer
        self.summarizer = MemorySummarizer(**kwargs)
    
    def add(self, messages: list, user_id: str = "default", **kwargs):
        """mem0 å…¼å®¹çš„ add æ–¹æ³•"""
        # è§£æ messages æ ¼å¼
        user_msg = ""
        assistant_msg = ""
        for msg in messages:
            if msg.get('role') == 'user':
                user_msg = msg.get('content', '')
            elif msg.get('role') == 'assistant':
                assistant_msg = msg.get('content', '')
        
        return self.summarizer.add(user_msg, assistant_msg, turn=0)
    
    def search(self, query: str, user_id: str = "default", limit: int = 10, **kwargs):
        """mem0 å…¼å®¹çš„ search æ–¹æ³•"""
        results = self.summarizer.search(query, limit)
        return {"results": [{"memory": m.content} for m in results]}
    
    def get_all(self, user_id: str = "default", **kwargs):
        """mem0 å…¼å®¹çš„ get_all æ–¹æ³•"""
        return {"results": [{"memory": m.content} for m in self.summarizer.get_all()]}


# ä½¿ç”¨ç¤ºä¾‹ï¼ˆä¸ mem0 å®Œå…¨ç›¸åŒçš„ä»£ç ï¼‰ï¼š
# from recall.compat.mem0_compat import Memory
# memory = Memory()
# memory.add(messages, user_id="user123")
# results = memory.search("query", user_id="user123")
```

---

## å››ã€ä¼ç¬”è¿½è¸ªç³»ç»Ÿï¼ˆMANUAL + LLM è¾…åŠ©ï¼‰

> **è®¾è®¡ç†å¿µ**ï¼šæ‰‹åŠ¨æ“ä½œå§‹ç»ˆå¯ç”¨ï¼ŒLLM åªæ˜¯è¾…åŠ©æ£€æµ‹ã€‚ç”¨æˆ·éšæ—¶å¯ä»¥æ‰‹åŠ¨æ·»åŠ /ç¼–è¾‘/åˆ é™¤ä¼ç¬”ã€‚
> 
> | æ¨¡å¼ | æ‰‹åŠ¨æ“ä½œ | è‡ªåŠ¨æ£€æµ‹ | è¯´æ˜ |
> |------|:--------:|:--------:|------|
> | **MANUAL**ï¼ˆé»˜è®¤ï¼‰ | âœ… | âŒ | ç”¨æˆ·è‡ªå·±ç®¡ç†ä¼ç¬” |
> | **LLM** | âœ… | âœ… | æ‰‹åŠ¨ + LLM è¾…åŠ©æ£€æµ‹ |

### 4.1 ä¼ç¬”æ•°æ®ç»“æ„

```python
# recall/processor/foreshadowing.py
"""ä¼ç¬”è¿½è¸ª - MANUAL + LLM è¾…åŠ©è®¾è®¡"""

from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any
from datetime import datetime
from enum import Enum

class ForeshadowingStatus(Enum):
    """ä¼ç¬”çŠ¶æ€"""
    ACTIVE = "active"           # æ´»è·ƒï¼ˆæœªè§£å†³ï¼‰
    RESOLVED = "resolved"       # å·²è§£å†³
    ARCHIVED = "archived"       # å·²å½’æ¡£ï¼ˆä¸å†è¿½è¸ªï¼‰

@dataclass
class Foreshadowing:
    """ä¼ç¬”è®°å½•"""
    id: str
    
    # åŸºæœ¬ä¿¡æ¯
    content: str                # ä¼ç¬”å†…å®¹æè¿°
    created_at: datetime        # åˆ›å»ºæ—¶é—´
    created_turn: int           # åˆ›å»ºè½®æ¬¡
    user_id: str = "default"    # æ‰€å±ç”¨æˆ·
    
    # çŠ¶æ€
    status: ForeshadowingStatus = ForeshadowingStatus.ACTIVE
    resolved_at: Optional[datetime] = None
    resolved_turn: Optional[int] = None
    resolution_note: Optional[str] = None  # è§£å†³è¯´æ˜
    
    # å…ƒæ•°æ®
    importance: float = 0.5     # é‡è¦æ€§ 0-1
    related_entities: List[str] = field(default_factory=list)  # ç›¸å…³è§’è‰²/ç‰©å“
    tags: List[str] = field(default_factory=list)              # æ ‡ç­¾
    
    # æé†’æœºåˆ¶
    remind_after_turns: int = 100  # å¤šå°‘è½®åæé†’
    last_reminded_turn: Optional[int] = None
    
    # LLM æ£€æµ‹æ¥æºï¼ˆå¦‚æœæ˜¯ LLM è‡ªåŠ¨æ£€æµ‹çš„ï¼‰
    detected_by: str = "manual"  # "manual" | "llm"
    detection_evidence: Optional[str] = None  # LLM æ£€æµ‹çš„åŸæ–‡ä¾æ®
```

### 4.2 ä¼ç¬”è¿½è¸ªå™¨ï¼ˆæ‰‹åŠ¨ç®¡ç†æ ¸å¿ƒï¼‰

```python
class ForeshadowingTracker:
    """ä¼ç¬”è¿½è¸ªå™¨ - æ‰‹åŠ¨ç®¡ç†ä¸ºä¸»ï¼ŒLLM è¾…åŠ©ä¸ºè¾…"""
    
    def __init__(self, storage_path: str = None):
        self.storage_path = storage_path
        self._foreshadowings: Dict[str, Foreshadowing] = {}
        self._load()
    
    # ==================== æ‰‹åŠ¨æ“ä½œ APIï¼ˆå§‹ç»ˆå¯ç”¨ï¼‰ ====================
    
    def plant(
        self,
        content: str,
        user_id: str = "default",
        importance: float = 0.5,
        related_entities: List[str] = None,
        tags: List[str] = None,
        current_turn: int = 0
    ) -> Foreshadowing:
        """
        æ‰‹åŠ¨åŸ‹ä¸‹ä¼ç¬”
        
        Args:
            content: ä¼ç¬”å†…å®¹æè¿°
            user_id: ç”¨æˆ·ID
            importance: é‡è¦æ€§ 0-1
            related_entities: ç›¸å…³è§’è‰²/ç‰©å“
            tags: æ ‡ç­¾
            current_turn: å½“å‰è½®æ¬¡
            
        Returns:
            åˆ›å»ºçš„ä¼ç¬”å¯¹è±¡
        """
        fsh_id = f"fsh_{user_id}_{datetime.now().strftime('%Y%m%d%H%M%S')}_{len(self._foreshadowings)}"
        
        foreshadowing = Foreshadowing(
            id=fsh_id,
            content=content,
            created_at=datetime.now(),
            created_turn=current_turn,
            user_id=user_id,
            importance=importance,
            related_entities=related_entities or [],
            tags=tags or [],
            detected_by="manual"
        )
        
        self._foreshadowings[fsh_id] = foreshadowing
        self._save()
        return foreshadowing
    
    def resolve(
        self,
        fsh_id: str,
        resolution_note: str = None,
        current_turn: int = 0
    ) -> Optional[Foreshadowing]:
        """
        æ‰‹åŠ¨æ ‡è®°ä¼ç¬”å·²è§£å†³
        
        Args:
            fsh_id: ä¼ç¬”ID
            resolution_note: è§£å†³è¯´æ˜
            current_turn: å½“å‰è½®æ¬¡
            
        Returns:
            æ›´æ–°åçš„ä¼ç¬”å¯¹è±¡ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        if fsh_id not in self._foreshadowings:
            return None
        
        fsh = self._foreshadowings[fsh_id]
        fsh.status = ForeshadowingStatus.RESOLVED
        fsh.resolved_at = datetime.now()
        fsh.resolved_turn = current_turn
        fsh.resolution_note = resolution_note
        
        self._save()
        return fsh
    
    def update(
        self,
        fsh_id: str,
        content: str = None,
        importance: float = None,
        related_entities: List[str] = None,
        tags: List[str] = None
    ) -> Optional[Foreshadowing]:
        """æ‰‹åŠ¨æ›´æ–°ä¼ç¬”ä¿¡æ¯"""
        if fsh_id not in self._foreshadowings:
            return None
        
        fsh = self._foreshadowings[fsh_id]
        if content is not None:
            fsh.content = content
        if importance is not None:
            fsh.importance = importance
        if related_entities is not None:
            fsh.related_entities = related_entities
        if tags is not None:
            fsh.tags = tags
        
        self._save()
        return fsh
    
    def delete(self, fsh_id: str) -> bool:
        """æ‰‹åŠ¨åˆ é™¤ä¼ç¬”"""
        if fsh_id in self._foreshadowings:
            del self._foreshadowings[fsh_id]
            self._save()
            return True
        return False
    
    def archive(self, fsh_id: str) -> Optional[Foreshadowing]:
        """å½’æ¡£ä¼ç¬”ï¼ˆä¸å†è¿½è¸ªä½†ä¿ç•™è®°å½•ï¼‰"""
        if fsh_id not in self._foreshadowings:
            return None
        
        fsh = self._foreshadowings[fsh_id]
        fsh.status = ForeshadowingStatus.ARCHIVED
        self._save()
        return fsh
    
    # ==================== æŸ¥è¯¢ API ====================
    
    def get(self, fsh_id: str) -> Optional[Foreshadowing]:
        """è·å–å•ä¸ªä¼ç¬”"""
        return self._foreshadowings.get(fsh_id)
    
    def get_active(self, user_id: str = None) -> List[Foreshadowing]:
        """è·å–æ‰€æœ‰æ´»è·ƒä¼ç¬”"""
        result = [
            fsh for fsh in self._foreshadowings.values()
            if fsh.status == ForeshadowingStatus.ACTIVE
        ]
        if user_id:
            result = [fsh for fsh in result if fsh.user_id == user_id]
        return sorted(result, key=lambda x: x.importance, reverse=True)
    
    def get_resolved(self, user_id: str = None) -> List[Foreshadowing]:
        """è·å–æ‰€æœ‰å·²è§£å†³ä¼ç¬”"""
        result = [
            fsh for fsh in self._foreshadowings.values()
            if fsh.status == ForeshadowingStatus.RESOLVED
        ]
        if user_id:
            result = [fsh for fsh in result if fsh.user_id == user_id]
        return result
    
    def get_all(self, user_id: str = None) -> List[Foreshadowing]:
        """è·å–æ‰€æœ‰ä¼ç¬”"""
        result = list(self._foreshadowings.values())
        if user_id:
            result = [fsh for fsh in result if fsh.user_id == user_id]
        return result
    
    # ==================== æé†’æœºåˆ¶ ====================
    
    def get_reminders(self, current_turn: int, user_id: str = None) -> List[Foreshadowing]:
        """è·å–éœ€è¦æé†’çš„ä¼ç¬”ï¼ˆé•¿æœŸæœªè§£å†³ï¼‰"""
        reminders = []
        
        for fsh in self.get_active(user_id):
            turns_since_creation = current_turn - fsh.created_turn
            turns_since_remind = current_turn - (fsh.last_reminded_turn or fsh.created_turn)
            
            # è¶…è¿‡æé†’é˜ˆå€¼ï¼Œä¸”è·ç¦»ä¸Šæ¬¡æé†’è¶³å¤Ÿä¹…
            if turns_since_creation > fsh.remind_after_turns and turns_since_remind > 50:
                reminders.append(fsh)
                fsh.last_reminded_turn = current_turn
        
        if reminders:
            self._save()
        
        return reminders
    
    def get_context_for_prompt(self, user_id: str = None, max_count: int = 5) -> str:
        """ç”Ÿæˆç”¨äºæ³¨å…¥ prompt çš„ä¼ç¬”ä¸Šä¸‹æ–‡"""
        active = self.get_active(user_id)[:max_count]
        
        if not active:
            return ""
        
        lines = ["ã€å½“å‰æ´»è·ƒçš„ä¼ç¬”ã€‘"]
        for fsh in active:
            importance_str = "â­" * int(fsh.importance * 3 + 1)
            lines.append(f"- {importance_str} {fsh.content}")
            if fsh.related_entities:
                lines.append(f"  ç›¸å…³ï¼š{', '.join(fsh.related_entities)}")
        
        return "\n".join(lines)
    
    # ==================== æŒä¹…åŒ– ====================
    
    def _load(self):
        """ä»å­˜å‚¨åŠ è½½"""
        if not self.storage_path:
            return
        # å®é™…å®ç°ï¼šä» JSON æ–‡ä»¶åŠ è½½
        pass
    
    def _save(self):
        """ä¿å­˜åˆ°å­˜å‚¨"""
        if not self.storage_path:
            return
        # å®é™…å®ç°ï¼šä¿å­˜åˆ° JSON æ–‡ä»¶
        pass
```

### 4.3 LLM ä¼ç¬”åˆ†æå™¨ï¼ˆå¯é€‰è¾…åŠ©åŠŸèƒ½ï¼‰

```python
from enum import Enum
from dataclasses import dataclass
from typing import Optional, List, Dict, Any

class AnalyzerBackend(Enum):
    """åˆ†æå™¨åç«¯ç±»å‹"""
    MANUAL = "manual"  # æ‰‹åŠ¨æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
    LLM = "llm"        # LLM æ™ºèƒ½åˆ†æï¼ˆéœ€é…ç½® APIï¼‰

@dataclass
class ForeshadowingAnalyzerConfig:
    """ä¼ç¬”åˆ†æå™¨é…ç½®"""
    # åç«¯é€‰æ‹©ï¼ˆé»˜è®¤ MANUAL = æ‰‹åŠ¨æ¨¡å¼ï¼Œéœ€é…ç½® API æ‰èƒ½å¯ç”¨ LLM åˆ†æï¼‰
    backend: AnalyzerBackend = AnalyzerBackend.MANUAL
    
    # è§¦å‘æ¡ä»¶ï¼ˆLLM æ¨¡å¼ï¼‰
    trigger_interval: int = 10      # æ¯Nè½®è§¦å‘ä¸€æ¬¡åˆ†æï¼ˆæœ€å°1=æ¯è½®éƒ½è§¦å‘ï¼‰
    
    # LLM é…ç½®
    llm_model: str = "gpt-4o-mini"  # é»˜è®¤ç”¨ä¾¿å®œçš„æ¨¡å‹
    llm_api_key: Optional[str] = None
    llm_base_url: Optional[str] = None  # æ”¯æŒè‡ªå®šä¹‰ API åœ°å€
    
    # è¡Œä¸ºé…ç½®
    auto_plant: bool = True         # è‡ªåŠ¨åŸ‹ä¸‹æ£€æµ‹åˆ°çš„ä¼ç¬”
    auto_resolve: bool = False      # è‡ªåŠ¨æ ‡è®°è§£å†³ï¼ˆå»ºè®® Falseï¼Œè®©ç”¨æˆ·ç¡®è®¤ï¼‰
    
    # é«˜çº§é…ç½®
    max_context_turns: int = 20     # å‘é€ç»™ LLM çš„æœ€å¤§è½®æ¬¡æ•°
    
    @classmethod
    def manual(cls) -> 'ForeshadowingAnalyzerConfig':
        """æ‰‹åŠ¨æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰- ç”¨æˆ·è‡ªå·±ç®¡ç†ä¼ç¬”"""
        return cls(backend=AnalyzerBackend.MANUAL)
    
    @classmethod
    def llm_based(
        cls, 
        api_key: str, 
        model: str = "gpt-4o-mini",
        trigger_interval: int = 10
    ) -> 'ForeshadowingAnalyzerConfig':
        """ä½¿ç”¨ LLM APIï¼ˆæ™ºèƒ½è¾…åŠ©ï¼‰"""
        return cls(
            backend=AnalyzerBackend.LLM,
            llm_api_key=api_key,
            llm_model=model,
            trigger_interval=trigger_interval
        )


class ForeshadowingAnalyzer:
    """ä¼ç¬”åˆ†æå™¨ - æ‰‹åŠ¨æ¨¡å¼ / LLM æ™ºèƒ½è¾…åŠ©"""
    
    # LLM æç¤ºè¯æ¨¡æ¿
    ANALYSIS_PROMPT = '''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å™äº‹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œè¯†åˆ«å…¶ä¸­çš„ä¼ç¬”ï¼ˆforeshadowingï¼‰ã€‚

## ä»€ä¹ˆæ˜¯ä¼ç¬”ï¼Ÿ
ä¼ç¬”æ˜¯æ•…äº‹ä¸­åŸ‹ä¸‹çš„çº¿ç´¢ï¼Œæš—ç¤ºæœªæ¥ä¼šå‘ç”Ÿçš„äº‹æƒ…ï¼ŒåŒ…æ‹¬ï¼š
- ç¥ç§˜çš„æš—ç¤ºæˆ–é¢„è¨€
- æœªè§£é‡Šçš„äº‹ä»¶æˆ–ç°è±¡
- è§’è‰²æåˆ°çš„"æœ‰ä¸€å¤©ä¼š..."
- éšè—çš„ç§˜å¯†æˆ–è°œå›¢
- ä¸ç¥¥çš„å¾å…†

## å½“å‰æ´»è·ƒçš„ä¼ç¬”ï¼ˆå¦‚æœæœ‰ï¼‰ï¼š
{active_foreshadowings}

## æœ€è¿‘çš„å¯¹è¯å†…å®¹ï¼š
{conversation}

## è¯·è¾“å‡º JSON æ ¼å¼ï¼š
```json
{
  "new_foreshadowings": [
    {
      "content": "ä¼ç¬”å†…å®¹æè¿°",
      "importance": 0.8,
      "evidence": "åŸæ–‡ä¾æ®ï¼ˆå¼•ç”¨å¯¹è¯ä¸­çš„å¥å­ï¼‰",
      "related_entities": ["è§’è‰²A", "ç‰©å“B"]
    }
  ],
  "potentially_resolved": [
    {
      "foreshadowing_id": "fsh_xxx",
      "evidence": "è§£å†³çš„ä¾æ®",
      "confidence": 0.9
    }
  ]
}
```

åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä¼ç¬”ï¼Œè¿”å›ç©ºæ•°ç»„ã€‚'''

    def __init__(
        self, 
        config: ForeshadowingAnalyzerConfig,
        tracker: ForeshadowingTracker
    ):
        self.config = config
        self.tracker = tracker
        self.llm_client = None
        
        # å¯¹è¯ç¼“å†²åŒºï¼ˆæŒ‰ç”¨æˆ·åˆ†éš”ï¼‰
        self._buffers: Dict[str, List[Dict]] = {}
        self._turn_counters: Dict[str, int] = {}
        
        if config.backend == AnalyzerBackend.LLM:
            self._init_llm_client()
    
    def _init_llm_client(self):
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        # å®é™…å®ç°ï¼šåˆ›å»º OpenAI/å…¶ä»– API å®¢æˆ·ç«¯
        pass
    
    def on_new_turn(
        self, 
        content: str, 
        role: str,
        user_id: str = "default"
    ) -> Optional[Dict[str, Any]]:
        """
        æ¯è½®å¯¹è¯åè°ƒç”¨ï¼Œè¿”å›åˆ†æç»“æœï¼ˆå¦‚æœè§¦å‘äº†åˆ†æï¼‰
        
        âš ï¸ æ‰‹åŠ¨æ¨¡å¼ä¸‹ç›´æ¥è¿”å› Noneï¼Œä¸åšä»»ä½•è‡ªåŠ¨æ£€æµ‹
        """
        if self.config.backend == AnalyzerBackend.MANUAL:
            return None
        
        # LLM æ¨¡å¼ï¼šç´¯ç§¯å¯¹è¯åˆ°ç¼“å†²åŒº
        if user_id not in self._buffers:
            self._buffers[user_id] = []
            self._turn_counters[user_id] = 0
        
        self._buffers[user_id].append({
            'role': role,
            'content': content,
            'turn': self._turn_counters[user_id]
        })
        self._turn_counters[user_id] += 1
        
        # æ£€æŸ¥æ˜¯å¦è§¦å‘åˆ†æ
        if self._should_trigger_analysis(user_id):
            return self._analyze_with_llm(user_id)
        
        return None
    
    def _should_trigger_analysis(self, user_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘åˆ†æ"""
        turn_count = self._turn_counters.get(user_id, 0)
        # trigger_interval=1 è¡¨ç¤ºæ¯è½®éƒ½è§¦å‘ï¼Œ=10 è¡¨ç¤ºæ¯10è½®è§¦å‘ä¸€æ¬¡
        return turn_count > 0 and turn_count % self.config.trigger_interval == 0
    
    def trigger_manual_analysis(self, user_id: str = "default") -> Dict[str, Any]:
        """æ‰‹åŠ¨è§¦å‘ LLM åˆ†æï¼ˆå³ä½¿æ˜¯ MANUAL æ¨¡å¼ä¹Ÿå¯ä»¥ä¸´æ—¶è°ƒç”¨ï¼‰"""
        if not self.llm_client and self.config.llm_api_key:
            self._init_llm_client()
        
        if not self.llm_client:
            return {'error': 'LLM API æœªé…ç½®'}
        
        return self._analyze_with_llm(user_id)
    
    def _analyze_with_llm(self, user_id: str) -> Dict[str, Any]:
        """ä½¿ç”¨ LLM åˆ†æ"""
        buffer = self._buffers.get(user_id, [])
        if not buffer:
            return {'new_foreshadowings': [], 'potentially_resolved': []}
        
        # æ„å»ºå¯¹è¯æ–‡æœ¬
        conversation = self._format_conversation(buffer[-self.config.max_context_turns:])
        
        # è·å–å½“å‰æ´»è·ƒçš„ä¼ç¬”
        active = self.tracker.get_active(user_id)
        active_text = self._format_active_foreshadowings(active)
        
        # æ„å»ºæç¤ºè¯å¹¶è°ƒç”¨ LLM
        prompt = self.ANALYSIS_PROMPT.format(
            active_foreshadowings=active_text or "ï¼ˆæš‚æ— ï¼‰",
            conversation=conversation
        )
        
        try:
            # è°ƒç”¨ LLM APIï¼ˆå®é™…å®ç°ï¼‰
            response = self._call_llm(prompt)
            result = self._parse_llm_response(response)
            
            # å¤„ç†ç»“æœ
            if self.config.auto_plant:
                for fsh in result.get('new_foreshadowings', []):
                    self.tracker.plant(
                        content=fsh['content'],
                        user_id=user_id,
                        importance=fsh.get('importance', 0.5),
                        related_entities=fsh.get('related_entities', [])
                    )
                    # æ ‡è®°ä¸º LLM æ£€æµ‹
                    # fsh.detected_by = "llm"
                    # fsh.detection_evidence = fsh.get('evidence')
            
            # æ¸…ç©ºå·²åˆ†æçš„ç¼“å†²åŒº
            self._buffers[user_id] = []
            
            return result
            
        except Exception as e:
            print(f"[Recall] LLM ä¼ç¬”åˆ†æå¤±è´¥: {e}")
            return {'new_foreshadowings': [], 'potentially_resolved': [], 'error': str(e)}
    
    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨ LLM API"""
        # å®é™…å®ç°ï¼šä½¿ç”¨ OpenAI/å…¶ä»– API
        pass
    
    def _format_conversation(self, turns: List[Dict]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†…å®¹"""
        lines = []
        for t in turns:
            role = "ç”¨æˆ·" if t['role'] == 'user' else "AI"
            lines.append(f"[{role}]: {t['content']}")
        return "\n\n".join(lines)
    
    def _format_active_foreshadowings(self, foreshadowings: List[Foreshadowing]) -> str:
        """æ ¼å¼åŒ–æ´»è·ƒä¼ç¬”åˆ—è¡¨"""
        if not foreshadowings:
            return ""
        lines = []
        for f in foreshadowings:
            lines.append(f"- [{f.id}] {f.content} (é‡è¦æ€§: {f.importance})")
        return "\n".join(lines)
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """è§£æ LLM è¿”å›çš„ JSON"""
        import json
        try:
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0]
            elif "```" in response:
                response = response.split("```")[1].split("```")[0]
            return json.loads(response.strip())
        except:
            return {'new_foreshadowings': [], 'potentially_resolved': []}
```

### 4.4 ä½¿ç”¨ç¤ºä¾‹

```python
# æ–¹å¼1ï¼šçº¯æ‰‹åŠ¨æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
from recall.processor.foreshadowing import ForeshadowingTracker

tracker = ForeshadowingTracker(storage_path="./recall_data/foreshadowing.json")

# æ‰‹åŠ¨åŸ‹ä¼ç¬”
tracker.plant(
    content="è€è€…äº¤ç»™ä¸»è§’ä¸€æŠŠç¥ç§˜é’¥åŒ™ï¼Œè¯´'æ—¶æœºåˆ°äº†ä½ å°±ä¼šçŸ¥é“å®ƒçš„ç”¨é€”'",
    importance=0.9,
    related_entities=["è€è€…", "ç¥ç§˜é’¥åŒ™", "ä¸»è§’"],
    tags=["ç‰©å“", "æ‚¬å¿µ"]
)

# æ‰‹åŠ¨æ ‡è®°è§£å†³
tracker.resolve(
    fsh_id="fsh_xxx",
    resolution_note="ä¸»è§’ç”¨é’¥åŒ™æ‰“å¼€äº†åœ°ä¸‹å®¤çš„é—¨"
)

# è·å–æ´»è·ƒä¼ç¬”ç”¨äº prompt æ³¨å…¥
context = tracker.get_context_for_prompt()


# æ–¹å¼2ï¼šå¯ç”¨ LLM è¾…åŠ©æ£€æµ‹
from recall.processor.foreshadowing import (
    ForeshadowingTracker, 
    ForeshadowingAnalyzer,
    ForeshadowingAnalyzerConfig
)

tracker = ForeshadowingTracker(storage_path="./recall_data/foreshadowing.json")
analyzer = ForeshadowingAnalyzer(
    config=ForeshadowingAnalyzerConfig.llm_based(
        api_key="sk-xxx",
        model="gpt-4o-mini",
        trigger_interval=10  # æ¯10è½®è‡ªåŠ¨åˆ†æä¸€æ¬¡
    ),
    tracker=tracker
)

# æ¯è½®å¯¹è¯åè°ƒç”¨ï¼ˆLLM æ¨¡å¼ä¼šè‡ªåŠ¨åˆ†æï¼‰
result = analyzer.on_new_turn(
    content="é»‘è¡£äººä½å£°è¯´ï¼š'ä¸‰å¹´ä¹‹çº¦å°†è‡³ï¼Œå±Šæ—¶å¤©ä¸‹å°†å¤§å˜ã€‚'",
    role="assistant",
    user_id="user123"
)

# æ‰‹åŠ¨æ“ä½œä»ç„¶å¯ç”¨ï¼
tracker.plant(content="æ‰‹åŠ¨æ·»åŠ çš„ä¼ç¬”", importance=0.8)
tracker.resolve(fsh_id="fsh_xxx")
```

---

## äº”ã€ä»£ç åœºæ™¯æ”¯æŒï¼ˆä¸Šåƒæ–‡ä»¶é¡¹ç›®ï¼‰

### 5.1 ä»£ç ç´¢å¼•ç³»ç»Ÿï¼ˆå®Œæ•´å®ç°ï¼‰

```python
class CodeIndexer:
    """ä»£ç ç´¢å¼•å™¨ - æ”¯æŒä¸Šåƒæ–‡ä»¶é¡¹ç›®"""
    
    # æ”¯æŒçš„è¯­è¨€åŠå…¶è§£æå™¨
    LANGUAGE_PARSERS = {
        '.py': 'python',
        '.js': 'javascript', 
        '.ts': 'typescript',
        '.java': 'java',
        '.go': 'go',
        '.rs': 'rust',
    }
    
    def __init__(self, project_root: str):
        self.project_root = project_root
        self.symbol_index = {}      # ç¬¦å· â†’ å®šä¹‰ä½ç½®
        self.import_graph = {}      # ä¾èµ–å›¾
        self.usage_index = {}       # ç¬¦å· â†’ ä½¿ç”¨ä½ç½®
        self.style_profile = {}     # ä»£ç é£æ ¼ç‰¹å¾
        self._file_hashes = {}      # æ–‡ä»¶å“ˆå¸Œï¼Œç”¨äºå¢é‡æ›´æ–°
        
    def index_project(self, incremental: bool = True):
        """ç´¢å¼•æ•´ä¸ªé¡¹ç›®ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰"""
        for file_path in self._walk_source_files():
            if incremental and not self._file_changed(file_path):
                continue
            self._index_file(file_path)
        
        self._build_dependency_graph()
        self._infer_code_style()
    
    def _walk_source_files(self) -> Iterator[str]:
        """éå†æºæ–‡ä»¶ï¼Œè‡ªåŠ¨è·³è¿‡ node_modulesã€__pycache__ ç­‰"""
        ignore_dirs = {'node_modules', '__pycache__', '.git', 'venv', 'dist', 'build'}
        for root, dirs, files in os.walk(self.project_root):
            dirs[:] = [d for d in dirs if d not in ignore_dirs]
            for file in files:
                ext = os.path.splitext(file)[1]
                if ext in self.LANGUAGE_PARSERS:
                    yield os.path.join(root, file)
    
    def _file_changed(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å˜åŒ–ï¼ˆå¢é‡æ›´æ–°ç”¨ï¼‰"""
        import hashlib
        with open(file_path, 'rb') as f:
            current_hash = hashlib.md5(f.read()).hexdigest()
        old_hash = self._file_hashes.get(file_path)
        self._file_hashes[file_path] = current_hash
        return current_hash != old_hash
    
    def _index_file(self, file_path: str):
        """ç´¢å¼•å•ä¸ªæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        ext = os.path.splitext(file_path)[1]
        language = self.LANGUAGE_PARSERS.get(ext, 'unknown')
        
        # æå–ç¬¦å·
        symbols = self._extract_symbols(content, language)
        for symbol in symbols:
            self.symbol_index[symbol['name']] = {
                'name': symbol['name'],
                'type': symbol['type'],
                'file': file_path,
                'line': symbol['line'],
                'signature': symbol.get('signature', ''),
                'docstring': symbol.get('docstring', ''),
            }
        
        # æå–å¯¼å…¥
        imports = self._extract_imports(content, language)
        self.import_graph[file_path] = imports
        
        # æå–ä½¿ç”¨
        usages = self._extract_usages(content, symbols)
        for usage in usages:
            if usage['symbol'] not in self.usage_index:
                self.usage_index[usage['symbol']] = []
            self.usage_index[usage['symbol']].append({
                'file': file_path,
                'line': usage['line'],
            })
    
    def _extract_symbols(self, content: str, language: str) -> List[dict]:
        """æå–ç¬¦å·ï¼ˆå‡½æ•°ã€ç±»ã€å˜é‡ï¼‰- å¤šè¯­è¨€æ”¯æŒ"""
        symbols = []
        lines = content.split('\n')
        
        if language == 'python':
            # Python: def, class, å…¨å±€å˜é‡
            patterns = [
                (r'^(async\s+)?def\s+(\w+)\s*\(([^)]*)\)', 'function'),
                (r'^class\s+(\w+)(?:\([^)]*\))?:', 'class'),
                (r'^([A-Z_][A-Z0-9_]*)\s*=', 'constant'),
            ]
        elif language in ('javascript', 'typescript'):
            # JS/TS: function, class, const/let
            patterns = [
                (r'(?:async\s+)?function\s+(\w+)\s*\(([^)]*)\)', 'function'),
                (r'(?:const|let|var)\s+(\w+)\s*=\s*(?:async\s+)?\([^)]*\)\s*=>', 'function'),
                (r'class\s+(\w+)', 'class'),
                (r'(?:export\s+)?(?:const|let)\s+([A-Z_][A-Z0-9_]*)\s*=', 'constant'),
            ]
        elif language == 'java':
            patterns = [
                (r'(?:public|private|protected)?\s*(?:static\s+)?(?:\w+\s+)+(\w+)\s*\([^)]*\)\s*(?:throws\s+\w+)?\s*\{', 'method'),
                (r'(?:public|private)?\s*class\s+(\w+)', 'class'),
            ]
        elif language == 'go':
            patterns = [
                (r'func\s+(?:\([^)]+\)\s+)?(\w+)\s*\(([^)]*)\)', 'function'),
                (r'type\s+(\w+)\s+struct', 'struct'),
            ]
        else:
            patterns = []
        
        for i, line in enumerate(lines, 1):
            for pattern, symbol_type in patterns:
                match = re.search(pattern, line)
                if match:
                    name = match.group(2) if symbol_type == 'function' and language == 'python' else match.group(1)
                    symbols.append({
                        'name': name,
                        'type': symbol_type,
                        'line': i,
                        'signature': line.strip(),
                        'docstring': self._extract_docstring(lines, i, language),
                    })
        
        return symbols
    
    def _extract_docstring(self, lines: List[str], start_line: int, language: str) -> str:
        """æå–æ–‡æ¡£å­—ç¬¦ä¸²"""
        if language == 'python' and start_line < len(lines):
            next_line = lines[start_line].strip() if start_line < len(lines) else ''
            if next_line.startswith('"""') or next_line.startswith("'''"):
                # æ‰¾åˆ°docstringç»“æŸ
                quote = next_line[:3]
                if next_line.count(quote) >= 2:
                    return next_line.strip(quote).strip()
                for i in range(start_line + 1, min(start_line + 10, len(lines))):
                    if quote in lines[i]:
                        return ' '.join(lines[start_line:i+1]).replace(quote, '').strip()
        return ''
    
    def _extract_imports(self, content: str, language: str) -> List[str]:
        """æå–å¯¼å…¥ä¾èµ–"""
        imports = []
        
        if language == 'python':
            imports.extend(re.findall(r'^import\s+([\w.]+)', content, re.M))
            imports.extend(re.findall(r'^from\s+([\w.]+)\s+import', content, re.M))
        elif language in ('javascript', 'typescript'):
            imports.extend(re.findall(r"(?:import|require)\s*\(?['\"]([^'\"]+)['\"]", content))
        elif language == 'java':
            imports.extend(re.findall(r'^import\s+([\w.]+);', content, re.M))
        elif language == 'go':
            imports.extend(re.findall(r'"([\w./]+)"', content))
        
        return imports
    
    def _extract_usages(self, content: str, symbols: List[dict]) -> List[dict]:
        """æå–ç¬¦å·ä½¿ç”¨ä½ç½®"""
        usages = []
        lines = content.split('\n')
        symbol_names = {s['name'] for s in symbols}
        
        for i, line in enumerate(lines, 1):
            for name in symbol_names:
                # ç®€å•çš„ä½¿ç”¨æ£€æµ‹ï¼šç¬¦å·åå‡ºç°ä½†ä¸æ˜¯å®šä¹‰è¡Œ
                if re.search(rf'\b{re.escape(name)}\b', line):
                    if not any(s['line'] == i and s['name'] == name for s in symbols):
                        usages.append({'symbol': name, 'line': i})
        
        return usages
    
    def search_symbol(self, query: str) -> List[SymbolInfo]:
        """æœç´¢ç¬¦å·"""
        results = []
        
        # ç²¾ç¡®åŒ¹é…
        if query in self.symbol_index:
            results.append(self.symbol_index[query])
        
        # æ¨¡ç³ŠåŒ¹é…
        for name, info in self.symbol_index.items():
            if query.lower() in name.lower():
                results.append(info)
        
        return results
    
    def get_symbol_context(self, symbol_name: str) -> str:
        """è·å–ç¬¦å·çš„å®Œæ•´ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        # å®šä¹‰
        if symbol_name in self.symbol_index:
            info = self.symbol_index[symbol_name]
            context_parts.append(f"ã€å®šä¹‰ã€‘{info.file}:{info.line}")
            context_parts.append(f"```\n{info.signature}\n{info.docstring}\n```")
        
        # ä½¿ç”¨ä½ç½®
        if symbol_name in self.usage_index:
            usages = self.usage_index[symbol_name][:5]  # æœ€å¤š5ä¸ªä½¿ç”¨ç¤ºä¾‹
            context_parts.append(f"ã€ä½¿ç”¨ç¤ºä¾‹ã€‘({len(self.usage_index[symbol_name])}å¤„)")
            for usage in usages:
                context_parts.append(f"- {usage.file}:{usage.line}")
        
        # ä¾èµ–å…³ç³»
        deps = self.get_dependencies(symbol_name)
        if deps:
            context_parts.append(f"ã€ä¾èµ–ã€‘{', '.join(deps)}")
        
        dependents = self.get_dependents(symbol_name)
        if dependents:
            context_parts.append(f"ã€è¢«ä¾èµ–ã€‘{', '.join(dependents)}")
        
        return '\n'.join(context_parts)
```

### 5.2 ä»£ç é£æ ¼æ¨æ–­

```python
class CodeStyleInferrer:
    """ä»£ç é£æ ¼æ¨æ–­å™¨ - è‡ªåŠ¨å­¦ä¹ é¡¹ç›®è§„èŒƒ"""
    
    def infer_style(self, code_samples: List[str]) -> CodeStyleProfile:
        """ä»ä»£ç æ ·æœ¬ä¸­æ¨æ–­é£æ ¼"""
        profile = CodeStyleProfile()
        
        # å‘½åé£æ ¼
        profile.naming = self.infer_naming_style(code_samples)
        # snake_case, camelCase, PascalCase
        
        # ç¼©è¿›é£æ ¼
        profile.indentation = self.infer_indentation(code_samples)
        # spaces_2, spaces_4, tabs
        
        # æ³¨é‡Šé£æ ¼
        profile.comments = self.infer_comment_style(code_samples)
        # docstringæ ¼å¼ã€è¡Œå†…æ³¨é‡Šä¹ æƒ¯
        
        # å¯¼å…¥é£æ ¼
        profile.imports = self.infer_import_style(code_samples)
        # åˆ†ç»„ã€æ’åºã€ç»å¯¹/ç›¸å¯¹å¯¼å…¥
        
        # ä»£ç ç»„ç»‡
        profile.organization = self.infer_organization(code_samples)
        # å‡½æ•°é•¿åº¦ã€ç±»ç»“æ„ã€æ¨¡å—åˆ’åˆ†
        
        return profile
    
    def validate_against_style(self, new_code: str, profile: CodeStyleProfile) -> List[StyleViolation]:
        """æ£€æŸ¥æ–°ä»£ç æ˜¯å¦ç¬¦åˆæ¨æ–­çš„é£æ ¼"""
        violations = []
        
        # æ£€æŸ¥å‘½å
        naming_issues = self.check_naming(new_code, profile.naming)
        violations.extend(naming_issues)
        
        # æ£€æŸ¥ç¼©è¿›
        indent_issues = self.check_indentation(new_code, profile.indentation)
        violations.extend(indent_issues)
        
        # ... å…¶ä»–æ£€æŸ¥
        
        return violations
```

### 5.3 ä»£ç ä¾èµ–è¿½è¸ª

```python
class DependencyTracker:
    """ä¾èµ–è¿½è¸ªå™¨ - ç†è§£ä»£ç é—´çš„å…³ç³»"""
    
    def __init__(self, import_graph: dict):
        self.import_graph = import_graph
        self.reverse_graph = self.build_reverse_graph()
    
    def get_impact_analysis(self, file_path: str) -> ImpactAnalysis:
        """åˆ†æä¿®æ”¹æŸæ–‡ä»¶çš„å½±å“èŒƒå›´"""
        # ç›´æ¥ä¾èµ–è¿™ä¸ªæ–‡ä»¶çš„
        direct_dependents = self.reverse_graph.get(file_path, [])
        
        # é€’å½’æ‰¾åˆ°æ‰€æœ‰å—å½±å“çš„æ–‡ä»¶
        all_affected = set()
        queue = list(direct_dependents)
        while queue:
            current = queue.pop(0)
            if current not in all_affected:
                all_affected.add(current)
                queue.extend(self.reverse_graph.get(current, []))
        
        return ImpactAnalysis(
            file=file_path,
            direct_dependents=direct_dependents,
            all_affected=list(all_affected),
            risk_level=self.assess_risk(len(all_affected)),
        )
    
    def get_related_files(self, file_path: str, depth: int = 2) -> List[str]:
        """è·å–ç›¸å…³æ–‡ä»¶ï¼ˆç”¨äºä¸Šä¸‹æ–‡æ³¨å…¥ï¼‰"""
        related = set()
        
        # å‘ä¸Šï¼šè¿™ä¸ªæ–‡ä»¶ä¾èµ–çš„
        deps = self.get_dependencies_recursive(file_path, depth)
        related.update(deps)
        
        # å‘ä¸‹ï¼šä¾èµ–è¿™ä¸ªæ–‡ä»¶çš„
        dependents = self.get_dependents_recursive(file_path, depth)
        related.update(dependents)
        
        # åŒç›®å½•çš„ï¼ˆå¯èƒ½æœ‰éšå¼å…³ç³»ï¼‰
        same_dir = self.get_same_directory_files(file_path)
        related.update(same_dir[:5])  # é™åˆ¶æ•°é‡
        
        return list(related)
```

---

## å…­ã€ä¸€è‡´æ€§æ ¡éªŒç³»ç»Ÿï¼ˆå®Œæ•´å®ç°ï¼‰

```python
# recall/processor/consistency.py
import re
from typing import List, Optional, Any
from dataclasses import dataclass, field

@dataclass
class Violation:
    """è¿è§„è®°å½•"""
    type: str
    severity: str  # CRITICAL, HIGH, MEDIUM, LOW
    rule: str = ""
    evidence: str = ""
    entity: str = ""
    attribute: str = ""
    expected: Any = None
    found: Any = None
    stored_value: Any = None
    output_claim: str = ""

@dataclass
class ConsistencyResult:
    """ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ"""
    is_consistent: bool
    violations: List[Violation] = field(default_factory=list)
    warnings: List[Violation] = field(default_factory=list)
    suggested_fixes: List[str] = field(default_factory=list)

class ConsistencyChecker:
    """ä¸€è‡´æ€§æ ¡éªŒå™¨ - ç¡®ä¿è§„èŒƒ100%éµå®ˆ"""
    
    def __init__(self, core_settings: 'CoreSettings', memory: 'ConsolidatedMemory'):
        self.core = core_settings
        self.memory = memory
        self.violation_log = []
        
        # é¢„ç¼–è¯‘æ ¸å¿ƒè®¾å®šä¸­çš„è§„åˆ™
        self._compiled_rules = self._compile_core_rules()
    
    def _compile_core_rules(self) -> List[dict]:
        """å°†æ ¸å¿ƒè®¾å®šç¼–è¯‘ä¸ºå¯æ£€æŸ¥çš„è§„åˆ™"""
        rules = []
        
        # ä» L0 æ ¸å¿ƒè®¾å®šä¸­æå–æ˜ç¡®çš„çº¦æŸ
        if self.core.absolute_rules:
            for rule in self.core.absolute_rules:
                rules.append({
                    'type': 'absolute',
                    'content': rule,
                    'keywords': self._extract_rule_keywords(rule),
                })
        
        # ä»è§’è‰²å¡ä¸­æå–å±æ€§çº¦æŸ
        if self.core.character_card:
            char_attrs = self._extract_character_attributes(self.core.character_card)
            for attr_name, attr_value in char_attrs.items():
                rules.append({
                    'type': 'character_attribute',
                    'attribute': attr_name,
                    'value': attr_value,
                })
        
        return rules
    
    def _extract_rule_keywords(self, rule: str) -> List[str]:
        """ä»è§„åˆ™ä¸­æå–å…³é”®è¯"""
        # æå–åè¯å’ŒåŠ¨è¯
        keywords = re.findall(r'[\u4e00-\u9fa5]{2,}|[a-zA-Z]+', rule)
        return keywords
    
    def _extract_character_attributes(self, card: str) -> dict:
        """ä»è§’è‰²å¡æå–å±æ€§"""
        attrs = {}
        
        # å¸¸è§å±æ€§æ¨¡å¼
        patterns = [
            (r'æ€§åˆ«[ï¼š:]\s*(\S+)', 'gender'),
            (r'å¹´é¾„[ï¼š:]\s*(\d+)', 'age'),
            (r'èº«é«˜[ï¼š:]\s*(\S+)', 'height'),
            (r'å‘è‰²[ï¼š:]\s*(\S+)', 'hair_color'),
            (r'ç³è‰²[ï¼š:]\s*(\S+)', 'eye_color'),
        ]
        
        for pattern, attr_name in patterns:
            match = re.search(pattern, card)
            if match:
                attrs[attr_name] = match.group(1)
        
        return attrs
    
    def check_output(self, ai_output: str, context: dict) -> ConsistencyResult:
        """æ£€æŸ¥AIè¾“å‡ºæ˜¯å¦ä¸è®¾å®šä¸€è‡´"""
        violations = []
        warnings = []
        
        # 1. æ£€æŸ¥ç»å¯¹è§„åˆ™
        for rule in self._compiled_rules:
            if rule['type'] == 'absolute':
                violation = self._check_absolute_rule(ai_output, rule)
                if violation:
                    violations.append(violation)
            
            elif rule['type'] == 'character_attribute':
                violation = self._check_character_attribute(ai_output, rule)
                if violation:
                    violations.append(violation)
        
        # 2. æ£€æŸ¥ä¸ L1 é•¿æœŸè®°å¿†çš„å†²çª
        memory_conflicts = self._check_against_memory(ai_output)
        violations.extend(memory_conflicts)
        
        # 3. æ£€æŸ¥ä¸æœ€è¿‘å¯¹è¯çš„å†²çªï¼ˆè­¦å‘Šçº§åˆ«ï¼‰
        recent_conflicts = self._check_against_recent(ai_output, context)
        warnings.extend(recent_conflicts)
        
        # 4. ä»£ç åœºæ™¯ï¼šé£æ ¼ä¸€è‡´æ€§
        if context.get('scenario') == 'coding':
            style_issues = self._check_code_style(ai_output)
            warnings.extend(style_issues)
        
        return ConsistencyResult(
            is_consistent=len(violations) == 0,
            violations=violations,
            warnings=warnings,
            suggested_fixes=self._suggest_fixes(violations),
        )
    
    def _check_absolute_rule(self, output: str, rule: dict) -> Optional[Violation]:
        """æ£€æŸ¥ç»å¯¹è§„åˆ™"""
        # æå–è¾“å‡ºä¸­ä¸è§„åˆ™ç›¸å…³çš„æ–­è¨€
        rule_keywords = rule['keywords']
        
        # å¦‚æœè¾“å‡ºåŒ…å«è§„åˆ™å…³é”®è¯ï¼Œéœ€è¦è¯¦ç»†æ£€æŸ¥
        relevant_keywords = [kw for kw in rule_keywords if kw in output]
        if not relevant_keywords:
            return None  # ä¸ç›¸å…³
        
        # æ£€æŸ¥æ˜¯å¦æ˜æ˜¾è¿å
        # ä¾‹å¦‚è§„åˆ™æ˜¯"è§’è‰²ä¸ä¼šæ€äºº"ï¼Œè¾“å‡ºåŒ…å«"æ€æ­»äº†"
        negative_indicators = ['ä¸', 'æ²¡æœ‰', 'ä»ä¸', 'ç»ä¸', 'ä¸ä¼š']
        rule_has_negative = any(neg in rule['content'] for neg in negative_indicators)
        
        if rule_has_negative:
            # è§„åˆ™æ˜¯å¦å®šå¥ï¼Œæ£€æŸ¥è¾“å‡ºæ˜¯å¦è‚¯å®šäº†è¿™ä»¶äº‹
            for neg in negative_indicators:
                if neg in rule['content']:
                    action = rule['content'].split(neg)[-1][:10]  # å–åŠ¨ä½œéƒ¨åˆ†
                    # æ£€æŸ¥è¾“å‡ºæ˜¯å¦è‚¯å®šåœ°åšäº†è¿™ä¸ªåŠ¨ä½œ
                    affirmative_patterns = [
                        rf'{action}äº†', rf'æ­£åœ¨{action}', rf'å¼€å§‹{action}'
                    ]
                    for pattern in affirmative_patterns:
                        if re.search(pattern, output):
                            return Violation(
                                type='ABSOLUTE_RULE_VIOLATION',
                                rule=rule['content'],
                                evidence=output[:100],
                                severity='CRITICAL',
                            )
        
        return None
    
    def _check_character_attribute(self, output: str, rule: dict) -> Optional[Violation]:
        """æ£€æŸ¥è§’è‰²å±æ€§ä¸€è‡´æ€§"""
        attr_name = rule['attribute']
        expected_value = rule['value']
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦æè¿°äº†ä¸ä¸€è‡´çš„å±æ€§
        conflict_patterns = {
            'gender': {
                'ç”·': ['å¥¹', 'å¥³å­©', 'å¥³å­', 'å°å§'],
                'å¥³': ['ä»–', 'ç”·å­©', 'ç”·å­', 'å…ˆç”Ÿ'],
            },
            'hair_color': lambda v: [f'ä¸æ˜¯{v}', f'{v}å˜æˆäº†'],
        }
        
        if attr_name in conflict_patterns:
            patterns = conflict_patterns[attr_name]
            if callable(patterns):
                check_patterns = patterns(expected_value)
            else:
                check_patterns = patterns.get(expected_value, [])
            
            for pattern in check_patterns:
                if pattern in output:
                    return Violation(
                        type='CHARACTER_ATTRIBUTE_CONFLICT',
                        attribute=attr_name,
                        expected=expected_value,
                        found=pattern,
                        severity='HIGH',
                    )
        
        return None
    
    def _check_against_memory(self, output: str) -> List[Violation]:
        """æ£€æŸ¥æ˜¯å¦ä¸é•¿æœŸè®°å¿†å†²çª"""
        violations = []
        
        # æå–è¾“å‡ºä¸­æåˆ°çš„å®ä½“
        entities = self._extract_entities(output)
        
        for entity_name in entities:
            entity = self.memory.get_entity(entity_name)
            if not entity:
                continue
            
            # æ£€æŸ¥çŠ¶æ€å†²çª
            for attr, stored_value in entity.current_state.items():
                # æ£€æŸ¥è¾“å‡ºæ˜¯å¦å£°ç§°äº†ä¸åŒçš„çŠ¶æ€
                conflict = self._find_state_conflict(output, entity_name, attr, stored_value)
                if conflict:
                    violations.append(Violation(
                        type='MEMORY_CONFLICT',
                        entity=entity_name,
                        attribute=attr,
                        stored_value=stored_value,
                        output_claim=conflict,
                        severity='MEDIUM',
                    ))
        
        return violations
    
    def _find_state_conflict(self, output: str, entity: str, attr: str, stored_value) -> Optional[str]:
        """æŸ¥æ‰¾çŠ¶æ€å†²çª"""
        # çŠ¶æ€å¯¹ç«‹è¯å…¸
        opposites = {
            'alive': ['æ­»äº†', 'å»ä¸–', 'ç‰ºç‰²', 'æ­»äº¡'],
            'dead': ['è¿˜æ´»ç€', 'æ´»è¿‡æ¥', 'å¤æ´»'],
            'present': ['ç¦»å¼€äº†', 'ä¸åœ¨'],
            'absent': ['å‡ºç°äº†', 'å›æ¥äº†'],
        }
        
        if stored_value in opposites:
            for opposite in opposites[stored_value]:
                if entity in output and opposite in output:
                    return opposite
        
        return None
    
    def _extract_entities(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬æå–å®ä½“å"""
        # ç®€å•å®ç°ï¼šåŒ¹é…å·²çŸ¥å®ä½“
        known_entities = list(self.memory.get_all_entity_names())
        found = [e for e in known_entities if e in text]
        return found
    
    def _check_against_recent(self, output: str, context: dict) -> List[Violation]:
        """æ£€æŸ¥ä¸æœ€è¿‘å¯¹è¯çš„å†²çª"""
        # å®ç°ç•¥ï¼Œè¿”å›è­¦å‘Šçº§åˆ«çš„Violation
        return []
    
    def _check_code_style(self, output: str) -> List[Violation]:
        """æ£€æŸ¥ä»£ç é£æ ¼"""
        warnings = []
        
        # æ£€æµ‹ä»£ç å—
        code_blocks = re.findall(r'```[\w]*\n(.*?)```', output, re.DOTALL)
        
        for code in code_blocks:
            # æ£€æŸ¥å‘½åé£æ ¼
            if self.core.code_standards:
                if 'camelCase' in self.core.code_standards:
                    snake_vars = re.findall(r'\b[a-z]+_[a-z]+\b', code)
                    if snake_vars:
                        warnings.append(Warning(
                            type='STYLE_INCONSISTENCY',
                            message=f'å‘ç° snake_case å˜é‡ï¼Œä½†é¡¹ç›®ä½¿ç”¨ camelCase: {snake_vars[:3]}',
                        ))
        
        return warnings
    
    def _suggest_fixes(self, violations: List[Violation]) -> List[str]:
        """ä¸ºè¿è§„å»ºè®®ä¿®å¤æ–¹æ¡ˆ"""
        suggestions = []
        for v in violations:
            if v.type == 'CHARACTER_ATTRIBUTE_CONFLICT':
                suggestions.append(f"è¯·å°† '{v.found}' ä¿®æ”¹ä¸ºç¬¦åˆ {v.attribute}={v.expected} çš„æè¿°")
            elif v.type == 'MEMORY_CONFLICT':
                suggestions.append(f"è§’è‰² {v.entity} çš„ {v.attribute} åº”è¯¥æ˜¯ {v.stored_value}ï¼Œè¯·ä¿®æ­£")
        return suggestions
```

### 6.1 è§„åˆ™ç¼–è¯‘å™¨ï¼ˆå¾…å®ç°ï¼‰

> ğŸ”§ **å¾…å®ç°**ï¼šå®Œæ•´çš„è§„åˆ™ç¼–è¯‘å™¨ï¼Œå°†è‡ªç„¶è¯­è¨€è§„åˆ™è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„æ£€æŸ¥é€»è¾‘ã€‚
> 
> è¯¦ç»†å®ç°è®¡åˆ’è¯·å‚è§ [CHECKLIST-REPORT.md](./CHECKLIST-REPORT.md) ç¬¬å››èŠ‚ "é˜¶æ®µä¸€ç‚¹äº”ï¼šè§„åˆ™ç¼–è¯‘å™¨"ã€‚

**å¾…æ·»åŠ åŠŸèƒ½**ï¼š
- `RuleCompiler` - è§„åˆ™ç¼–è¯‘å™¨ç±»
- `CompiledRule` - ç»“æ„åŒ–è§„åˆ™ç±»å‹
- æ”¯æŒè§„åˆ™ç±»å‹ï¼šç¦æ­¢(PROHIBITION)ã€å¿…é¡»(REQUIREMENT)ã€å…³ç³»(RELATIONSHIP)ã€å±æ€§(ATTRIBUTE)
- é›†æˆåˆ° `ConsistencyChecker._check_rule()` æ–¹æ³•
- è§„åˆ™ç®¡ç† API (`/v1/rules`)

**å½“å‰çŠ¶æ€**ï¼šL0 æ³¨å…¥ + åŸºç¡€å±æ€§æ£€æŸ¥å·²å®ç°ï¼Œå¯¹ RP åœºæ™¯è¶³å¤Ÿä½¿ç”¨ã€‚

---

## ä¸ƒã€é›¶é…ç½®è‡ªåŠ¨åŒ–å±‚ï¼ˆçœŸæ­£çš„å³æ’å³ç”¨ï¼‰

### 7.0 åˆå§‹åŒ–é…ç½®

```python
# recall/init.py
"""åˆå§‹åŒ–å‘å¯¼ - çº¯æœ¬åœ°æ¨¡å¼ï¼Œç¯å¢ƒå®Œå…¨éš”ç¦»"""

import os
import sys

class RecallInit:
    """åˆå§‹åŒ–å‘å¯¼ - ç®€å•3æ­¥ï¼Œæ— ç—•å®‰è£…ï¼ˆæ‰€æœ‰æ•°æ®åœ¨é¡¹ç›®ç›®å½•å†…ï¼‰"""
    
    @classmethod
    def get_data_root(cls, base_path: str = None) -> str:
        """
        è·å–æ•°æ®æ ¹ç›®å½•ï¼ˆé»˜è®¤åœ¨å½“å‰å·¥ä½œç›®å½•ä¸‹ï¼‰
        
        ä¼˜å…ˆçº§ï¼š
        1. æ˜¾å¼ä¼ å…¥çš„ base_path
        2. ç¯å¢ƒå˜é‡ RECALL_DATA_ROOT
        3. å½“å‰å·¥ä½œç›®å½•ä¸‹çš„ recall_data/
        
        è¿™æ ·ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½åœ¨é¡¹ç›®ç›®å½•å†…ï¼Œåˆ é™¤é¡¹ç›®æ–‡ä»¶å¤¹å³å¯å®Œå…¨å¸è½½ã€‚
        """
        if base_path:
            return os.path.abspath(os.path.join(base_path, 'recall_data'))
        
        # ç¯å¢ƒå˜é‡ï¼ˆé«˜çº§ç”¨æˆ·å¯è‡ªå®šä¹‰ï¼‰
        custom_root = os.environ.get('RECALL_DATA_ROOT')
        if custom_root:
            return os.path.abspath(custom_root)
        
        # é»˜è®¤ï¼šå½“å‰å·¥ä½œç›®å½•ä¸‹çš„ recall_data/
        return os.path.abspath('./recall_data')
    
    @classmethod
    def ensure_directories(cls, base_path: str = None):
        """ç¡®ä¿æ‰€æœ‰å¿…è¦ç›®å½•å­˜åœ¨ï¼ˆå…¨éƒ¨åœ¨é¡¹ç›®ç›®å½•å†…ï¼‰"""
        root = cls.get_data_root(base_path)
        dirs = [
            root,
            os.path.join(root, 'data'),
            os.path.join(root, 'models'),
            os.path.join(root, 'models', 'spacy'),
            os.path.join(root, 'models', 'sentence-transformers'),
            os.path.join(root, 'models', 'huggingface'),
            os.path.join(root, 'models', 'torch'),
            os.path.join(root, 'cache'),
            os.path.join(root, 'logs'),
        ]
        for d in dirs:
            os.makedirs(d, exist_ok=True)
        return root
    
    @classmethod
    def setup_environment(cls, base_path: str = None):
        """
        è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿æ‰€æœ‰æ¨¡å‹å’Œç¼“å­˜éƒ½ä¸‹è½½åˆ°é¡¹ç›®ç›®å½•å†…ã€‚
        
        è¿™æ˜¯å®ç°"åˆ é™¤é¡¹ç›®æ–‡ä»¶å¤¹å³å®Œå…¨å¸è½½"çš„å…³é”®ï¼
        æ‰€æœ‰ç¬¬ä¸‰æ–¹åº“ï¼ˆsentence-transformers, huggingface, torch, spacyï¼‰
        çš„ç¼“å­˜éƒ½è¢«é‡å®šå‘åˆ°é¡¹ç›®ç›®å½•å†…ã€‚
        """
        root = cls.get_data_root(base_path)
        models_dir = os.path.join(root, 'models')
        
        # sentence-transformers æ¨¡å‹ç¼“å­˜ç›®å½•
        os.environ['SENTENCE_TRANSFORMERS_HOME'] = os.path.join(models_dir, 'sentence-transformers')
        
        # HuggingFace ç¼“å­˜ç›®å½•ï¼ˆtransformers, datasets ç­‰ï¼‰
        os.environ['HF_HOME'] = os.path.join(models_dir, 'huggingface')
        os.environ['HUGGINGFACE_HUB_CACHE'] = os.path.join(models_dir, 'huggingface', 'hub')
        os.environ['TRANSFORMERS_CACHE'] = os.path.join(models_dir, 'huggingface', 'transformers')
        
        # PyTorch ç¼“å­˜ç›®å½•
        os.environ['TORCH_HOME'] = os.path.join(models_dir, 'torch')
        
        # XDG ç¼“å­˜ç›®å½•ï¼ˆæŸäº›åº“ä¼šç”¨ï¼‰
        os.environ['XDG_CACHE_HOME'] = os.path.join(root, 'cache')
        
        # ç¦æ­¢åŒ¿åæ•°æ®æ”¶é›†
        os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
        os.environ['DO_NOT_TRACK'] = '1'
        os.environ['ANONYMIZED_TELEMETRY'] = 'false'
    
    def run_init_wizard(self, base_path: str = None):
        """äº¤äº’å¼åˆå§‹åŒ–"""
        # å…ˆè®¾ç½®ç¯å¢ƒï¼ˆç¡®ä¿æ‰€æœ‰ç¼“å­˜éƒ½åœ¨é¡¹ç›®ç›®å½•å†…ï¼‰
        self.setup_environment(base_path)
        root = self.ensure_directories(base_path)
        
        print("ğŸ§  æ¬¢è¿ä½¿ç”¨ Recall - AIæ°¸ä¹…è®°å¿†ç³»ç»Ÿ")
        print("=" * 40)
        print(f"\nğŸ“‚ æ•°æ®ç›®å½•ï¼š{root}")
        print("ğŸ“¦ æ‰€æœ‰æ•°æ®éƒ½å­˜å‚¨åœ¨æ­¤ç›®å½•å†…ï¼Œåˆ é™¤é¡¹ç›®æ–‡ä»¶å¤¹å³å¯å®Œå…¨å¸è½½ã€‚")
        print("   ä¸ä¼šåœ¨ç”¨æˆ·ç›®å½•æˆ–ç³»ç»Ÿç›®å½•åˆ›å»ºä»»ä½•æ–‡ä»¶ã€‚")
        print("   ä½ éœ€è¦è‡ªå·±çš„ AI API key æ¥è°ƒç”¨å¤§æ¨¡å‹ã€‚\n")
        
        # è·å– API key
        api_key = os.environ.get('OPENAI_API_KEY')
        if not api_key:
            print("æ”¯æŒçš„ API æä¾›å•†ï¼š")
            print("  - OpenAI (sk-xxx)")
            print("  - Claude (sk-ant-xxx)")
            print("  - å…¶ä»–å…¼å®¹ OpenAI æ ¼å¼çš„ API\n")
            api_key = input("è¯·è¾“å…¥ä½ çš„ API key: ").strip()
        
        if not api_key:
            print("âš ï¸  æœªè®¾ç½® API keyï¼ŒRecall å°†åªæä¾›è®°å¿†å­˜å‚¨åŠŸèƒ½ï¼Œæ— æ³•è‡ªåŠ¨æ€»ç»“ã€‚")
        
        # ä¿å­˜é…ç½®
        config = {
            'api_key': api_key,
            'initialized': True,
            'version': '3.0',
            'data_path': os.path.join(root, 'data'),
        }
        self._save_config(config, root)
        
        print("\nâœ… åˆå§‹åŒ–å®Œæˆï¼")
        print(f"   æ•°æ®ç›®å½•: {root}")
        print("\nğŸ—‘ï¸ å¸è½½æ–¹æ³•ï¼š")
        print(f"   1. pip uninstall recall-ai")
        print(f"   2. åˆ é™¤ç›®å½•: {root}")
        print("\nç°åœ¨å¯ä»¥ä½¿ç”¨ 'recall chat' å¼€å§‹å¯¹è¯äº†ï¼")
        
        return config
    
    def auto_init_for_st(self):
        """SillyTavern è‡ªåŠ¨åˆå§‹åŒ–ï¼ˆé™é»˜ï¼‰"""
        self.setup_environment()
        self.ensure_directories()
        # ST ç”¨æˆ·åœ¨æ’ä»¶è®¾ç½®ä¸­é…ç½® API key
        return {
            'api_key': None,  # ç”± ST æ’ä»¶é…ç½®
            'initialized': True,
            'st_plugin': True,
        }
    
    def _save_config(self, config, root):
        """ä¿å­˜é…ç½®åˆ°æœ¬åœ°"""
        import json
        config_file = os.path.join(root, 'config.json')
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    
    @classmethod
    def get_uninstall_instructions(cls, base_path: str = None) -> str:
        """è¿”å›å®Œæ•´å¸è½½è¯´æ˜"""
        root = cls.get_data_root(base_path)
        return f"""
ğŸ—‘ï¸ å®Œæ•´å¸è½½ Recallï¼ˆæœ€ç®€å•çš„æ–¹å¼ï¼‰ï¼š

æ–¹æ³•ä¸€ï¼šç›´æ¥åˆ é™¤é¡¹ç›®æ–‡ä»¶å¤¹ï¼ˆæ¨èï¼‰
- æ‰€æœ‰æ•°æ®éƒ½åœ¨é¡¹ç›®ç›®å½•å†…ï¼Œåˆ é™¤æ•´ä¸ªé¡¹ç›®æ–‡ä»¶å¤¹å³å¯

æ–¹æ³•äºŒï¼šåªåˆ é™¤ Recall æ•°æ®
- åˆ é™¤æ•°æ®ç›®å½•ï¼š{root}
- å¯é€‰ï¼špip uninstall recall-ai

âœ… å¸è½½åç³»ç»Ÿå®Œå…¨æ¢å¤åŸçŠ¶ï¼Œä¸ä¼šåœ¨ç”¨æˆ·ç›®å½•æˆ–ç³»ç»Ÿç›®å½•ç•™ä¸‹ä»»ä½•æ–‡ä»¶ã€‚
"""

### 7.1 åœºæ™¯è‡ªåŠ¨æ£€æµ‹

```python
# recall/processor/scenario.py
import re
from typing import List

class ScenarioDetector:
    """åœºæ™¯è‡ªåŠ¨æ£€æµ‹ - é›¶é…ç½®çš„å…³é”®"""
    
    def detect(self, user_input: str, history: List[dict]) -> str:
        """è‡ªåŠ¨æ£€æµ‹å½“å‰åœºæ™¯"""
        
        # ä»£ç åœºæ™¯ç‰¹å¾
        code_indicators = [
            r'```',                    # ä»£ç å—
            r'def |class |function ',  # å‡½æ•°/ç±»å®šä¹‰
            r'import |require\(',      # å¯¼å…¥è¯­å¥
            r'\.py|\.js|\.ts|\.java',  # æ–‡ä»¶æ‰©å±•å
            r'bug|error|fix|å®ç°|é‡æ„', # å¼€å‘ç›¸å…³è¯æ±‡
        ]
        
        # RPåœºæ™¯ç‰¹å¾
        rp_indicators = [
            r'\*[^*]+\*',              # åŠ¨ä½œæè¿° *xxx*
            r'ã€Œ[^ã€]+ã€|"[^"]+"',      # å¯¹è¯å¼•ç”¨
            r'è¯´é“|å›ç­”|çœ‹ç€|èµ°å‘',      # å™äº‹åŠ¨è¯
            r'è§’è‰²|NPC|ä¸–ç•Œè§‚',          # RPæœ¯è¯­
        ]
        
        code_score = sum(1 for p in code_indicators if re.search(p, user_input))
        rp_score = sum(1 for p in rp_indicators if re.search(p, user_input))
        
        # ç»“åˆå†å²åˆ¤æ–­
        history_scenario = self.analyze_history_scenario(history)
        
        if code_score > rp_score and code_score >= 2:
            return 'coding'
        elif rp_score > code_score and rp_score >= 2:
            return 'roleplay'
        elif history_scenario:
            return history_scenario  # æ²¿ç”¨å†å²åœºæ™¯
        else:
            return 'general'
    
    def analyze_history_scenario(self, history: List[dict]) -> str:
        """åˆ†æå†å²æ¶ˆæ¯çš„åœºæ™¯å€¾å‘"""
        if not history:
            return ''
        
        code_count = 0
        rp_count = 0
        
        # åˆ†ææœ€è¿‘10æ¡æ¶ˆæ¯
        for msg in history[-10:]:
            content = msg.get('content', '') or msg.get('user', '') or msg.get('assistant', '')
            if '```' in content or 'def ' in content or 'import ' in content:
                code_count += 1
            if '*' in content or 'ã€Œ' in content or 'è¯´é“' in content:
                rp_count += 1
        
        if code_count > rp_count:
            return 'coding'
        elif rp_count > code_count:
            return 'roleplay'
        return ''
```

### 7.2 è‡ªåŠ¨å‚æ•°è°ƒä¼˜

```python
class AutoTuner:
    """è‡ªåŠ¨å‚æ•°è°ƒä¼˜ - ç”¨æˆ·æ— éœ€é…ç½®"""
    
    # æ‰€æœ‰å‚æ•°éƒ½æœ‰åˆç†é»˜è®¤å€¼
    DEFAULT_PARAMS = {
        # å­˜å‚¨å‚æ•°
        'l3_buffer_size': 50,          # L3ä¿ç•™æœ€è¿‘50è½®
        'l2_capacity': 200,             # L2æœ€å¤š200ä¸ªå®ä½“
        'consolidation_threshold': 3,   # è®¿é—®3æ¬¡åè€ƒè™‘å·©å›º
        
        # æ£€ç´¢å‚æ•°
        'vector_top_k': 20,             # å‘é‡æ£€ç´¢top-k
        'ngram_n': 3,                   # N-gramçš„N
        
        # Tokené¢„ç®—
        'total_budget': 8000,           # æ€»tokené¢„ç®—
        'l0_budget': 2000,              # L0é¢„ç®—
        'retrieved_ratio': 0.4,         # æ£€ç´¢ç»“æœå æ¯”
        
        # æƒŠè®¶åº¦é˜ˆå€¼
        'surprise_low': 0.3,            # ä½æƒŠè®¶åº¦é˜ˆå€¼
        'surprise_high': 0.7,           # é«˜æƒŠè®¶åº¦é˜ˆå€¼
        
        # ä¼ç¬”å‚æ•°
        'foreshadowing_remind_turns': 100,  # 100è½®åæé†’
    }
    
    def __init__(self):
        self.params = self.DEFAULT_PARAMS.copy()
        self.usage_stats = {}  # ä½¿ç”¨ç»Ÿè®¡ï¼Œç”¨äºè‡ªé€‚åº”è°ƒæ•´
    
    def auto_adjust(self):
        """æ ¹æ®ä½¿ç”¨ç»Ÿè®¡è‡ªåŠ¨è°ƒæ•´å‚æ•°"""
        
        # å¦‚æœæ£€ç´¢ç»å¸¸è§¦å‘N-gramå…œåº•ï¼Œè¯´æ˜å…¶ä»–å±‚ä¸å¤Ÿå¥½
        if self.usage_stats.get('ngram_fallback_rate', 0) > 0.3:
            # å¢åŠ å‘é‡æ£€ç´¢çš„top_k
            self.params['vector_top_k'] = min(50, self.params['vector_top_k'] + 10)
        
        # å¦‚æœL2ç»å¸¸æ»¡ï¼Œå¢åŠ å®¹é‡æˆ–é™ä½å·©å›ºé˜ˆå€¼
        if self.usage_stats.get('l2_eviction_rate', 0) > 0.5:
            self.params['consolidation_threshold'] = max(2, self.params['consolidation_threshold'] - 1)
        
        # å¦‚æœtokenç»å¸¸è¶…é¢„ç®—ï¼Œè°ƒæ•´åˆ†é…æ¯”ä¾‹
        if self.usage_stats.get('budget_overflow_rate', 0) > 0.1:
            self.params['retrieved_ratio'] = max(0.2, self.params['retrieved_ratio'] - 0.05)
```

### 7.3 è‡ªåŠ¨ç´¢å¼•ç»´æŠ¤

```python
# recall/utils/auto_maintain.py
import asyncio

class AutoIndexMaintainer:
    """è‡ªåŠ¨ç´¢å¼•ç»´æŠ¤ - åå°é™é»˜è¿è¡Œ"""
    
    def __init__(self):
        self.maintenance_interval = 100  # æ¯100è½®ç»´æŠ¤ä¸€æ¬¡
        self.last_maintenance = 0
    
    async def maybe_maintain(self, current_turn: int):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ç»´æŠ¤"""
        if current_turn - self.last_maintenance < self.maintenance_interval:
            return
        
        # åå°å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡ä¸»æµç¨‹
        asyncio.create_task(self.do_maintenance())
        self.last_maintenance = current_turn
    
    async def do_maintenance(self):
        """æ‰§è¡Œç»´æŠ¤ä»»åŠ¡"""
        
        # 1. æ£€æŸ¥ç´¢å¼•ä¸€è‡´æ€§
        inconsistencies = await self.check_index_consistency()
        if inconsistencies:
            await self.repair_indexes(inconsistencies)
        
        # 2. ä¼˜åŒ–å‘é‡ç´¢å¼•
        if self.vector_index.needs_optimization():
            await self.optimize_vector_index()
        
        # 3. æ¸…ç†è¿‡æœŸçš„L2æ¡ç›®
        await self.cleanup_stale_l2_entries()
        
        # 4. å‹ç¼©æ—§çš„Archiveå·
        await self.compress_old_volumes()
```

---

## ä¸ƒç‚¹äº”ã€æ€§èƒ½ä¼˜åŒ–ï¼šç¡®ä¿3-5ç§’å“åº”ï¼ˆæ–°å¢éœ€æ±‚ï¼‰

### 7.5.1 æ€§èƒ½ç›®æ ‡

| æ“ä½œ | ç›®æ ‡å»¶è¿Ÿ | ä¼˜åŒ–ç­–ç•¥ |
|------|---------|---------|
| **æ£€ç´¢ï¼ˆbuild_contextï¼‰** | <800ms | å¹¶è¡Œæ£€ç´¢ + ç¼“å­˜çƒ­è·¯å¾„ |
| **å­˜å‚¨ï¼ˆprocess_turnï¼‰** | <200ms | å¼‚æ­¥å†™å…¥ + æ‰¹é‡ç´¢å¼• |
| **æ€»å“åº”ï¼ˆå«LLMï¼‰** | 3-5ç§’ | Recalléƒ¨åˆ†<1.5ç§’ï¼ŒLLM 2-4ç§’ |
| **é¦–æ¬¡å†·å¯åŠ¨** | 10-15ç§’ | éœ€åŠ è½½NLPæ¨¡å‹ï¼Œä»…é¦–æ¬¡ |
| **åç»­çƒ­å¯åŠ¨** | <3ç§’ | æ¨¡å‹ç¼“å­˜ + æ‡’åŠ è½½ + é¢„çƒ­ |

> âš ï¸ **æ³¨æ„**ï¼šé¦–æ¬¡è¿è¡Œéœ€è¦åŠ è½½ sentence-transformers æ¨¡å‹ï¼ˆçº¦400MBï¼‰ï¼ŒspaCy æ¨¡å‹åˆå§‹åŒ–ä¹Ÿéœ€æ—¶é—´ã€‚è¿™æ˜¯ä¸€æ¬¡æ€§å¼€é”€ï¼Œåç»­å¯åŠ¨ä¼šå¿«å¾ˆå¤šã€‚

### 7.5.2 å¹¶è¡Œæ£€ç´¢å¼•æ“ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰

```python
# recall/retrieval/parallel_retrieval.py
"""å¹¶è¡Œæ£€ç´¢ - æ‰€æœ‰ç‹¬ç«‹å±‚åŒæ—¶æ‰§è¡Œ"""

import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any
import time

class ParallelRetrieval:
    """å¹¶è¡Œ8å±‚æ£€ç´¢ - ç¡®ä¿<800msæ€»å»¶è¿Ÿ"""
    
    def __init__(self, engine):
        self.engine = engine
        # çº¿ç¨‹æ± ç”¨äºCPUå¯†é›†å‹æ“ä½œ
        self.executor = ThreadPoolExecutor(max_workers=4)
        # ç¼“å­˜æœ€è¿‘æŸ¥è¯¢ç»“æœï¼ˆLRUï¼‰
        self.query_cache = LRUCache(maxsize=100)
        # é¢„è®¡ç®—çš„çƒ­é—¨å®ä½“
        self.hot_entities_cache = {}
    
    async def retrieve_parallel(self, query: str, context: dict) -> RetrievalResult:
        """å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ç‹¬ç«‹çš„æ£€ç´¢å±‚"""
        start_time = time.perf_counter()
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._make_cache_key(query, context)
        if cache_key in self.query_cache:
            return self.query_cache[cache_key]
        
        # ç¬¬1-4å±‚å¯ä»¥å¹¶è¡Œï¼ˆéƒ½æ˜¯ç‹¬ç«‹çš„ç´¢å¼•æŸ¥è¯¢ï¼‰
        layer1_4_tasks = [
            asyncio.get_event_loop().run_in_executor(
                self.executor, self._exact_match, query
            ),
            asyncio.get_event_loop().run_in_executor(
                self.executor, self._alias_match, query
            ),
            asyncio.get_event_loop().run_in_executor(
                self.executor, self._trigger_match, query
            ),
            asyncio.get_event_loop().run_in_executor(
                self.executor, self._time_range_scan, query, context
            ),
        ]
        
        # ç¬¬6å±‚å‘é‡æ£€ç´¢ï¼ˆç‹¬ç«‹ï¼‰
        vector_task = asyncio.get_event_loop().run_in_executor(
            self.executor, self._vector_search, query
        )
        
        # ç­‰å¾…æ‰€æœ‰å¹¶è¡Œä»»åŠ¡
        results_1_4 = await asyncio.gather(*layer1_4_tasks)
        vector_results = await vector_task
        
        # åˆå¹¶ç»“æœ
        all_results = []
        for layer_results in results_1_4:
            all_results.extend(layer_results)
        all_results.extend(vector_results)
        
        # ç¬¬5å±‚å…³ç³»æ‰©å±•ï¼ˆä¾èµ–å‰é¢çš„ç»“æœï¼‰
        if all_results:
            related = await asyncio.get_event_loop().run_in_executor(
                self.executor, self._expand_relations, all_results
            )
            all_results.extend(related)
        
        # ç¬¬7å±‚N-gramå…œåº•ï¼ˆåªåœ¨å¿…è¦æ—¶è§¦å‘ï¼‰
        if self._need_fallback(all_results, query):
            ngram_results = await asyncio.get_event_loop().run_in_executor(
                self.executor, self._ngram_search, query
            )
            all_results.extend(ngram_results)
        
        # å»é‡æ’åº
        final_results = self._deduplicate_and_rank(all_results)
        
        elapsed = time.perf_counter() - start_time
        result = RetrievalResult(
            results=final_results,
            elapsed_ms=elapsed * 1000,
            layers_hit=self._get_layers_hit(results_1_4, vector_results),
        )
        
        # ç¼“å­˜ç»“æœ
        self.query_cache[cache_key] = result
        
        return result
    
    def _exact_match(self, query: str) -> List:
        """O(1) ç²¾ç¡®åŒ¹é… - é¢„æœŸ<5ms"""
        # ç›´æ¥å“ˆå¸ŒæŸ¥æ‰¾
        entity = self.engine.entity_index.get_by_name(query)
        return [entity] if entity else []
    
    def _alias_match(self, query: str) -> List:
        """O(1) åˆ«ååŒ¹é… - é¢„æœŸ<5ms"""
        return self.engine.entity_index.search(query)[:10]
    
    def _trigger_match(self, query: str) -> List:
        """è§¦å‘è¯åŒ¹é… - é¢„æœŸ<20ms"""
        keywords = self.engine.entity_extractor.extract_keywords(query)
        return self.engine.inverted_index.search_any(keywords)[:20]
    
    def _vector_search(self, query: str) -> List:
        """å‘é‡æ£€ç´¢ - é¢„æœŸ<100msï¼ˆFAISSä¼˜åŒ–åï¼‰"""
        return self.engine.vector_index.search(query, top_k=20)
    
    def _ngram_search(self, query: str) -> List:
        """N-gramå…œåº• - é¢„æœŸ<200ms"""
        return self.engine.ngram_index.search(query)[:50]
    
    def _need_fallback(self, current_results, query) -> bool:
        """æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦å…œåº•"""
        if not current_results:
            return True
        if '"' in query:  # ç”¨æˆ·è¦ç²¾ç¡®æœç´¢
            return True
        if max(r.get('score', 0) for r in current_results) < 0.3:
            return True
        return False


class LRUCache:
    """ç®€å•LRUç¼“å­˜"""
    def __init__(self, maxsize=100):
        from collections import OrderedDict
        self.cache = OrderedDict()
        self.maxsize = maxsize
    
    def __contains__(self, key):
        return key in self.cache
    
    def __getitem__(self, key):
        self.cache.move_to_end(key)
        return self.cache[key]
    
    def __setitem__(self, key, value):
        if len(self.cache) >= self.maxsize:
            self.cache.popitem(last=False)
        self.cache[key] = value
```

### 7.5.3 å¼‚æ­¥å†™å…¥ç®¡é“

```python
# recall/storage/async_writer.py
"""å¼‚æ­¥å†™å…¥ - ä¸é˜»å¡ä¸»æµç¨‹"""

import asyncio
from queue import Queue
from threading import Thread
from typing import Dict, Any

class AsyncWritePipeline:
    """å¼‚æ­¥å†™å…¥ç®¡é“ - process_turnåªéœ€<100ms"""
    
    def __init__(self, engine):
        self.engine = engine
        self.write_queue = Queue(maxsize=1000)
        self.index_queue = Queue(maxsize=1000)
        
        # å¯åŠ¨åå°å†™å…¥çº¿ç¨‹
        self.writer_thread = Thread(target=self._writer_loop, daemon=True)
        self.indexer_thread = Thread(target=self._indexer_loop, daemon=True)
        self.writer_thread.start()
        self.indexer_thread.start()
    
    def enqueue_turn(self, turn_data: Dict[str, Any]) -> int:
        """å¿«é€Ÿå…¥é˜Ÿè¿”å›turnå· - <10ms"""
        turn_number = self.engine.volume_manager.get_next_turn_number()
        turn_data['turn'] = turn_number
        
        # ç«‹å³å†™å…¥L2å·¥ä½œè®°å¿†ï¼ˆå†…å­˜æ“ä½œï¼Œæå¿«ï¼‰
        self._update_working_memory(turn_data)
        
        # å¼‚æ­¥å†™å…¥L3å’Œç´¢å¼•
        self.write_queue.put_nowait(turn_data)
        self.index_queue.put_nowait(turn_data)
        
        return turn_number
    
    def _writer_loop(self):
        """åå°å†™å…¥å¾ªç¯"""
        batch = []
        while True:
            try:
                item = self.write_queue.get(timeout=0.5)
                batch.append(item)
                
                # æ‰¹é‡å†™å…¥ï¼ˆæ¯10æ¡æˆ–0.5ç§’ï¼‰
                if len(batch) >= 10:
                    self._flush_batch(batch)
                    batch = []
            except:
                if batch:
                    self._flush_batch(batch)
                    batch = []
    
    def _indexer_loop(self):
        """åå°ç´¢å¼•å¾ªç¯"""
        batch = []
        while True:
            try:
                item = self.index_queue.get(timeout=0.5)
                batch.append(item)
                
                if len(batch) >= 5:
                    self._index_batch(batch)
                    batch = []
            except:
                if batch:
                    self._index_batch(batch)
                    batch = []
    
    def _flush_batch(self, batch):
        """æ‰¹é‡å†™å…¥å­˜å‚¨"""
        for turn in batch:
            self.engine.volume_manager.append_turn(turn)
    
    def _index_batch(self, batch):
        """æ‰¹é‡æ›´æ–°ç´¢å¼•"""
        for turn in batch:
            content = turn['user'] + ' ' + turn['assistant']
            
            # æ›´æ–°å„ç´¢å¼•
            keywords = self.engine.entity_extractor.extract_keywords(content)
            self.engine.inverted_index.add_batch(keywords, turn['turn'])
            
            self.engine.ngram_index.add(turn['turn'], content)
            self.engine.vector_index.add_text(turn['turn'], content)
    
    def _update_working_memory(self, turn_data):
        """æ›´æ–°å·¥ä½œè®°å¿†ï¼ˆåŒæ­¥ï¼Œä½†æå¿«ï¼‰"""
        entities = self.engine.entity_extractor.extract(
            turn_data['user'] + ' ' + turn_data['assistant']
        )
        for entity in entities[:10]:  # é™åˆ¶æ•°é‡
            self.engine.working.update_with_delta_rule(entity)
```

### 7.5.4 é¢„çƒ­ä¸æ‡’åŠ è½½

```python
# recall/utils/warmup.py
"""é¢„çƒ­ç³»ç»Ÿ - åŠ é€Ÿåç»­è¯·æ±‚"""

import threading
import time

class SystemWarmup:
    """ç³»ç»Ÿé¢„çƒ­
    
    æ³¨æ„ï¼šé¦–æ¬¡å†·å¯åŠ¨éœ€è¦åŠ è½½NLPæ¨¡å‹ï¼Œçº¦10-15ç§’ã€‚
    é¢„çƒ­æ˜¯ä¸ºäº†è®©è¿™ä¸ªè¿‡ç¨‹åœ¨åå°è¿›è¡Œï¼Œä¸é˜»å¡ç”¨æˆ·é¦–æ¬¡äº¤äº’ã€‚
    åç»­å¯åŠ¨ä¼šå¿«å¾ˆå¤šï¼ˆæ¨¡å‹ä¼šè¢«OSç¼“å­˜ï¼‰ã€‚
    """
    
    @staticmethod
    def warmup_async(engine):
        """åå°é¢„çƒ­ï¼ˆä¸é˜»å¡ç”¨æˆ·ï¼‰"""
        thread = threading.Thread(target=lambda: SystemWarmup._do_warmup(engine))
        thread.daemon = True
        thread.start()
    
    @staticmethod
    def _do_warmup(engine):
        """å®é™…é¢„çƒ­æ“ä½œ"""
        start = time.time()
        
        # 1. é¢„åŠ è½½æœ€è¿‘2å·
        engine.volume_manager.preload_recent(num_volumes=2)
        
        # 2. é¢„çƒ­å‘é‡æ¨¡å‹ï¼ˆç¬¬ä¸€æ¬¡encodeè¾ƒæ…¢ï¼Œçº¦3-5ç§’ï¼‰
        engine.vector_index.encode("é¢„çƒ­æ–‡æœ¬ warmup text")
        
        # 3. é¢„åŠ è½½çƒ­é—¨å®ä½“åˆ°ç¼“å­˜
        hot_entities = engine.entity_index.get_top_entities(limit=100)
        engine.hot_entity_cache = {e.name: e for e in hot_entities}
        
        # 4. é¢„åŠ è½½FAISSç´¢å¼•åˆ°å†…å­˜
        if hasattr(engine.vector_index, 'index'):
            # è§¦å‘mmapåŠ è½½
            _ = engine.vector_index.index.ntotal
        
        elapsed = time.time() - start
        print(f"[Recall] ç³»ç»Ÿé¢„çƒ­å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f}s")


class LazyLoader:
    """æ‡’åŠ è½½è£…é¥°å™¨"""
    
    def __init__(self, loader_func):
        self.loader_func = loader_func
        self.loaded = False
        self.value = None
    
    def get(self):
        if not self.loaded:
            self.value = self.loader_func()
            self.loaded = True
        return self.value
```

### 7.5.5 LLMå®¢æˆ·ç«¯å°è£…

```python
# recall/utils/llm_client.py
"""LLMè°ƒç”¨å°è£… - æ”¯æŒç”¨æˆ·è‡ªå·±çš„API Key"""

import os
from typing import Optional, Dict, List, Any
from dataclasses import dataclass

@dataclass
class LLMConfig:
    """LLMé…ç½®"""
    api_key: Optional[str] = None       # API Keyï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
    base_url: Optional[str] = None      # APIåŸºç¡€URLï¼ˆè‡ªå®šä¹‰ç«¯ç‚¹ï¼‰
    model: str = "gpt-4o-mini"          # é»˜è®¤æ¨¡å‹
    timeout: int = 30                    # è¶…æ—¶ç§’æ•°
    max_retries: int = 2                # æœ€å¤§é‡è¯•æ¬¡æ•°


class LLMClient:
    """
    è½»é‡çº§LLMå®¢æˆ·ç«¯
    
    æ”¯æŒå¤šç§åç«¯ï¼š
    - OpenAI API
    - Azure OpenAI
    - Claude (é€šè¿‡ litellm)
    - æœ¬åœ°æ¨¡å‹ (Ollama, vLLM ç­‰)
    - ç”¨æˆ·è‡ªå®šä¹‰ç«¯ç‚¹
    
    è®¾è®¡åŸåˆ™ï¼š
    - ä½¿ç”¨ç”¨æˆ·è‡ªå·±çš„API Keyï¼ˆä¸ç»è¿‡ä»»ä½•ç¬¬ä¸‰æ–¹æœåŠ¡å™¨ï¼‰
    - æ‰€æœ‰æ•°æ®æœ¬åœ°å¤„ç†
    - å…¼å®¹ OpenAI SDK æ¥å£
    """
    
    def __init__(self, config: Optional[LLMConfig] = None):
        self.config = config or LLMConfig()
        self._client = None
        
    @property
    def chat(self):
        """è¿”å› chat æ¥å£ï¼ˆå…¼å®¹ openai.chatï¼‰"""
        return self
    
    @property
    def completions(self):
        """è¿”å› completions æ¥å£"""
        return self
    
    def _get_client(self):
        """æ‡’åŠ è½½ OpenAI å®¢æˆ·ç«¯"""
        if self._client is None:
            try:
                from openai import OpenAI
                
                api_key = self.config.api_key or os.getenv('OPENAI_API_KEY')
                if not api_key:
                    raise ValueError(
                        "æœªæ‰¾åˆ° API Keyã€‚è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡æˆ–åœ¨é…ç½®ä¸­æŒ‡å®š api_key"
                    )
                
                self._client = OpenAI(
                    api_key=api_key,
                    base_url=self.config.base_url,
                    timeout=self.config.timeout,
                    max_retries=self.config.max_retries,
                )
            except ImportError:
                # å›é€€åˆ° litellm
                try:
                    import litellm
                    self._client = litellm
                except ImportError:
                    raise ImportError(
                        "è¯·å®‰è£… openai æˆ– litellm: pip install openai æˆ– pip install litellm"
                    )
        return self._client
    
    def create(
        self, 
        model: Optional[str] = None,
        messages: List[Dict[str, str]] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Any:
        """
        åˆ›å»ºèŠå¤©å®Œæˆ
        
        Args:
            model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹ï¼‰
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            API å“åº”å¯¹è±¡
        """
        client = self._get_client()
        model = model or self.config.model
        
        if hasattr(client, 'chat'):
            # OpenAI SDK
            return client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
        else:
            # litellm
            return client.completion(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
    
    def simple_call(self, prompt: str, system: str = None) -> str:
        """
        ç®€å•è°ƒç”¨æ¥å£ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            system: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
        
        Returns:
            æ¨¡å‹å›å¤æ–‡æœ¬
        """
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})
        
        response = self.create(messages=messages)
        return response.choices[0].message.content


def create_llm_client(
    api_key: str = None,
    base_url: str = None,
    model: str = "gpt-4o-mini"
) -> LLMClient:
    """
    åˆ›å»º LLM å®¢æˆ·ç«¯çš„ä¾¿æ·å‡½æ•°
    
    Examples:
        # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ API Key
        client = create_llm_client()
        
        # æ˜¾å¼æŒ‡å®š API Key
        client = create_llm_client(api_key="sk-...")
        
        # ä½¿ç”¨è‡ªå®šä¹‰ç«¯ç‚¹ï¼ˆå¦‚ Ollamaï¼‰
        client = create_llm_client(
            base_url="http://localhost:11434/v1",
            model="llama3"
        )
    """
    config = LLMConfig(
        api_key=api_key,
        base_url=base_url,
        model=model
    )
    return LLMClient(config)
```

### 7.5.6 æ€§èƒ½ç›‘æ§

```python
# recall/utils/perf_monitor.py
"""æ€§èƒ½ç›‘æ§ - ç¡®ä¿æ»¡è¶³SLA"""

import time
from dataclasses import dataclass, field
from typing import Dict, List
from collections import deque

@dataclass
class PerfMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    operation: str
    elapsed_ms: float
    timestamp: float
    success: bool = True

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    # SLAç›®æ ‡ï¼ˆæ¯«ç§’ï¼‰
    SLA_TARGETS = {
        'retrieve': 500,      # æ£€ç´¢<500ms
        'process_turn': 100,  # å­˜å‚¨<100ms
        'build_context': 600, # ä¸Šä¸‹æ–‡æ„å»º<600ms
        'total': 1000,        # Recallæ€»å»¶è¿Ÿ<1ç§’
    }
    
    def __init__(self):
        self.metrics: Dict[str, deque] = {}
        self.violations = []
    
    def record(self, operation: str, elapsed_ms: float):
        """è®°å½•æ€§èƒ½æ•°æ®"""
        if operation not in self.metrics:
            self.metrics[operation] = deque(maxlen=1000)
        
        self.metrics[operation].append(PerfMetrics(
            operation=operation,
            elapsed_ms=elapsed_ms,
            timestamp=time.time(),
        ))
        
        # æ£€æŸ¥SLAè¿è§„
        target = self.SLA_TARGETS.get(operation, 1000)
        if elapsed_ms > target:
            self.violations.append(PerfMetrics(
                operation=operation,
                elapsed_ms=elapsed_ms,
                timestamp=time.time(),
                success=False,
            ))
    
    def get_p99(self, operation: str) -> float:
        """è·å–P99å»¶è¿Ÿ"""
        if operation not in self.metrics:
            return 0
        
        values = sorted([m.elapsed_ms for m in self.metrics[operation]])
        if not values:
            return 0
        
        idx = int(len(values) * 0.99)
        return values[min(idx, len(values)-1)]
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡"""
        stats = {}
        for op, metrics in self.metrics.items():
            if not metrics:
                continue
            values = [m.elapsed_ms for m in metrics]
            stats[op] = {
                'avg_ms': sum(values) / len(values),
                'p50_ms': sorted(values)[len(values)//2],
                'p99_ms': self.get_p99(op),
                'count': len(values),
                'sla_target_ms': self.SLA_TARGETS.get(op, 1000),
            }
        return stats
    
    def is_healthy(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¥åº·ï¼ˆP99æ»¡è¶³SLAï¼‰"""
        for op, target in self.SLA_TARGETS.items():
            if self.get_p99(op) > target * 1.5:  # å…è®¸50%ä½™é‡
                return False
        return True


# å…¨å±€ç›‘æ§å®ä¾‹
perf_monitor = PerformanceMonitor()

def timed(operation: str):
    """æ€§èƒ½è®¡æ—¶è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            start = time.perf_counter()
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                elapsed = (time.perf_counter() - start) * 1000
                perf_monitor.record(operation, elapsed)
        return wrapper
    return decorator
```

### 7.5.6 æ€§èƒ½ä¼˜åŒ–æ€»ç»“

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å“åº”æ—¶é—´åˆ†è§£ï¼ˆç›®æ ‡ï¼š3-5ç§’ï¼‰                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                    â”‚
â”‚  ç”¨æˆ·è¾“å…¥                                                           â”‚
â”‚      â”‚                                                             â”‚
â”‚      â–¼                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚ Recall æ£€ç´¢          â”‚ â† ç›®æ ‡: <500ms                           â”‚
â”‚  â”‚  â”œâ”€ å¹¶è¡Œ1-4å±‚: ~50ms â”‚                                          â”‚
â”‚  â”‚  â”œâ”€ å‘é‡æ£€ç´¢: ~100ms â”‚                                          â”‚
â”‚  â”‚  â”œâ”€ å…³ç³»æ‰©å±•: ~50ms  â”‚                                          â”‚
â”‚  â”‚  â””â”€ å…œåº•(å¯é€‰): ~200msâ”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚      â”‚                                                             â”‚
â”‚      â–¼                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚ ä¸Šä¸‹æ–‡ç»„è£…           â”‚ â† ç›®æ ‡: <100ms                           â”‚
â”‚  â”‚  â””â”€ Tokenè®¡æ•°+è£å‰ª   â”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚      â”‚                                                             â”‚
â”‚      â–¼                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚ LLM è°ƒç”¨             â”‚ â† 2-4ç§’ï¼ˆå¤–éƒ¨æœåŠ¡ï¼‰                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚      â”‚                                                             â”‚
â”‚      â–¼                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚ å¼‚æ­¥å­˜å‚¨             â”‚ â† <100msï¼ˆä¸é˜»å¡å“åº”ï¼‰                    â”‚
â”‚  â”‚  â””â”€ åå°å†™å…¥é˜Ÿåˆ—     â”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚      â”‚                                                             â”‚
â”‚      â–¼                                                             â”‚
â”‚  å“åº”è¿”å›ç»™ç”¨æˆ·                                                      â”‚
â”‚                                                                    â”‚
â”‚  æ€»è®¡ï¼šRecalléƒ¨åˆ† <700ms + LLM 2-4ç§’ = 3-5ç§’ âœ…                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å…«ã€å®Œæ•´æ•°æ®æµ

### 8.1 è¾“å…¥å¤„ç†æµç¨‹

```
ç”¨æˆ·è¾“å…¥
    â”‚
    â”œâ”€â†’ [é›¶é…ç½®] åœºæ™¯è‡ªåŠ¨æ£€æµ‹
    â”‚   â””â”€ roleplay / coding / general
    â”‚
    â”œâ”€â†’ [é¢„å¤„ç†]
    â”‚   â”œâ”€ æŒ‡ä»£æ¶ˆè§£ï¼š"ä»–" â†’ "ç¥ç§˜è€äºº"
    â”‚   â”œâ”€ æ—¶é—´è§£æï¼š"æœ€å¼€å§‹" â†’ turns 1-100
    â”‚   â””â”€ å®ä½“è¯†åˆ«ï¼šæå–æŸ¥è¯¢ä¸­çš„å®ä½“
    â”‚
    â”œâ”€â†’ [8å±‚æ£€ç´¢]
    â”‚   â”œâ”€ 1. ç²¾ç¡®åŒ¹é…
    â”‚   â”œâ”€ 2. åˆ«ååŒ¹é…
    â”‚   â”œâ”€ 3. è§¦å‘ç‰¹å¾
    â”‚   â”œâ”€ 4. å…³ç³»æ‰©å±•
    â”‚   â”œâ”€ 5. æ—¶é—´èŒƒå›´
    â”‚   â”œâ”€ 6. å‘é‡è¯­ä¹‰
    â”‚   â”œâ”€ 7. N-gramå…œåº• â† 100%ä¸æ¼çš„ä¿è¯
    â”‚   â””â”€ 8. å¼•å¯¼è¿½é—®
    â”‚
    â”œâ”€â†’ [ä¼ç¬”æ£€æŸ¥]
    â”‚   â”œâ”€ ç›¸å…³çš„æœªè§£å†³ä¼ç¬”
    â”‚   â””â”€ éœ€è¦æé†’çš„æ—§ä¼ç¬”
    â”‚
    â”œâ”€â†’ [ä»£ç åœºæ™¯] é¢å¤–å¤„ç†
    â”‚   â”œâ”€ ç¬¦å·æŸ¥æ‰¾
    â”‚   â”œâ”€ ä¾èµ–åˆ†æ
    â”‚   â””â”€ ç›¸å…³æ–‡ä»¶
    â”‚
    â””â”€â†’ [ä¸Šä¸‹æ–‡ç»„è£…]
        â”œâ”€ [å¤´éƒ¨] L0æ ¸å¿ƒè®¾å®š
        â”œâ”€ [æ£€ç´¢] ç›¸å…³è®°å¿† + ä¼ç¬”
        â”œâ”€ [ç¼“å†²] æœ€è¿‘å¯¹è¯
        â”œâ”€ [å½“å‰] ç”¨æˆ·è¾“å…¥
        â””â”€ [å°¾éƒ¨] è§„èŒƒæé†’
            â”‚
            â–¼
        å‘é€ç»™LLM
```

### 8.2 è¾“å‡ºå¤„ç†æµç¨‹

```
LLMè¾“å‡º
    â”‚
    â”œâ”€â†’ [ä¸€è‡´æ€§æ ¡éªŒ]
    â”‚   â”œâ”€ vs L0æ ¸å¿ƒè®¾å®š
    â”‚   â”œâ”€ vs L1é•¿æœŸè®°å¿†
    â”‚   â”œâ”€ vs æœ€è¿‘å¯¹è¯
    â”‚   â””â”€ vs ä»£ç é£æ ¼ï¼ˆå¦‚æœæ˜¯ä»£ç åœºæ™¯ï¼‰
    â”‚       â”‚
    â”‚       â””â”€â†’ å¦‚æœæœ‰å†²çª â†’ è­¦å‘Š/è¦æ±‚ä¿®æ­£
    â”‚
    â”œâ”€â†’ [ä¿¡æ¯æå–]
    â”‚   â”œâ”€ å®ä½“ï¼šæ–°å®ä½“ / çŠ¶æ€å˜åŒ–
    â”‚   â”œâ”€ äº‹ä»¶ï¼šå…³é”®åŠ¨ä½œ
    â”‚   â””â”€ ä¼ç¬”ï¼šæ–°ä¼ç¬” / ä¼ç¬”è§£å†³
    â”‚
    â”œâ”€â†’ [æƒŠè®¶åº¦è®¡ç®—]
    â”‚   â””â”€ å†³å®šä¿¡æ¯çš„å¤„ç†æ–¹å¼
    â”‚
    â”œâ”€â†’ [å†™å…¥å­˜å‚¨]
    â”‚   â”œâ”€ L3: å®Œæ•´åŸæ–‡ï¼ˆæ— æ¡ä»¶ï¼‰
    â”‚   â”œâ”€ L2: é«˜/ä¸­æƒŠè®¶åº¦ä¿¡æ¯ï¼ˆDelta Ruleç®¡ç†ï¼‰
    â”‚   â””â”€ Archive: æ°¸ä¹…å­˜æ¡£
    â”‚
    â”œâ”€â†’ [ç´¢å¼•æ›´æ–°]
    â”‚   â”œâ”€ å®ä½“ç´¢å¼•
    â”‚   â”œâ”€ å€’æ’ç´¢å¼•
    â”‚   â”œâ”€ N-gramç´¢å¼•
    â”‚   â”œâ”€ å‘é‡ç´¢å¼•
    â”‚   â””â”€ ä»£ç ç´¢å¼•ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
    â”‚
    â””â”€â†’ [å·©å›ºæ£€æŸ¥]
        â””â”€ æ»¡è¶³æ¡ä»¶çš„L2æ¡ç›® â†’ å·©å›ºåˆ°L1
```

## å…«ç‚¹äº”ã€ç”¨æˆ·ç•Œé¢ï¼ˆå…¨ä¸­æ–‡ï¼Œäººè¯æ–‡æ¡ˆï¼‰

### ST æ’ä»¶çŠ¶æ€æŒ‡ç¤ºå™¨

```javascript
// çŠ¶æ€æ–‡æ¡ˆï¼ˆäººè¯ï¼‰
const statusText = {
    'normal': 'ğŸ§  è®°å¿†æ­£å¸¸',
    'processing': 'ğŸ§  æ­£åœ¨è®°å¿†...',
    'warning': 'âš ï¸ å‘ç°çŸ›ç›¾',
};

// ç»Ÿè®¡é¢æ¿ï¼ˆç®€æ´ç‰ˆï¼‰
function renderStats(stats) {
    return `
    <div class="recall-panel">
        <h3>ğŸ§  Recall è®°å¿†çŠ¶æ€</h3>
        <div class="stats">
            <div>${stats.memories} ä»¶äº‹è¢«è®°ä½äº†</div>
            <div>${stats.entities} ä¸ªè§’è‰²/ç‰©å“</div>
            <div>${stats.foreshadowing} ä¸ªä¼ç¬”å¾…æ­æ™“</div>
        </div>
        <input placeholder="æœç´¢è®°å¿†...è¯•è¯•è§’è‰²åæˆ–"æœ€å¼€å§‹"" />
        <small>ğŸ’¡ AIä¼šè‡ªåŠ¨è®°ä½æ‰€æœ‰é‡è¦çš„äº‹ï¼Œä½ ä¸éœ€è¦åšä»»ä½•äº‹</small>
    </div>
    `;
}
```

### CLI ä¸­æ–‡æ–‡æ¡ˆ

```python
CLI_MESSAGES = {
    'welcome': 'ğŸ§  Recall - AIæ°¸ä¹…è®°å¿†ç³»ç»Ÿ\nè®©AIæ°¸è¿œä¸ä¼šå¿˜è®°ä½ è¯´è¿‡çš„æ¯ä¸€å¥è¯ã€‚\nğŸ“¦ çº¯æœ¬åœ°å­˜å‚¨ï¼Œæ•°æ®å®Œå…¨ç§æœ‰ã€‚',
    'api_key_prompt': 'è¯·è¾“å…¥ä½ çš„ AI API keyï¼ˆæ”¯æŒ OpenAI/Claude ç­‰ï¼‰ï¼š',
    'init_success': 'âœ… åˆå§‹åŒ–æˆåŠŸï¼ç°åœ¨å¯ä»¥ç”¨ recall chat å¼€å§‹å¯¹è¯äº†ã€‚',
    'search_empty': 'ğŸ˜… æ²¡æ‰¾åˆ°ç›¸å…³è®°å¿†ï¼Œè¯•è¯•å…¶ä»–å…³é”®è¯ï¼Ÿ',
    'help': '''
å¯ç”¨å‘½ä»¤ï¼š
  /search <è¯>  - æœç´¢è®°å¿†
  /memory      - æŸ¥çœ‹ç»Ÿè®¡  
  /foreshadow  - æŸ¥çœ‹ä¼ç¬”
  /quit        - é€€å‡º
''',
}
```

---

## ä¹ã€ä¸»å¼•æ“å®Œæ•´å®ç°ï¼ˆæ ¸å¿ƒå…¥å£ï¼‰

```python
# recall/engine.py
"""
RecallEngine - æ ¸å¿ƒå¼•æ“
è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„å…¥å£ï¼Œæ‰€æœ‰åŠŸèƒ½é€šè¿‡æ­¤ç±»è°ƒç”¨
"""

import os
import threading
from typing import Optional, List, Dict, Any
from datetime import datetime
from dataclasses import dataclass

class RecallEngine:
    """Recall æ ¸å¿ƒå¼•æ“ - ç»Ÿä¸€å…¥å£
    
    çº¯æœ¬åœ°è¿è¡Œï¼Œç”¨æˆ·ä½¿ç”¨è‡ªå·±çš„ API key è°ƒç”¨å¤§æ¨¡å‹ã€‚
    Recall åªè´Ÿè´£è®°å¿†å­˜å‚¨å’Œæ£€ç´¢ã€‚
    
    æ‰€æœ‰æ•°æ®å­˜å‚¨åœ¨é¡¹ç›®ç›®å½•å†…ï¼Œåˆ é™¤é¡¹ç›®æ–‡ä»¶å¤¹å³å¯å®Œå…¨å¸è½½ã€‚
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        data_path: str = None,         # é»˜è®¤ä¸º ./recall_data/data
        scene_type: str = 'auto',      # auto | roleplay | coding | general
        lightweight: bool = False,     # è½»é‡æ¨¡å¼
    ):
        # åˆå§‹åŒ–ç¯å¢ƒï¼ˆç¡®ä¿æ‰€æœ‰ç¼“å­˜éƒ½åœ¨é¡¹ç›®ç›®å½•å†…ï¼‰
        from .init import RecallInit
        RecallInit.setup_environment()
        
        self.api_key = api_key or os.environ.get('OPENAI_API_KEY')
        # é»˜è®¤ä½¿ç”¨é¡¹ç›®ç›®å½•ä¸‹çš„ recall_data/data
        self.data_path = data_path or os.path.join(RecallInit.get_data_root(), 'data')
        self.data_path = os.path.abspath(self.data_path)
        self.scene_type = scene_type
        self.lightweight = lightweight
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        RecallInit.ensure_directories()
        
        # åˆå§‹åŒ–å„ç»„ä»¶
        self._init_components()
    
    def _init_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        # å­˜å‚¨å±‚
        from .storage.volume_manager import VolumeManager
        from .storage.layer0_core import CoreSettings
        from .storage.layer1_consolidated import ConsolidatedMemory
        from .storage.layer2_working import WorkingMemory
        
        self.volume_manager = VolumeManager(self.data_path)
        self.core_settings = CoreSettings.load(self.data_path)
        self.consolidated = ConsolidatedMemory(self.data_path)
        self.working = WorkingMemory(capacity=200)
        
        # ç´¢å¼•å±‚
        from .index.entity_index import EntityIndex
        from .index.inverted_index import InvertedIndex
        from .index.ngram_index import OptimizedNgramIndex
        
        self.entity_index = EntityIndex(self.data_path)
        self.inverted_index = InvertedIndex(self.data_path)
        self.ngram_index = OptimizedNgramIndex()
        
        # å‘é‡ç´¢å¼•ï¼ˆè½»é‡æ¨¡å¼è·³è¿‡ï¼‰
        if not self.lightweight:
            from .index.vector_index import VectorIndex
            self.vector_index = VectorIndex(self.data_path)
        else:
            self.vector_index = None  # è½»é‡æ¨¡å¼ä¸åŠ è½½å‘é‡ç´¢å¼•
        
        # æ£€ç´¢å±‚
        from .retrieval.eight_layer import EightLayerRetrieval
        from .retrieval.context_builder import ContextBuilder
        
        self.retrieval = EightLayerRetrieval(self, lightweight=self.lightweight)
        self.context_builder = ContextBuilder(self)
        
        # å¤„ç†å™¨
        if not self.lightweight:
            from .processor.entity_extractor import EntityExtractor
            self.entity_extractor = EntityExtractor()
        else:
            from .config import LightweightEntityExtractor
            self.entity_extractor = LightweightEntityExtractor()
        
        # ä¼ç¬”è¿½è¸ªå™¨ï¼ˆç»Ÿä¸€ä½¿ç”¨æ–°è®¾è®¡ï¼Œä¸åŒºåˆ†è½»é‡/æ ‡å‡†ï¼‰
        from .processor.foreshadowing import ForeshadowingTracker, ForeshadowingAnalyzer, ForeshadowingAnalyzerConfig
        fsh_storage = os.path.join(self.data_path, "foreshadowing.json")
        self.foreshadowing_tracker = ForeshadowingTracker(storage_path=fsh_storage)
        
        # ä¼ç¬”åˆ†æå™¨ï¼ˆå¯é€‰ LLM è¾…åŠ©ï¼‰
        fsh_config = ForeshadowingAnalyzerConfig.manual()  # é»˜è®¤æ‰‹åŠ¨æ¨¡å¼
        self.foreshadowing_analyzer = ForeshadowingAnalyzer(
            config=fsh_config,
            tracker=self.foreshadowing_tracker
        )
        
        from .processor.consistency import ConsistencyChecker
        self.consistency_checker = ConsistencyChecker(self.core_settings, self.consolidated)
        
        # åœºæ™¯æ£€æµ‹å™¨
        from .processor.scenario import ScenarioDetector
        self.scenario_detector = ScenarioDetector()
        
        # é¢„åŠ è½½çƒ­æ•°æ®
        self.volume_manager.preload_recent()
    
    def process_turn(
        self,
        user_input: str,
        assistant_output: str,
        metadata: Optional[Dict] = None
    ) -> 'ProcessResult':
        """
        å¤„ç†ä¸€è½®å¯¹è¯ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            assistant_output: AIè¾“å‡º
            metadata: å¯é€‰å…ƒæ•°æ®
        
        Returns:
            ProcessResult: å¤„ç†ç»“æœ
        """
        metadata = metadata or {}
        
        # 1. è‡ªåŠ¨æ£€æµ‹åœºæ™¯
        if self.scene_type == 'auto':
            detected = self.scenario_detector.detect(user_input, [])
            metadata['scenario'] = detected
        
        # 2. å­˜å‚¨åŸæ–‡ï¼ˆL3ï¼Œæ°¸ä¸å‹ç¼©ï¼‰
        turn_number = self.volume_manager.append_turn({
            'turn': self.volume_manager.manifest.get('total_turns', 0),
            'timestamp': datetime.now().isoformat(),
            'user': user_input,
            'assistant': assistant_output,
            'metadata': metadata,
        })
        
        # 3. æå–å®ä½“
        entities = self.entity_extractor.extract(user_input + ' ' + assistant_output)
        
        # 4. æ›´æ–°ç´¢å¼•ï¼ˆå¼‚æ­¥æ‰§è¡Œä¸é˜»å¡ï¼‰
        self._update_indexes_async(turn_number, user_input, assistant_output, entities)
        
        # 5. ä¼ç¬”å¤„ç†ï¼ˆLLM æ¨¡å¼ä¼šè‡ªåŠ¨åˆ†æï¼ŒMANUAL æ¨¡å¼è¿”å› Noneï¼‰
        fsh_analysis = self.foreshadowing_analyzer.on_new_turn(
            content=assistant_output,
            role="assistant",
            user_id=metadata.get('user_id', 'default')
        )
        
        # è·å–éœ€è¦æé†’çš„ä¼ç¬”
        fsh_reminders = self.foreshadowing_tracker.get_reminders(
            current_turn=turn_number,
            user_id=metadata.get('user_id')
        )
        
        # 6. ä¸€è‡´æ€§æ ¡éªŒ
        consistency = self.consistency_checker.check_output(
            assistant_output, {'scenario': metadata.get('scenario')}
        )
        
        # 7. æ›´æ–°å·¥ä½œè®°å¿†ï¼ˆL2ï¼‰
        for entity in entities:
            self.working.update_with_delta_rule(entity)
        
        return ProcessResult(
            turn_number=turn_number,
            entities_detected=entities,
            foreshadowing_analysis=fsh_analysis,  # LLM åˆ†æç»“æœï¼ˆMANUAL æ¨¡å¼ä¸º Noneï¼‰
            foreshadowing_reminders=fsh_reminders,  # éœ€è¦æé†’çš„ä¼ç¬”
            consistency_result=consistency,
        )
    
    def build_context(
        self,
        user_input: str,
        max_tokens: int = 8000
    ) -> 'ContextResult':
        """
        ä¸ºç”¨æˆ·è¾“å…¥æ„å»ºä¸Šä¸‹æ–‡
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            max_tokens: æœ€å¤§tokené¢„ç®—
        
        Returns:
            ContextResult: åŒ…å«ç»„è£…å¥½çš„ä¸Šä¸‹æ–‡
        """
        # 1. 8å±‚æ£€ç´¢
        retrieval_result = self.retrieval.retrieve(user_input, {
            'current_turn': self.volume_manager.manifest.get('total_turns', 0),
        })
        
        # 2. ç»„è£…ä¸Šä¸‹æ–‡
        context = self.context_builder.build(
            user_input=user_input,
            retrieved=retrieval_result,
            max_tokens=max_tokens,
        )
        
        return context
    
    def search(self, query: str, limit: int = 20) -> List[Dict]:
        """æœç´¢è®°å¿†"""
        result = self.retrieval.retrieve(query, {})
        return result.results[:limit]
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_turns': self.volume_manager.manifest.get('total_turns', 0),
            'total_entities': len(self.entity_index.all_entities()),
            'unresolved_foreshadowing': len(self.foreshadowing_tracker.active_foreshadowing),
            'storage_mb': self._calculate_storage_size(),
        }
    
    def _update_indexes_async(self, turn: int, user: str, assistant: str, entities: List):
        """å¼‚æ­¥æ›´æ–°ç´¢å¼•"""
        import threading
        import uuid
        from .index.entity_index import IndexedEntity
        
        def update():
            combined = user + ' ' + assistant
            
            # æ›´æ–°å®ä½“ç´¢å¼•ï¼ˆå°†ExtractedEntityè½¬æ¢ä¸ºIndexedEntityï¼‰
            for entity in entities:
                indexed = IndexedEntity(
                    id=str(uuid.uuid4()),
                    name=entity.name,
                    aliases=[],  # åç»­å¯é€šè¿‡åˆ«åå­¦ä¹ å¡«å……
                    entity_type=entity.entity_type,
                    turn_references=[turn]
                )
                self.entity_index.add(indexed)
            
            # æ›´æ–°å€’æ’ç´¢å¼•
            keywords = self.entity_extractor.extract_keywords(combined)
            for kw in keywords:
                self.inverted_index.add(kw, turn)
            
            # æ›´æ–°N-gramç´¢å¼•
            self.ngram_index.add(turn, combined)
            
            # æ›´æ–°å‘é‡ç´¢å¼•ï¼ˆè½»é‡æ¨¡å¼è·³è¿‡ï¼‰
            if self.vector_index is not None:
                embedding = self.vector_index.encode(combined)
                self.vector_index.add(turn, embedding)
        
        thread = threading.Thread(target=update)
        thread.start()
    
    def _calculate_storage_size(self) -> float:
        """è®¡ç®—å­˜å‚¨å¤§å°ï¼ˆMBï¼‰"""
        total = 0
        for root, dirs, files in os.walk(self.data_path):
            for f in files:
                total += os.path.getsize(os.path.join(root, f))
        return total / 1024 / 1024


@dataclass
class ProcessResult:
    """å¤„ç†ç»“æœ"""
    turn_number: int
    entities_detected: List
    new_foreshadowing: List
    foreshadowing_resolved: List
    consistency_result: Any

# æ³¨æ„ï¼šContextResult å®šä¹‰åœ¨ recall/retrieval/context_builder.py ä¸­
```

---

## åã€CLIå®Œæ•´å®ç°

```python
# recall/cli.py
"""
Recall CLI - å‘½ä»¤è¡Œå·¥å…·
"""

import click
import os
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

console = Console()

@click.group()
def main():
    """ğŸ§  Recall - AIæ°¸ä¹…è®°å¿†ç³»ç»Ÿ"""
    pass

@main.command()
@click.option('--lightweight', is_flag=True, help='è½»é‡æ¨¡å¼ï¼Œå†…å­˜å ç”¨çº¦80MB')
def init(lightweight):
    """åˆå§‹åŒ– Recall"""
    console.print("\nğŸ§  [bold]Recall - AIæ°¸ä¹…è®°å¿†ç³»ç»Ÿ[/bold]")
    console.print("=" * 40)
    console.print("\nğŸ“¦ çº¯æœ¬åœ°å­˜å‚¨ï¼Œæ•°æ®å®Œå…¨ç§æœ‰ã€‚")
    console.print("   éœ€è¦ä½ è‡ªå·±çš„ AI API key æ¥è°ƒç”¨å¤§æ¨¡å‹ã€‚\n")
    
    if lightweight:
        console.print("ğŸ’¡ [yellow]è½»é‡æ¨¡å¼[/yellow]ï¼šå†…å­˜å ç”¨çº¦ ~80MB")
        console.print("   - ç¦ç”¨å‘é‡è¯­ä¹‰æ£€ç´¢")
        console.print("   - ä¿ç•™å…³é”®è¯åŒ¹é…ã€ä¼ç¬”è¿½è¸ªã€è§„èŒƒæ£€æŸ¥\n")
    
    # è·å– API key
    api_key = os.environ.get('OPENAI_API_KEY')
    if not api_key:
        console.print("æ”¯æŒçš„ API æä¾›å•†ï¼š")
        console.print("  - OpenAI (sk-xxx)")
        console.print("  - Claude (sk-ant-xxx)")
        console.print("  - å…¶ä»–å…¼å®¹ OpenAI æ ¼å¼çš„ API\n")
        api_key = click.prompt("è¯·è¾“å…¥ä½ çš„ API key", hide_input=True)
    
    # ä¿å­˜é…ç½®
    # ä¿å­˜é…ç½®åˆ°é¡¹ç›®ç›®å½•
    from .init import RecallInit
    config_dir = RecallInit.get_data_root()
    os.makedirs(config_dir, exist_ok=True)
    
    import json
    with open(f'{config_dir}/config.json', 'w') as f:
        json.dump({
            'api_key': api_key,
            'initialized': True,
            'lightweight': lightweight,  # ä¿å­˜è½»é‡æ¨¡å¼è®¾ç½®
        }, f)
    
    console.print(f"\nâœ… [green]åˆå§‹åŒ–æˆåŠŸï¼[/green]")
    console.print(f"   æ•°æ®ç›®å½•ï¼š{config_dir}")
    if lightweight:
        console.print(f"   æ¨¡å¼ï¼šè½»é‡æ¨¡å¼ (~80MB)\n")
    else:
        console.print(f"   æ¨¡å¼ï¼šæ ‡å‡†æ¨¡å¼ (~565MB)\n")
    console.print("ç°åœ¨å¯ä»¥ä½¿ç”¨ [bold]recall chat[/bold] å¼€å§‹å¯¹è¯äº†ï¼")

@main.command()
def chat():
    """å¼€å§‹å¯¹è¯"""
    from .engine import RecallEngine
    
    config = _load_config()
    engine = RecallEngine(
        api_key=config.get('api_key'),
        lightweight=config.get('lightweight', False),  # ä½¿ç”¨é…ç½®ä¸­çš„è½»é‡æ¨¡å¼è®¾ç½®
    )
    
    mode_str = "è½»é‡æ¨¡å¼" if config.get('lightweight') else "æ ‡å‡†æ¨¡å¼"
    console.print(f"\nğŸ§  [bold]Recall Chat[/bold] ({mode_str})")
    console.print("è¾“å…¥ /help æŸ¥çœ‹å‘½ä»¤ï¼Œ/quit é€€å‡º\n")
    
    while True:
        try:
            user_input = console.input("[bold blue]ä½ : [/bold blue]")
        except (KeyboardInterrupt, EOFError):
            break
        
        if user_input.startswith('/'):
            if user_input == '/quit':
                break
            elif user_input == '/help':
                _show_help()
            elif user_input == '/stats':
                _show_stats(engine)
            elif user_input == '/foreshadow':
                _show_foreshadowing(engine)
            elif user_input.startswith('/search '):
                query = user_input[8:]
                _search(engine, query)
            continue
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = engine.build_context(user_input)
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨LLMï¼Œç®€åŒ–ç¤ºä¾‹ç›´æ¥è¿”å›
        console.print(f"\n[dim]ï¼ˆä¸Šä¸‹æ–‡å·²æ³¨å…¥ {context.token_count} tokensï¼‰[/dim]")
        console.print("[yellow]AI: [/yellow]ï¼ˆè¯·å°†ä¸Šä¸‹æ–‡å‘é€ç»™ä½ çš„LLMï¼‰\n")

@main.command()
@click.argument('query')
def search(query):
    """æœç´¢è®°å¿†"""
    from .engine import RecallEngine
    config = _load_config()
    engine = RecallEngine(api_key=config.get('api_key'))
    _search(engine, query)

@main.command()
def stats():
    """æŸ¥çœ‹ç»Ÿè®¡"""
    from .engine import RecallEngine
    config = _load_config()
    engine = RecallEngine(api_key=config.get('api_key'))
    _show_stats(engine)

def _load_config() -> dict:
    from .init import RecallInit
    config_path = os.path.join(RecallInit.get_data_root(), 'config.json')
    if os.path.exists(config_path):
        import json
        with open(config_path) as f:
            return json.load(f)
    return {}

def _show_help():
    console.print(Panel("""
å¯ç”¨å‘½ä»¤ï¼š
  /search <è¯>  - æœç´¢è®°å¿†
  /stats       - æŸ¥çœ‹ç»Ÿè®¡
  /foreshadow  - æŸ¥çœ‹ä¼ç¬”
  /quit        - é€€å‡º

ğŸ’¡ å°æŠ€å·§ï¼š
  - æœç´¢ "æœ€å¼€å§‹" å¯æ‰¾åˆ°æœ€æ—©çš„å¯¹è¯
  - AIä¼šè‡ªåŠ¨è®°å¿†ï¼Œæ— éœ€æ‰‹åŠ¨æ·»åŠ 
""", title="å¸®åŠ©"))

def _show_stats(engine):
    s = engine.get_stats()
    table = Table(title="ğŸ“Š Recall ç»Ÿè®¡")
    table.add_column("æŒ‡æ ‡", style="cyan")
    table.add_column("æ•°å€¼", style="green")
    table.add_row("è®°å¿†æ€»æ•°", f"{s['total_turns']} è½®")
    table.add_row("å®ä½“æ•°é‡", f"{s['total_entities']} ä¸ª")
    table.add_row("æœªè§£å†³ä¼ç¬”", f"{s['unresolved_foreshadowing']} ä¸ª")
    table.add_row("å­˜å‚¨å¤§å°", f"{s['storage_mb']:.1f} MB")
    console.print(table)

def _show_foreshadowing(engine):
    fs_list = engine.foreshadowing_tracker.active_foreshadowing
    if not fs_list:
        console.print("[yellow]æš‚æ— æœªè§£å†³çš„ä¼ç¬”[/yellow]")
        return
    console.print("\nğŸ“Œ [bold]æœªè§£å†³çš„ä¼ç¬”ï¼š[/bold]\n")
    for fs in fs_list:
        console.print(f"  ç¬¬ {fs.created_turn} è½®: {fs.summary}")

def _search(engine, query):
    results = engine.search(query)
    if not results:
        console.print(f"[yellow]ğŸ˜… æ²¡æ‰¾åˆ°å…³äº \"{query}\" çš„è®°å¿†[/yellow]")
        return
    console.print(f"\nğŸ” æ‰¾åˆ° {len(results)} æ¡ç›¸å…³è®°å¿†ï¼š\n")
    for r in results[:10]:
        console.print(f"  ç¬¬ {r.get('turn', '?')} è½®: {r.get('summary', r.get('user', '')[:50])}")

if __name__ == '__main__':
    main()
```

---

## åä¸€ã€æµ‹è¯•ç”¨ä¾‹ï¼ˆç¡®ä¿åŠŸèƒ½æ­£ç¡®ï¼‰

```python
# tests/test_integration.py
"""
é›†æˆæµ‹è¯• - éªŒè¯æ‰€æœ‰éœ€æ±‚
"""

import pytest
from recall.engine import RecallEngine

class TestRecallEngine:
    """å¼•æ“é›†æˆæµ‹è¯•"""
    
    @pytest.fixture
    def engine(self, tmp_path):
        """åˆ›å»ºæµ‹è¯•å¼•æ“"""
        return RecallEngine(
            mode='local',
            data_path=str(tmp_path / 'recall_data'),
        )
    
    def test_ä¸Šä¸‡è½®å­˜å‚¨(self, engine):
        """æµ‹è¯•ï¼šæ”¯æŒä¸Šä¸‡è½®å¯¹è¯"""
        for i in range(100):  # ç®€åŒ–æµ‹è¯•ï¼Œå®é™…åº”æµ‹æ›´å¤š
            result = engine.process_turn(
                f"ç”¨æˆ·æ¶ˆæ¯{i}",
                f"AIå›å¤{i}ï¼Œæåˆ°è§’è‰²{i % 10}"
            )
            assert result.turn_number == i
        
        # éªŒè¯èƒ½æ£€ç´¢åˆ°æ—©æœŸå†…å®¹
        results = engine.search("è§’è‰²5")
        assert len(results) > 0
    
    def test_ä¼ç¬”è¿½è¸ª(self, engine):
        """æµ‹è¯•ï¼šä¼ç¬”ä¸é—å¿˜"""
        # åŸ‹ä¸‹ä¼ç¬”
        engine.process_turn(
            "è€äººè¯´äº†ä»€ä¹ˆï¼Ÿ",
            "ç¥ç§˜è€äººè¯´ï¼š'å½“è¡€æœˆå‡èµ·æ—¶ï¼Œä½ ä¼šæ˜ç™½è¿™æŠŠé’¥åŒ™çš„ç”¨é€”ã€‚'"
        )
        
        # éªŒè¯ä¼ç¬”è¢«è®°å½•
        assert len(engine.foreshadowing_tracker.active_foreshadowing) > 0
        
        # æ¨¡æ‹Ÿå¾ˆå¤šè½®å
        for i in range(50):
            engine.process_turn(f"å¯¹è¯{i}", f"å›å¤{i}")
        
        # è§¦å‘ä¼ç¬”
        engine.process_turn(
            "å¤©ç©ºå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ",
            "æœˆäº®çªç„¶å˜æˆäº†è¡€çº¢è‰²ï¼åŸæ¥è¿™å°±æ˜¯é¢„è¨€ä¸­çš„è¡€æœˆï¼"
        )
        
        # éªŒè¯ä¼ç¬”è¢«è§£å†³
        assert len(engine.foreshadowing_tracker.resolved_foreshadowing) > 0
    
    def test_ä¸€è‡´æ€§æ ¡éªŒ(self, engine):
        """æµ‹è¯•ï¼šè§„èŒƒ100%éµå®ˆ"""
        # è®¾ç½®æ ¸å¿ƒè§„åˆ™
        engine.core_settings.absolute_rules = ["è§’è‰²ä¸ä¼šæ€äºº"]
        engine.consistency_checker._compiled_rules = engine.consistency_checker._compile_core_rules()
        
        # æµ‹è¯•è¿è§„æ£€æµ‹
        result = engine.consistency_checker.check_output(
            "è§’è‰²æ‹¿èµ·åˆ€æ€æ­»äº†æ•Œäºº",
            {}
        )
        
        # åº”è¯¥æ£€æµ‹åˆ°è¿è§„
        assert not result.is_consistent or len(result.warnings) > 0
    
    def test_åŸæ–‡ä¿ç•™(self, engine):
        """æµ‹è¯•ï¼š100%ä¸é—å¿˜"""
        original_text = "è¿™æ˜¯ä¸€æ®µéå¸¸é‡è¦çš„åŸæ–‡ï¼ŒåŒ…å«ç‹¬ç‰¹å­—ç¬¦ï¼šÎ±Î²Î³123"
        
        engine.process_turn(original_text, "æ”¶åˆ°")
        
        # N-gramå…œåº•æœç´¢
        results = engine.ngram_index.search("ç‹¬ç‰¹å­—ç¬¦")
        assert len(results) > 0
        
        # åŸæ–‡æ£€ç´¢
        turn = engine.volume_manager.get_turn(0)
        assert original_text in turn['user']
    
    def test_é›¶é…ç½®(self, engine):
        """æµ‹è¯•ï¼šå³æ’å³ç”¨"""
        # ä¸ä¼ ä»»ä½•å‚æ•°ä¹Ÿèƒ½å·¥ä½œ
        engine2 = RecallEngine()
        
        # è‡ªåŠ¨æ£€æµ‹åœºæ™¯
        engine2.process_turn(
            "def hello(): pass",
            "è¿™æ˜¯ä¸€ä¸ªPythonå‡½æ•°"
        )
        # åº”è¯¥è‡ªåŠ¨æ£€æµ‹ä¸ºcodingåœºæ™¯


# tests/test_foreshadowing.py
"""ä¼ç¬”è¿½è¸ªæµ‹è¯• - MANUAL + LLM è®¾è®¡"""

from recall.processor.foreshadowing import (
    ForeshadowingTracker, 
    ForeshadowingAnalyzer,
    ForeshadowingAnalyzerConfig,
    ForeshadowingStatus
)

class TestForeshadowingTracker:
    """æµ‹è¯•æ‰‹åŠ¨ä¼ç¬”ç®¡ç†"""
    
    @pytest.fixture
    def tracker(self, tmp_path):
        storage_path = str(tmp_path / "foreshadowing.json")
        return ForeshadowingTracker(storage_path=storage_path)
    
    def test_plant_foreshadowing(self, tracker):
        """æµ‹è¯•ï¼šæ‰‹åŠ¨åŸ‹ä¼ç¬”"""
        fsh = tracker.plant(
            content="è€äººäº¤ç»™ä¸»è§’ä¸€æŠŠç¥ç§˜é’¥åŒ™",
            importance=0.9,
            related_entities=["è€äºº", "ä¸»è§’", "ç¥ç§˜é’¥åŒ™"],
            tags=["ç‰©å“", "æ‚¬å¿µ"]
        )
        
        assert fsh.id is not None
        assert fsh.content == "è€äººäº¤ç»™ä¸»è§’ä¸€æŠŠç¥ç§˜é’¥åŒ™"
        assert fsh.importance == 0.9
        assert fsh.status == ForeshadowingStatus.ACTIVE
        assert fsh.detected_by == "manual"
    
    def test_resolve_foreshadowing(self, tracker):
        """æµ‹è¯•ï¼šæ‰‹åŠ¨æ ‡è®°è§£å†³"""
        # å…ˆåŸ‹ä¼ç¬”
        fsh = tracker.plant(content="æµ‹è¯•ä¼ç¬”")
        fsh_id = fsh.id
        
        # æ ‡è®°è§£å†³
        resolved = tracker.resolve(
            fsh_id=fsh_id,
            resolution_note="ä¸»è§’ç”¨é’¥åŒ™æ‰“å¼€äº†é—¨"
        )
        
        assert resolved.status == ForeshadowingStatus.RESOLVED
        assert resolved.resolution_note == "ä¸»è§’ç”¨é’¥åŒ™æ‰“å¼€äº†é—¨"
    
    def test_get_active(self, tracker):
        """æµ‹è¯•ï¼šè·å–æ´»è·ƒä¼ç¬”"""
        tracker.plant(content="ä¼ç¬”1", importance=0.5)
        tracker.plant(content="ä¼ç¬”2", importance=0.9)
        tracker.plant(content="ä¼ç¬”3", importance=0.7)
        
        active = tracker.get_active()
        
        assert len(active) == 3
        # æŒ‰é‡è¦æ€§æ’åº
        assert active[0].importance == 0.9
    
    def test_update_foreshadowing(self, tracker):
        """æµ‹è¯•ï¼šæ›´æ–°ä¼ç¬”"""
        fsh = tracker.plant(content="åŸå†…å®¹", importance=0.5)
        
        updated = tracker.update(
            fsh_id=fsh.id,
            content="æ›´æ–°åçš„å†…å®¹",
            importance=0.8
        )
        
        assert updated.content == "æ›´æ–°åçš„å†…å®¹"
        assert updated.importance == 0.8
    
    def test_delete_foreshadowing(self, tracker):
        """æµ‹è¯•ï¼šåˆ é™¤ä¼ç¬”"""
        fsh = tracker.plant(content="è¦åˆ é™¤çš„ä¼ç¬”")
        fsh_id = fsh.id
        
        result = tracker.delete(fsh_id)
        
        assert result is True
        assert tracker.get(fsh_id) is None
    
    def test_get_context_for_prompt(self, tracker):
        """æµ‹è¯•ï¼šç”Ÿæˆ prompt ä¸Šä¸‹æ–‡"""
        tracker.plant(content="ä¼ç¬”1", importance=0.9, related_entities=["è§’è‰²A"])
        tracker.plant(content="ä¼ç¬”2", importance=0.5)
        
        context = tracker.get_context_for_prompt()
        
        assert "ã€å½“å‰æ´»è·ƒçš„ä¼ç¬”ã€‘" in context
        assert "ä¼ç¬”1" in context
        assert "è§’è‰²A" in context


class TestForeshadowingAnalyzer:
    """æµ‹è¯• LLM è¾…åŠ©åˆ†æï¼ˆæ¨¡æ‹Ÿï¼‰"""
    
    @pytest.fixture
    def analyzer_manual(self, tmp_path):
        tracker = ForeshadowingTracker(storage_path=str(tmp_path / "fsh.json"))
        config = ForeshadowingAnalyzerConfig.manual()
        return ForeshadowingAnalyzer(config=config, tracker=tracker)
    
    def test_manual_mode_no_auto_detect(self, analyzer_manual):
        """æµ‹è¯•ï¼šæ‰‹åŠ¨æ¨¡å¼ä¸è‡ªåŠ¨æ£€æµ‹"""
        result = analyzer_manual.on_new_turn(
            content="è€äººè¯´ï¼š'ç»ˆæœ‰ä¸€å¤©ä½ ä¼šæ˜ç™½çœŸç›¸ã€‚'",
            role="assistant"
        )
        
        # æ‰‹åŠ¨æ¨¡å¼è¿”å› None
        assert result is None
    
    def test_manual_operations_always_work(self, analyzer_manual):
        """æµ‹è¯•ï¼šæ‰‹åŠ¨æ“ä½œå§‹ç»ˆå¯ç”¨"""
        # å³ä½¿æ˜¯æ‰‹åŠ¨æ¨¡å¼ï¼Œtracker çš„æ‰‹åŠ¨æ“ä½œä¹Ÿèƒ½ç”¨
        fsh = analyzer_manual.tracker.plant(content="æ‰‹åŠ¨æ·»åŠ çš„ä¼ç¬”")
        
        assert fsh is not None
        assert len(analyzer_manual.tracker.get_active()) == 1


# tests/test_storage.py
"""å­˜å‚¨æµ‹è¯•"""

from recall.storage.volume_manager import VolumeManager, VolumeData

class TestVolumeManager:
    
    def test_append_and_get(self, tmp_path):
        """æµ‹è¯•ï¼šè¿½åŠ å’Œè·å–"""
        vm = VolumeManager(str(tmp_path))
        
        turn_data = {'user': 'hello', 'assistant': 'hi'}
        turn_num = vm.append_turn(turn_data)
        
        retrieved = vm.get_turn(turn_num)
        assert retrieved['user'] == 'hello'
    
    def test_volume_split(self, tmp_path):
        """æµ‹è¯•ï¼šè‡ªåŠ¨åˆ†å·"""
        vm = VolumeManager(str(tmp_path))
        vm.config['turns_per_volume'] = 10  # è®¾å°ä¸€ç‚¹ä¾¿äºæµ‹è¯•
        
        for i in range(25):
            vm.append_turn({'user': f'msg{i}', 'assistant': f'reply{i}'})
        
        # åº”è¯¥æœ‰3ä¸ªå·
        assert len(vm.loaded_volumes) >= 2
```

---

## åäºŒã€èµ„æºä¼°ç®—

### 10.1 å­˜å‚¨ç©ºé—´ä¼°ç®—

| è§„æ¨¡ | åŸæ–‡å­˜å‚¨ | ç´¢å¼•å­˜å‚¨ | æ€»è®¡ |
|------|---------|---------|------|
| 1ä¸‡è½®ï¼ˆæ™®é€šç”¨æˆ·ï¼‰ | ~5MB | ~10MB | ~15MB |
| 10ä¸‡è½®ï¼ˆé‡åº¦ç”¨æˆ·ï¼‰ | ~50MB | ~80MB | ~130MB |
| 100ä¸‡è½®ï¼ˆæé™ï¼‰ | ~500MB | ~600MB | ~1.1GB |
| 2äº¿å­—ï¼ˆç†è®ºä¸Šé™ï¼‰ | ~400MB | ~800MB | ~1.2GB |

### 10.2 å†…å­˜å ç”¨ä¼°ç®—

| ç»„ä»¶ | çƒ­æ•°æ®å†…å­˜ | è¯´æ˜ |
|------|-----------|------|
| **sentence-transformersæ¨¡å‹** | **~400MB** | Embeddingæ¨¡å‹ï¼ˆå¿…éœ€ï¼‰ |
| **spaCyæ¨¡å‹** | **~50MB** | ä¸­æ–‡NERæ¨¡å‹ |
| é¢„åŠ è½½å·ï¼ˆ2å·ï¼‰ | ~20MB | æœ€è¿‘2ä¸‡è½®å®Œæ•´åŠ è½½ |
| å®ä½“ç´¢å¼• | ~5MB | 1000å®ä½“ |
| å‘é‡ç´¢å¼• | ~50MB | FAISS mmapæ¨¡å¼ |
| N-gramç´¢å¼• | ~30MB | åè¯çŸ­è¯­ç´¢å¼• |
| å·¥ä½œå†…å­˜L2 | ~10MB | 200å®ä½“å®¹é‡ |
| **æ€»è®¡ï¼ˆæ™®é€šç”¨æˆ·ï¼‰** | **~565MB** | é¦–æ¬¡åŠ è½½å |

> âš ï¸ **è¯šå®è¯´æ˜**ï¼šä¹‹å‰ç‰ˆæœ¬å£°ç§°115MBæ˜¯ä¸å‡†ç¡®çš„ã€‚NLPæ¨¡å‹å ç”¨å¤§å¤´ï¼Œè¿™æ˜¯ä¸å¯é¿å…çš„å¼€é”€ã€‚å¦‚æœå†…å­˜æœ‰é™ï¼ˆ<1GBå¯ç”¨ï¼‰ï¼Œå»ºè®®ä½¿ç”¨è½»é‡æ¨¡å¼ï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚

#### è½»é‡æ¨¡å¼ï¼ˆä½é…ç”µè„‘ä¸“ç”¨ï¼‰

å¯¹äºå†…å­˜å—é™ç¯å¢ƒï¼ˆ<1GBå¯ç”¨å†…å­˜ï¼‰ï¼Œä½¿ç”¨è½»é‡é…ç½®ï¼š

```bash
# å®‰è£…æ—¶é€‰æ‹©è½»é‡æ¨¡å¼
recall init --lightweight
```

**è½»é‡æ¨¡å¼ vs æ ‡å‡†æ¨¡å¼å¯¹æ¯”**ï¼š

| åŠŸèƒ½ | æ ‡å‡†æ¨¡å¼ | è½»é‡æ¨¡å¼ |
|------|---------|---------|
| å†…å­˜å ç”¨ | ~565MB | **~80MB** |
| å‘é‡è¯­ä¹‰æ£€ç´¢ | âœ… | âŒ |
| å…³é”®è¯ç²¾ç¡®åŒ¹é… | âœ… | âœ… |
| ä¼ç¬”è¿½è¸ª | âœ… | âœ… |
| è§„èŒƒéµå®ˆæ£€æŸ¥ | âœ… | âœ… |
| å®ä½“è¯†åˆ« | spaCyå®Œæ•´æ¨¡å‹ | jieba + è§„åˆ™ |
| 100%ä¸é—å¿˜ | âœ… | âœ… |

```python
# recall/config.py
"""è½»é‡æ¨¡å¼é…ç½®"""

class LightweightConfig:
    """è½»é‡æ¨¡å¼ - é€‚åˆä½é…ç”µè„‘"""
    
    # ç¦ç”¨é‡å‹ç»„ä»¶
    ENABLE_VECTOR_INDEX = False      # ä¸åŠ è½½ sentence-transformers (~400MB)
    ENABLE_SPACY_FULL = False        # ä¸åŠ è½½å®Œæ•´ spaCy æ¨¡å‹ (~50MB)
    
    # ä½¿ç”¨è½»é‡æ›¿ä»£
    ENTITY_EXTRACTOR = 'jieba_rules'  # ç”¨ jieba + è§„åˆ™æ›¿ä»£ spaCy
    RETRIEVAL_LAYERS = [1, 2, 3, 5, 7, 8]  # è·³è¿‡ç¬¬4å±‚(å‘é‡)å’Œç¬¬6å±‚(è¯­ä¹‰)
    
    # å†…å­˜é™åˆ¶
    MAX_CACHED_TURNS = 1000          # å‡å°‘ç¼“å­˜
    MAX_INDEX_SIZE_MB = 30           # é™åˆ¶ç´¢å¼•å¤§å°
    
    @classmethod
    def apply(cls, engine):
        """åº”ç”¨è½»é‡é…ç½®"""
        engine.config.update({
            'vector_enabled': False,
            'spacy_model': 'blank',
            'retrieval_layers': cls.RETRIEVAL_LAYERS,
            'max_cache': cls.MAX_CACHED_TURNS,
        })
        print("[Recall] è½»é‡æ¨¡å¼å·²å¯ç”¨ï¼Œå†…å­˜å ç”¨çº¦ ~80MB")


from typing import List
from dataclasses import dataclass

@dataclass
class LightweightExtractedEntity:
    """è½»é‡ç‰ˆæå–å®ä½“"""
    name: str
    entity_type: str
    confidence: float = 0.5
    source_text: str = ""

class LightweightEntityExtractor:
    """è½»é‡çº§å®ä½“æå–å™¨ - ä¸ä¾èµ– spaCy"""
    
    def __init__(self):
        import re
        import jieba
        self.re = re
        self.jieba = jieba
        
        # ç®€å•çš„å‘½åå®ä½“æ¨¡å¼
        self.patterns = {
            'PERSON': r'[ã€Œã€"]([\u4e00-\u9fa5]{2,4})[ã€ã€"]è¯´|(\w{2,10})å…ˆç”Ÿ|(\w{2,10})å¥³å£«',
            'LOCATION': r'åœ¨([\u4e00-\u9fa5]{2,10})|å»([\u4e00-\u9fa5]{2,10})',
            'ITEM': r'[ã€Œã€"]([\u4e00-\u9fa5a-zA-Z]{2,20})[ã€ã€"]',
        }
    
    def extract(self, text: str) -> List[LightweightExtractedEntity]:
        """æå–å®ä½“ï¼ˆè½»é‡ç‰ˆï¼‰ï¼Œè¿”å›ä¸ EntityExtractor å…¼å®¹çš„å¯¹è±¡"""
        entities = []
        seen_names = set()
        
        # 1. æ­£åˆ™æ¨¡å¼åŒ¹é…
        for entity_type, pattern in self.patterns.items():
            for match in self.re.finditer(pattern, text):
                name = next((g for g in match.groups() if g), None)
                if name and len(name) >= 2 and name not in seen_names:
                    entities.append(LightweightExtractedEntity(
                        name=name,
                        entity_type=entity_type,
                        confidence=0.6,
                        source_text=text[max(0, match.start()-20):match.end()+20]
                    ))
                    seen_names.add(name)
        
        # 2. jieba åˆ†è¯ + è¯æ€§æ ‡æ³¨
        import jieba.posseg as pseg
        words = pseg.cut(text[:5000])  # é™åˆ¶é•¿åº¦
        for word, flag in words:
            if flag in ('nr', 'ns', 'nt', 'nz') and len(word) >= 2 and word not in seen_names:
                entity_type = {'nr': 'PERSON', 'ns': 'LOCATION', 'nt': 'ORG', 'nz': 'ITEM'}.get(flag, 'MISC')
                entities.append(LightweightExtractedEntity(
                    name=word,
                    entity_type=entity_type,
                    confidence=0.5,
                    source_text=""
                ))
                seen_names.add(word)
        
        return entities
    
    def extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆè½»é‡ç‰ˆï¼‰"""
        # ä¸­æ–‡è¯ç»„
        chinese = self.re.findall(r'[\u4e00-\u9fa5]{2,6}', text)
        # è‹±æ–‡å•è¯
        english = self.re.findall(r'[a-zA-Z]{3,}', text.lower())
        # è¿‡æ»¤åœç”¨è¯
        stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'æœ‰', 'è¿™', 'é‚£', 'å°±', 'éƒ½', 
                     'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be'}
        return [w for w in chinese + english if w not in stopwords]
```

> ğŸ’¡ **è½»é‡æ¨¡å¼æ ¸å¿ƒä¿è¯**ï¼šå³ä½¿ç¦ç”¨å‘é‡æ£€ç´¢ï¼Œå…³é”®è¯åŒ¹é… + N-gram å…œåº•ä»ç„¶ç¡®ä¿ **100% ä¸é—å¿˜**ã€‚
> è¯­ä¹‰æ£€ç´¢åªæ˜¯è®©å¬å›æ›´æ™ºèƒ½ï¼Œä¸æ˜¯å¿…éœ€çš„ã€‚

### 10.3 N-gramç´¢å¼•ä¼˜åŒ–

```python
# recall/index/ngram_index.py
"""ä¼˜åŒ–çš„N-gramç´¢å¼•"""

import re
from typing import List, Set
from pybloom_live import BloomFilter

class OptimizedNgramIndex:
    """ä¼˜åŒ–çš„N-gramç´¢å¼• - é¿å…ç©ºé—´çˆ†ç‚¸"""
    
    def __init__(self):
        # åªç´¢å¼•åè¯çŸ­è¯­ï¼Œä¸åšå…¨æ–‡n-gram
        self.noun_phrases = {}   # åè¯çŸ­è¯­ â†’ [turn_ids]
        self.bloom_filter = BloomFilter(capacity=1000000, error_rate=0.01)
    
    def add(self, turn: int, content: str):
        """æ·»åŠ ç´¢å¼•ï¼ˆåªæå–åè¯çŸ­è¯­ï¼‰"""
        # æå–åè¯çŸ­è¯­è€Œéæ‰€æœ‰n-gram
        phrases = self._extract_noun_phrases(content)
        
        for phrase in phrases:
            # å…ˆç”¨å¸ƒéš†è¿‡æ»¤å™¨å¿«é€Ÿåˆ¤æ–­
            self.bloom_filter.add(phrase)
            
            if phrase not in self.noun_phrases:
                self.noun_phrases[phrase] = []
            self.noun_phrases[phrase].append(turn)
    
    def _extract_noun_phrases(self, content: str) -> List[str]:
        """æå–åè¯çŸ­è¯­ï¼ˆè€Œéæ‰€æœ‰n-gramï¼‰"""
        # ä½¿ç”¨ç®€å•è§„åˆ™ï¼š2-4å­—çš„ä¸­æ–‡è¯ç»„ + è‹±æ–‡å•è¯
        chinese_phrases = re.findall(r'[\u4e00-\u9fa5]{2,4}', content)
        english_words = re.findall(r'[a-zA-Z]{3,}', content)
        
        # è¿‡æ»¤åœç”¨è¯
        stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'æœ‰', 'è¿™', 'é‚£', 'the', 'a', 'is'}
        phrases = [p for p in chinese_phrases + english_words if p not in stopwords]
        
        return phrases
    
    def search(self, query: str) -> List[int]:
        """æœç´¢"""
        phrases = self._extract_noun_phrases(query)
        
        candidate_turns = set()
        for phrase in phrases:
            # å…ˆç”¨å¸ƒéš†è¿‡æ»¤å™¨å¿«é€Ÿæ’é™¤
            if not self.bloom_filter.might_contain(phrase):
                continue
            
            if phrase in self.noun_phrases:
                candidate_turns.update(self.noun_phrases[phrase])
        
        return sorted(candidate_turns)
```

---

## åç‚¹äº”ã€ç³»ç»Ÿå½±å“å£°æ˜ï¼ˆå³æ’å³ç”¨ä¿è¯ï¼‰

### å®‰è£… Recall åçš„ç³»ç»Ÿå˜åŒ–

| æ–¹é¢ | å½±å“èŒƒå›´ | è¯¦æƒ… |
|------|---------|------|
| **æ–‡ä»¶ç³»ç»Ÿ** | ä»…é¡¹ç›®ç›®å½• `./recall_data/` | æ‰€æœ‰æ•°æ®ã€æ¨¡å‹ã€é…ç½®éƒ½åœ¨æ­¤ç›®å½• |
| **Python ç¯å¢ƒ** | pip åŒ…å®‰è£… | å¯ç”¨ `pip uninstall` å®Œå…¨ç§»é™¤ |
| **ç¯å¢ƒå˜é‡** | æ— æ°¸ä¹…ä¿®æ”¹ | åªåœ¨è¿è¡Œæ—¶ä¸´æ—¶è®¾ç½® |
| **æ³¨å†Œè¡¨** | âŒ ä¸ä¿®æ”¹ | Windows æ³¨å†Œè¡¨ä¸å—å½±å“ |
| **ç³»ç»ŸæœåŠ¡** | âŒ ä¸å®‰è£… | æ— åå°æœåŠ¡ã€æ— å¼€æœºå¯åŠ¨ |
| **å…¶ä»–åº”ç”¨** | âŒ ä¸ä¿®æ”¹ | ä¸ä¼šä¿®æ”¹ SillyTavern ç­‰åº”ç”¨çš„åŸæœ‰é…ç½® |
| **ç½‘ç»œè¿æ¥** | ä»…ä¸‹è½½æ¨¡å‹æ—¶ | è¿è¡Œæ—¶å®Œå…¨æœ¬åœ°ï¼Œé™¤éè°ƒç”¨ LLM API |

### âœ… å®Œæ•´å¸è½½æ£€æŸ¥æ¸…å•

```bash
# å¸è½½åï¼Œä»¥ä¸‹ä½ç½®åº”è¯¥å®Œå…¨å¹²å‡€ï¼š

# 1. Python åŒ…ï¼ˆåº”è¯¥ä¸å­˜åœ¨ï¼‰
pip show recall-ai  # åº”è¯¥æ˜¾ç¤º "Package(s) not found"

# 2. æ•°æ®ç›®å½•ï¼ˆåº”è¯¥ä¸å­˜åœ¨ï¼‰
# æ£€æŸ¥ä½ çš„é¡¹ç›®ç›®å½•ï¼Œrecall_data/ æ–‡ä»¶å¤¹åº”è¯¥å·²è¢«åˆ é™¤
ls ./recall_data  # åº”è¯¥æ˜¾ç¤º "No such file or directory"

# 3. éªŒè¯æ— æ®‹ç•™è¿›ç¨‹
# Windows:
tasklist | findstr recall  # åº”è¯¥æ— è¾“å‡º
# Linux/Mac:
ps aux | grep recall  # åº”è¯¥åªæœ‰ grep è‡ªå·±

# âœ… å¦‚æœä»¥ä¸Šéƒ½é€šè¿‡ï¼Œç³»ç»Ÿå·²å®Œå…¨æ¢å¤åŸçŠ¶
```

### ç¯å¢ƒéš”ç¦»æŠ€æœ¯å®ç°

```python
# recall/utils/environment.py
"""ç¯å¢ƒéš”ç¦»å·¥å…· - ç¡®ä¿ä¸æ±¡æŸ“å…¨å±€ç¯å¢ƒ"""

import os
import sys
import atexit

class EnvironmentIsolation:
    """ç¯å¢ƒéš”ç¦»ç®¡ç†å™¨ - ç¡®ä¿æ‰€æœ‰ç¼“å­˜éƒ½åœ¨é¡¹ç›®ç›®å½•å†…"""
    
    _original_env = {}
    _initialized = False
    
    @classmethod
    def setup(cls, base_path: str = None):
        """è®¾ç½®éš”ç¦»ç¯å¢ƒï¼ˆæ‰€æœ‰ç¼“å­˜é‡å®šå‘åˆ°é¡¹ç›®ç›®å½•ï¼‰"""
        if cls._initialized:
            return
        
        # è·å–æ•°æ®æ ¹ç›®å½•ï¼ˆåœ¨é¡¹ç›®ç›®å½•å†…ï¼‰
        from .init import RecallInit
        root = RecallInit.get_data_root(base_path)
        models_dir = os.path.join(root, 'models')
        
        # ä¿å­˜åŸå§‹ç¯å¢ƒå˜é‡ï¼ˆå¸è½½æ—¶æ¢å¤ï¼‰
        env_vars_to_set = {
            'SENTENCE_TRANSFORMERS_HOME': os.path.join(models_dir, 'sentence-transformers'),
            'HF_HOME': os.path.join(models_dir, 'huggingface'),
            'HUGGINGFACE_HUB_CACHE': os.path.join(models_dir, 'huggingface', 'hub'),
            'TRANSFORMERS_CACHE': os.path.join(models_dir, 'huggingface', 'transformers'),
            'TORCH_HOME': os.path.join(models_dir, 'torch'),
            'XDG_CACHE_HOME': os.path.join(root, 'cache'),
            'HF_HUB_DISABLE_TELEMETRY': '1',
            'DO_NOT_TRACK': '1',
            'ANONYMIZED_TELEMETRY': 'false',
        }
        
        for key, value in env_vars_to_set.items():
            cls._original_env[key] = os.environ.get(key)
            os.environ[key] = value
        
        # æ³¨å†Œé€€å‡ºæ—¶æ¸…ç†
        atexit.register(cls.cleanup)
        cls._initialized = True
    
    @classmethod
    def cleanup(cls):
        """æ¢å¤åŸå§‹ç¯å¢ƒ"""
        for key, original_value in cls._original_env.items():
            if original_value is None:
                os.environ.pop(key, None)
            else:
                os.environ[key] = original_value
    
    @classmethod
    def get_data_path(cls, base_path: str = None) -> str:
        """è·å–æ•°æ®å­˜å‚¨è·¯å¾„ï¼ˆåœ¨é¡¹ç›®ç›®å½•å†…ï¼‰"""
        from .init import RecallInit
        return os.path.join(RecallInit.get_data_root(base_path), 'data')
    
    @classmethod
    def get_model_path(cls, base_path: str = None) -> str:
        """è·å–æ¨¡å‹å­˜å‚¨è·¯å¾„ï¼ˆåœ¨é¡¹ç›®ç›®å½•å†…ï¼‰"""
        from .init import RecallInit
        return os.path.join(RecallInit.get_data_root(base_path), 'models')


# æ¨¡å—å¯¼å…¥æ—¶è‡ªåŠ¨è®¾ç½®
EnvironmentIsolation.setup()
```
```

---

## åäºŒç‚¹äº”ã€æœ€ç»ˆè‡ªæŸ¥ï¼ˆå®Œæ•´ç‰ˆï¼‰

| éœ€æ±‚ | å®ç°æ–¹æ¡ˆ | çŠ¶æ€ | ä½ç½® |
|------|---------|------|------|
| **ä¸Šä¸‡è½® RP** | åˆ†å·å­˜å‚¨ + O(1)å®šä½ + é¢„åŠ è½½ + å¹¶å‘é” | âœ… | ç¬¬äºŒèŠ‚ VolumeManager |
| **ä¼ç¬”ä¸é—å¿˜** | æ‰‹åŠ¨ç®¡ç† + LLM è‡ªåŠ¨æ£€æµ‹ + ä¸»åŠ¨æé†’ | âœ… | ç¬¬å››èŠ‚ ForeshadowingTracker + ForeshadowingAnalyzer |
| **å‡ ç™¾ä¸‡å­—è§„æ¨¡** | åˆ†å·æ¶æ„ + æ‡’åŠ è½½ + å¢é‡ç´¢å¼• | âœ… | ç¬¬äºŒèŠ‚åˆ†å·è®¾è®¡ |
| **ä¸Šåƒæ–‡ä»¶ä»£ç ** | å¤šè¯­è¨€è§£æå™¨ + ç¬¦å·è¡¨ + ä¾èµ–å›¾ | âŒ | ç¬¬äº”èŠ‚ CodeIndexerï¼ˆå¯é€‰ï¼Œv3.1ï¼‰ |
| **è§„èŒƒ100%éµå®ˆ** | L0æ³¨å…¥ + è§„åˆ™ç¼–è¯‘ + å±æ€§æ£€æŸ¥ | ğŸ”§ | L0æ³¨å…¥âœ… ä¸€è‡´æ€§æ£€æŸ¥âœ… è§„åˆ™ç¼–è¯‘å™¨å¾…v3.1 |
| **é›¶é…ç½®å³æ’å³ç”¨** | pip install + API key å³å¯ä½¿ç”¨ | âœ… | ç¬¬ä¸ƒèŠ‚åˆå§‹åŒ– |
| **100%ä¸é—å¿˜** | ArchiveåŸæ–‡ä¿å­˜ + 8å±‚æ£€ç´¢ + N-gramå…œåº• | âœ… | ç¬¬ä¸‰èŠ‚8å±‚æ£€ç´¢ |
| **é¢å‘å¤§ä¼—å‹å¥½** | STæ’ä»¶å¸‚åœºå®‰è£… + 3æ­¥å®Œæˆ + å…¨ä¸­æ–‡ | âœ… | ç¬¬åä¸‰èŠ‚STæ’ä»¶ |
| **é…ç½®keyå°±èƒ½ç”¨** | åªéœ€è®¾ç½®ä¸€ä¸ª API key ç¯å¢ƒå˜é‡ | âœ… | ç¬¬ã€‡èŠ‚å®‰è£… |
| **pip installå³æ’å³ç”¨** | å‘½ä»¤è¡Œä¸¤æ­¥å®Œæˆï¼ˆè‡ªåŠ¨ä¸‹è½½NLPæ¨¡å‹ï¼‰ | âœ… | ç¬¬åèŠ‚CLI |
| **æ™®é€šäººæ— é—¨æ§›** | çº¯æœ¬åœ°æ’ä»¶ + ç”¨æˆ·è‡ªå·±çš„API key | âœ… | ç¬¬ä¸ƒèŠ‚åˆå§‹åŒ– |
| **ğŸ†• 3-5ç§’å“åº”** | å¹¶è¡Œæ£€ç´¢ + å¼‚æ­¥å†™å…¥ + ç¼“å­˜çƒ­è·¯å¾„ | âœ… | ç¬¬ä¸ƒç‚¹äº”èŠ‚æ€§èƒ½ä¼˜åŒ– |
| **ğŸ†• çŸ¥è¯†å›¾è°±** | è½»é‡çº§æœ¬åœ°å›¾ç»“æ„ + å…³ç³»è‡ªåŠ¨æå– | âœ… | ç¬¬äºŒç‚¹ä¸‰èŠ‚ KnowledgeGraph |
| **ğŸ†• å¤šç”¨æˆ·/å¤šè§’è‰²** | MemoryScope ä½œç”¨åŸŸéš”ç¦» | âœ… | ç¬¬äºŒç‚¹å››èŠ‚ MultiTenantStorage |
| **ğŸ†• ä½é…ç”µè„‘æ”¯æŒ** | è½»é‡æ¨¡å¼ï¼ˆ~80MBå†…å­˜ï¼‰+ æ— GPUè¦æ±‚ | âœ… | ç¬¬åäºŒèŠ‚è½»é‡æ¨¡å¼ |

### ğŸ†• å³æ’å³ç”¨/ç¯å¢ƒéš”ç¦»æ£€æŸ¥é¡¹

| éœ€æ±‚ | å®ç°æ–¹æ¡ˆ | çŠ¶æ€ | ä½ç½® |
|------|---------|------|------|
| **å•ä¸€æ•°æ®ç›®å½•** | æ‰€æœ‰æ•°æ®å­˜å‚¨åœ¨é¡¹ç›®ç›®å½• `./recall_data/` | âœ… | ç¬¬ã€‡ç‚¹äº”èŠ‚ç›®å½•ç»“æ„ |
| **æ¨¡å‹éš”ç¦»å­˜å‚¨** | NLPæ¨¡å‹ä¸‹è½½åˆ° `./recall_data/models/` | âœ… | EntityExtractor, VectorIndex |
| **æ— ç³»ç»Ÿçº§ä¿®æ”¹** | ä¸ä¿®æ”¹æ³¨å†Œè¡¨/ç³»ç»ŸæœåŠ¡/PATH | âœ… | ç¬¬åç‚¹äº”èŠ‚ç³»ç»Ÿå½±å“å£°æ˜ |
| **ç¯å¢ƒå˜é‡éš”ç¦»** | è¿è¡Œæ—¶ä¸´æ—¶è®¾ç½®ï¼Œé€€å‡ºæ—¶æ¢å¤ | âœ… | EnvironmentIsolation ç±» |
| **å®Œæ•´å¸è½½æ”¯æŒ** | pip uninstall + åˆ é™¤ç›®å½• = å®Œå…¨å¹²å‡€ | âœ… | ç¬¬ã€‡èŠ‚å¸è½½è¯´æ˜ |
| **è™šæ‹Ÿç¯å¢ƒå…¼å®¹** | æ”¯æŒåœ¨ venv ä¸­å®‰è£… | âœ… | ç¬¬ã€‡èŠ‚å®‰è£…æ–¹å¼äºŒ |
| **ä¸ä¿®æ”¹å…¶ä»–åº”ç”¨** | ST æ’ä»¶ç‹¬ç«‹ï¼Œä¸ä¿®æ”¹ ST åŸé…ç½® | âœ… | ç¬¬åä¸‰èŠ‚STæ’ä»¶ |
| **ç¦»çº¿è¿è¡Œæ”¯æŒ** | æ¨¡å‹ä¸‹è½½åå¯ç¦»çº¿è¿è¡Œï¼ˆé™¤LLMè°ƒç”¨ï¼‰ | âœ… | æ¶æ„è®¾è®¡ |
| **è·¨å¹³å°æ”¯æŒ** | Windows/Mac/Linux ç»Ÿä¸€è¡Œä¸º | âœ… | ä½¿ç”¨ os.path.abspath å¤„ç†ç›¸å¯¹è·¯å¾„ |
| **é…ç½®æ–‡ä»¶éš”ç¦»** | é…ç½®å­˜å‚¨åœ¨ `./recall_data/config.json` | âœ… | RecallInit ç±» |

### ğŸ†• è®¡åˆ’å¤–æ–°å¢åŠŸèƒ½ï¼ˆ3é¡¹ï¼‰â­

> ğŸ“Œ ä»¥ä¸‹åŠŸèƒ½**è¶…å‡ºåŸè®¡åˆ’éœ€æ±‚**ï¼Œæ˜¯å¼€å‘è¿‡ç¨‹ä¸­æ–°å¢çš„å¢å¼ºåŠŸèƒ½ã€‚

| éœ€æ±‚ | å®ç°æ–¹æ¡ˆ | çŠ¶æ€ | ä½ç½® |
|------|---------|------|------|
| **â­ æŒä¹…æ¡ä»¶ç³»ç»Ÿ** | ContextTracker + 15ç§æ¡ä»¶ç±»å‹ + è‡ªåŠ¨æå–/å‹ç¼© | âœ… | `recall/processor/context_tracker.py` |
| **â­ é…ç½®çƒ­æ›´æ–°** | reload API + è¿æ¥æµ‹è¯• + è‡ªåŠ¨ç»´åº¦æ£€æµ‹ | âœ… | `plugins/sillytavern-extension/server.py` |
| **â­ ä¼ç¬”åˆ†æå™¨å¢å¼º** | LLMè‡ªåŠ¨æ£€æµ‹ + get_context_for_promptä¸»åŠ¨æé†’ | âœ… | `recall/processor/foreshadowing_analyzer.py` |

---

#### â­ 1. æŒä¹…æ¡ä»¶ç³»ç»Ÿ (ContextTracker) - è¯¦ç»†è¯´æ˜

**å®ç°ä½ç½®**ï¼š`recall/processor/context_tracker.py`

| åŠŸèƒ½ | è¯´æ˜ |
|------|------|
| **15ç§æ¡ä»¶ç±»å‹** | ç”¨æˆ·èº«ä»½ã€ç”¨æˆ·ç›®æ ‡ã€ç”¨æˆ·åå¥½ã€æŠ€æœ¯ç¯å¢ƒã€é¡¹ç›®ä¿¡æ¯ã€æ—¶é—´çº¦æŸã€è§’è‰²ç‰¹å¾ã€ä¸–ç•Œè§‚è®¾å®šã€å…³ç³»è®¾å®šã€æƒ…ç»ªçŠ¶æ€ã€æŠ€èƒ½èƒ½åŠ›ã€ç‰©å“é“å…·ã€å‡è®¾å‰æã€çº¦æŸæ¡ä»¶ã€è‡ªå®šä¹‰ |
| **è‡ªåŠ¨æå–** | ä»å¯¹è¯ä¸­è‡ªåŠ¨è¯†åˆ«åº”è¯¥æŒä¹…åŒ–çš„æ¡ä»¶ï¼ˆLLMè¾…åŠ©ï¼‰ |
| **æ™ºèƒ½å‹ç¼©** | å½“æ¡ä»¶è¿‡å¤šæ—¶è‡ªåŠ¨åˆå¹¶ç›¸ä¼¼æ¡ä»¶ï¼Œé¿å…ä¸Šä¸‹æ–‡è†¨èƒ€ |
| **ç½®ä¿¡åº¦è¡°å‡** | é•¿æœŸæœªä½¿ç”¨çš„æ¡ä»¶ç½®ä¿¡åº¦è‡ªåŠ¨ä¸‹é™ |
| **å¢é•¿æ§åˆ¶** | æ¯ç§ç±»å‹æœ€å¤š5æ¡ï¼Œæ€»å…±æœ€å¤š30æ¡ï¼Œé˜²æ­¢æ— é™å¢é•¿ |

**API ç«¯ç‚¹**ï¼š
- `POST /v1/persistent-contexts` - æ·»åŠ æŒä¹…æ¡ä»¶
- `GET /v1/persistent-contexts` - è·å–æ´»è·ƒæ¡ä»¶
- `DELETE /v1/persistent-contexts/{context_id}` - åˆ é™¤æ¡ä»¶
- `POST /v1/persistent-contexts/extract` - ä»æ–‡æœ¬è‡ªåŠ¨æå–
- `POST /v1/persistent-contexts/consolidate` - å‹ç¼©å†—ä½™æ¡ä»¶
- `GET /v1/persistent-contexts/stats` - è·å–ç»Ÿè®¡ä¿¡æ¯
- `POST /v1/persistent-contexts/{context_id}/used` - æ ‡è®°å·²ä½¿ç”¨

#### â­ 2. é…ç½®çƒ­æ›´æ–°ç³»ç»Ÿ

**å®ç°ä½ç½®**ï¼š`plugins/sillytavern-extension/server.py`

| åŠŸèƒ½ | è¯´æ˜ |
|------|------|
| **æ— éœ€é‡å¯æ›´æ–°é…ç½®** | ä¿®æ”¹ `api_keys.env` åè°ƒç”¨ reload å³å¯ç”Ÿæ•ˆ |
| **API è¿æ¥æµ‹è¯•** | ä¸€é”®æµ‹è¯• Embedding/LLM API æ˜¯å¦å¯ç”¨ |
| **è‡ªåŠ¨ç»´åº¦æ£€æµ‹** | è‡ªåŠ¨æ£€æµ‹ Embedding æ¨¡å‹çš„å‘é‡ç»´åº¦ |
| **æ¨¡å‹åˆ—è¡¨è·å–** | è·å– API å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ |

**API ç«¯ç‚¹**ï¼š
- `POST /v1/config/reload` - é‡æ–°åŠ è½½é…ç½®
- `GET /v1/config/test` - æµ‹è¯• Embedding è¿æ¥
- `GET /v1/config/test/llm` - æµ‹è¯• LLM è¿æ¥
- `GET /v1/config/models` - è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨

#### â­ 3. ä¼ç¬”åˆ†æå™¨å¢å¼º

**å®ç°ä½ç½®**ï¼š`recall/processor/foreshadowing_analyzer.py`

| åŠŸèƒ½ | è¯´æ˜ |
|------|------|
| **LLM è‡ªåŠ¨æ£€æµ‹** | ä½¿ç”¨ LLM è‡ªåŠ¨è¯†åˆ«å¯¹è¯ä¸­çš„ä¼ç¬” |
| **æ‰‹åŠ¨/è‡ªåŠ¨æ¨¡å¼åˆ‡æ¢** | æ”¯æŒåœ¨è¿è¡Œæ—¶åˆ‡æ¢åˆ†ææ¨¡å¼ |
| **å¯é…ç½®è§¦å‘é—´éš”** | æ¯ N è½®å¯¹è¯è§¦å‘ä¸€æ¬¡è‡ªåŠ¨åˆ†æ |
| **ä¸»åŠ¨æé†’** | åœ¨ build_context ä¸­æ³¨å…¥æ´»è·ƒä¼ç¬”ï¼Œæé†’ AI æ¨è¿› |
| **get_context_for_prompt** | ç”Ÿæˆç”¨äºæ³¨å…¥ prompt çš„ä¼ç¬”ä¸Šä¸‹æ–‡ |

**API ç«¯ç‚¹**ï¼š
- `GET /v1/foreshadowing/analyzer/config` - è·å–åˆ†æå™¨é…ç½®
- `PUT /v1/foreshadowing/analyzer/config` - æ›´æ–°åˆ†æå™¨é…ç½®
- `POST /v1/foreshadowing/analyze/turn` - åˆ†æå•è½®å¯¹è¯
- `POST /v1/foreshadowing/analyze/trigger` - æ‰‹åŠ¨è§¦å‘åˆ†æ

---

## åäºŒç‚¹å…­ã€ä¸ç«å“å¯¹æ¯”ï¼ˆRecall çš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼‰

### å¯¹æ¯” mem0 (45.6k stars)

| åŠŸèƒ½ | mem0 | Recall | ä¼˜åŠ¿æ–¹ |
|------|------|--------|--------|
| è®°å¿†è‡ªåŠ¨æ€»ç»“ | âœ… LLMæ€»ç»“ | âœ… LLMæ€»ç»“ | å¹³æ‰‹ |
| ç”¨æˆ·/ä¼šè¯çº§è®°å¿† | âœ… user_id | âœ… user_id + character_id | **Recall** (RPåœºæ™¯) |
| å‘é‡æ£€ç´¢ | âœ… æœ‰ | âœ… æœ‰ | å¹³æ‰‹ |
| **100%ä¸é—å¿˜** | âŒ ä¼šå‹ç¼©ä¸¢å¤± | âœ… L3åŸæ–‡å­˜æ¡£+8å±‚æ£€ç´¢ | **Recall** |
| **ä¼ç¬”è¿½è¸ª** | âŒ æ—  | âœ… æ‰‹åŠ¨ç®¡ç†+LLMè¾…åŠ©æ£€æµ‹ | **Recall** |
| **è§„èŒƒéµå®ˆæ£€æŸ¥** | âŒ æ—  | âœ… L0æ³¨å…¥+ä¸€è‡´æ€§æ ¡éªŒ | **Recall** |
| **RP/å°è¯´åœºæ™¯ä¼˜åŒ–** | âŒ é€šç”¨ | âœ… ä¸“é—¨ä¼˜åŒ– | **Recall** |
| **æŒä¹…æ¡ä»¶ç³»ç»Ÿ** | âŒ æ—  | âœ… 15ç§æ¡ä»¶ç±»å‹+è‡ªåŠ¨æå– | **Recall** |
| äº‘ç«¯æ‰˜ç®¡ | âœ… å¯é€‰ | âŒ çº¯æœ¬åœ° | mem0 (ä¾¿æ·) |
| éƒ¨ç½²å¤æ‚åº¦ | éœ€è¦é…ç½® | pip install | **Recall** |
| ä¸­æ–‡æ”¯æŒ | ä¸€èˆ¬ | âœ… jieba+spaCy | **Recall** |

### å¯¹æ¯” cognee (10.9k stars)

| åŠŸèƒ½ | cognee | Recall | ä¼˜åŠ¿æ–¹ |
|------|--------|--------|--------|
| çŸ¥è¯†å›¾è°± | âœ… Neo4j | âœ… è½»é‡æœ¬åœ°å›¾ | cognee (åŠŸèƒ½å¼º) |
| å¤šæ¨¡æ€ | âœ… å›¾ç‰‡/éŸ³é¢‘ | âŒ æ–‡æœ¬ | cognee |
| éƒ¨ç½²å¤æ‚åº¦ | éœ€Neo4j | pip install | **Recall** |
| **ä¼ç¬”è¿½è¸ª** | âŒ æ—  | âœ… å®Œæ•´ç³»ç»Ÿ | **Recall** |
| **100%ä¸é—å¿˜** | âŒ ä¼šå‹ç¼© | âœ… 8å±‚é˜²æŠ¤ | **Recall** |
| **RPåœºæ™¯** | âŒ é€šç”¨ | âœ… ä¸“é—¨ä¼˜åŒ– | **Recall** |
| GraphRAG | âœ… å®Œæ•´ | âœ… ç®€åŒ–ç‰ˆ | cognee |
| ä¾èµ–é¡¹ | å¤š | å°‘ | **Recall** |

### Recall çš„ç‹¬ç‰¹å®šä½

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Recall çš„æ ¸å¿ƒå·®å¼‚åŒ–                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. ã€100%ä¸é—å¿˜ã€‘                                           â”‚
â”‚     - mem0/cognee éƒ½ä¼šå‹ç¼©è®°å¿†ï¼Œå¯èƒ½ä¸¢å¤±ç»†èŠ‚                  â”‚
â”‚     - Recall ä¿ç•™åŸæ–‡ + 8å±‚æ£€ç´¢é˜²æŠ¤                          â”‚
â”‚                                                             â”‚
â”‚  2. ã€ä¼ç¬”è¿½è¸ªã€‘ï¼ˆç‹¬æœ‰ï¼‰                                      â”‚
â”‚     - æ‰‹åŠ¨åŸ‹ä¼ç¬” + LLM è¾…åŠ©æ£€æµ‹ï¼ˆå¯é€‰ï¼‰                       â”‚
â”‚     - ä¸»åŠ¨æé†’æœªè§£å†³çš„ä¼ç¬”                                    â”‚
â”‚     - ç¡®ä¿æ•…äº‹è¿è´¯æ€§                                         â”‚
â”‚                                                             â”‚
â”‚  3. ã€RP/å°è¯´åœºæ™¯ä¸“ç²¾ã€‘                                      â”‚
â”‚     - è§’è‰²éš”ç¦»ï¼ˆä¸åŒè§’è‰²è®°å¿†ä¸æ··æ·†ï¼‰                          â”‚
â”‚     - è§„èŒƒéµå®ˆæ£€æŸ¥ï¼ˆè®¾å®šä¸ä¼šè‡ªç›¸çŸ›ç›¾ï¼‰                        â”‚
â”‚     - å…³ç³»å›¾è°±ï¼ˆäººç‰©å…³ç³»å¯è§†åŒ–ï¼‰                             â”‚
â”‚                                                             â”‚
â”‚  4. ã€é›¶é—¨æ§›ã€‘                                               â”‚
â”‚     - ä¸éœ€è¦ Neo4j / å‘é‡æ•°æ®åº“                              â”‚
â”‚     - pip install + API key å³å¯                            â”‚
â”‚     - SillyTavern ä¸€é”®å®‰è£…                                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **æ¶æ„è¯´æ˜**ï¼š
> - âœ… **çº¯æœ¬åœ°æ’ä»¶**ï¼šæ‰€æœ‰æ•°æ®å­˜å‚¨åœ¨é¡¹ç›®ç›®å½•çš„ `./recall_data/` ä¸­
> - âœ… **æ— äº‘ç«¯ä¾èµ–**ï¼šä¸ä¾èµ–ä»»ä½•äº‘ç«¯æœåŠ¡
> - âœ… **ç”¨æˆ·è‡ªå¤‡ API key**ï¼šç”¨æˆ·ä½¿ç”¨è‡ªå·±çš„ OpenAI/Claude/å…¶ä»– API key è°ƒç”¨å¤§æ¨¡å‹
> - âš ï¸ é¦–æ¬¡å®‰è£…éœ€è¦ä¸‹è½½ ~600MB çš„ NLP æ¨¡å‹
> - âš ï¸ è¿è¡Œæ—¶å†…å­˜çº¦éœ€ 500-600MB

---

## åä¸‰ã€SillyTavern æ’ä»¶å®Œæ•´å®ç°

### manifest.json
```json
{
    "display_name": "Recall - AIæ°¸ä¹…è®°å¿†",
    "loading_order": 1,
    "js": "index.js",
    "css": "style.css",
    "author": "Recall Team",
    "version": "1.0.0",
    "homePage": "https://github.com/recall-ai/recall",
    "auto_update": true,
    "generate_interceptor": "recallPromptInterceptor",
    "i18n": {
        "zh-cn": "i18n/zh-cn.json",
        "en-us": "i18n/en-us.json"
    }
}
```

> **æ³¨æ„**ï¼š`requires` å’Œ `optional` å­—æ®µå·²å¼ƒç”¨ï¼ˆç”¨äºæ—§ç‰ˆ Extras APIï¼‰ï¼Œæ–°æ‰©å±•ä¸éœ€è¦ã€‚

### index.jsï¼ˆå®Œæ•´ç‰ˆï¼‰
```javascript
// SillyTavern Recall æ’ä»¶ - ç¬¦åˆå®˜æ–¹è§„èŒƒ
// ä½¿ç”¨ SillyTavern å…¨å±€å¯¹è±¡è·å–ä¸Šä¸‹æ–‡ï¼ˆå®˜æ–¹æ¨èæ–¹å¼ï¼‰

const MODULE_NAME = 'recall';
const defaultSettings = Object.freeze({
    enabled: true,
    apiKey: '',           // ç”¨æˆ·è‡ªå·±çš„ API key
    autoInject: true,     // è‡ªåŠ¨æ³¨å…¥ä¸Šä¸‹æ–‡
    maxContextTokens: 8000,
    showIndicator: true,
    language: 'zh',
    backendUrl: 'http://localhost:5000',  // æœ¬åœ°åç«¯åœ°å€
});

// è·å–æˆ–åˆå§‹åŒ–è®¾ç½®ï¼ˆå®˜æ–¹æ¨èæ¨¡å¼ï¼‰
function getSettings() {
    const { extensionSettings } = SillyTavern.getContext();
    
    if (!extensionSettings[MODULE_NAME]) {
        extensionSettings[MODULE_NAME] = structuredClone(defaultSettings);
    }
    
    // ç¡®ä¿æ‰€æœ‰é»˜è®¤é”®å­˜åœ¨ï¼ˆæ›´æ–°åå…¼å®¹ï¼‰
    for (const key of Object.keys(defaultSettings)) {
        if (!Object.hasOwn(extensionSettings[MODULE_NAME], key)) {
            extensionSettings[MODULE_NAME][key] = defaultSettings[key];
        }
    }
    
    return extensionSettings[MODULE_NAME];
}

// åˆå§‹åŒ– - ä½¿ç”¨ APP_READY äº‹ä»¶ç¡®ä¿ ST å®Œå…¨åŠ è½½
(async () => {
    const { eventSource, event_types } = SillyTavern.getContext();
    
    // ç­‰å¾…åº”ç”¨å‡†å¤‡å°±ç»ª
    eventSource.on(event_types.APP_READY, initRecallExtension);
})();

async function initRecallExtension() {
    const { eventSource, event_types, saveSettingsDebounced } = SillyTavern.getContext();
    const settings = getSettings();
    
    // æ·»åŠ è®¾ç½®é¢æ¿
    const settingsHtml = `
    <div id="recall-settings" class="recall-settings">
        <div class="inline-drawer">
            <div class="inline-drawer-toggle inline-drawer-header">
                <b>ğŸ§  Recall è®°å¿†è®¾ç½®</b>
                <div class="inline-drawer-icon fa-solid fa-circle-chevron-down down"></div>
            </div>
            <div class="inline-drawer-content">
                <div class="recall-setting">
                    <label>
                        <input type="checkbox" id="recall-enabled">
                        å¯ç”¨ Recall
                    </label>
                </div>
                <div class="recall-setting">
                    <label>API Keyï¼ˆä½ çš„ OpenAI/Claude keyï¼‰</label>
                    <input type="password" id="recall-api-key" placeholder="sk-...">
                    <small>ç”¨äºè°ƒç”¨å¤§æ¨¡å‹ï¼ŒRecall ä¸ä¼šä¸Šä¼ ä½ çš„ key</small>
                </div>
                <div class="recall-setting">
                    <label>æœ¬åœ°åç«¯åœ°å€</label>
                    <input type="text" id="recall-backend-url" placeholder="http://localhost:5000">
                    <small>è¿è¡Œ recall server åçš„åœ°å€</small>
                </div>
                <div class="recall-setting">
                    <label>
                        <input type="checkbox" id="recall-auto-inject">
                        è‡ªåŠ¨æ³¨å…¥è®°å¿†ä¸Šä¸‹æ–‡
                    </label>
                </div>
                <hr>
                <div class="recall-stats" id="recall-stats">
                    <span>è®°å¿†: åŠ è½½ä¸­...</span>
                </div>
                <button id="recall-search-btn" class="menu_button">ğŸ” æœç´¢è®°å¿†</button>
            </div>
        </div>
    </div>`;
    
    $("#extensions_settings").append(settingsHtml);
    
    // ç»‘å®šäº‹ä»¶ - ä½¿ç”¨ settings å˜é‡å’Œ saveSettingsDebounced
    $("#recall-enabled").prop("checked", settings.enabled).on("change", function() {
        settings.enabled = this.checked;
        saveSettingsDebounced();
    });
    
    $("#recall-api-key").val(settings.apiKey).on("change", function() {
        settings.apiKey = this.value;
        saveSettingsDebounced();
    });
    
    $("#recall-backend-url").val(settings.backendUrl).on("change", function() {
        settings.backendUrl = this.value;
        saveSettingsDebounced();
    });
    
    $("#recall-auto-inject").prop("checked", settings.autoInject).on("change", function() {
        settings.autoInject = this.checked;
        saveSettingsDebounced();
    });
    
    $("#recall-search-btn").on("click", showSearchDialog);
    
    // åˆå§‹åŒ–åç«¯è¿æ¥
    await initRecallBackend();
    
    // ç›‘å¬æ¶ˆæ¯äº‹ä»¶
    eventSource.on(event_types.MESSAGE_RECEIVED, onMessageReceived);
    eventSource.on(event_types.CHAT_CHANGED, onChatChanged);
    // æ³¨æ„ï¼šä¸Šä¸‹æ–‡æ³¨å…¥é€šè¿‡ manifest.json çš„ generate_interceptor å®ç°
    // ä¸ä½¿ç”¨ GENERATE_BEFOREï¼ˆè¯¥äº‹ä»¶ä¸å­˜åœ¨ï¼‰
    
    console.log('[Recall] æ‰©å±•åˆå§‹åŒ–å®Œæˆ');
}

// Recall åç«¯å®¢æˆ·ç«¯
let recallClient = null;

async function initRecallBackend() {
    const settings = getSettings();
    
    try {
        // è¿æ¥æœ¬åœ°åç«¯
        recallClient = new RecallClient(settings.backendUrl, settings.apiKey);
        await recallClient.init();
        updateStatsDisplay();
        toastr.success('Recall æœ¬åœ°åç«¯è¿æ¥æˆåŠŸ', 'Recall');
    } catch (error) {
        console.error('[Recall] åˆå§‹åŒ–å¤±è´¥:', error);
        toastr.warning('Recall åç«¯æœªå¯åŠ¨ï¼Œè¯·å…ˆè¿è¡Œ recall server', 'Recall');
    }
}

class RecallClient {
    constructor(backendUrl, apiKey) {
        this.apiKey = apiKey;
        this.baseUrl = backendUrl || 'http://localhost:5000';
    }
    
    async init() {
        // å¥åº·æ£€æŸ¥
        const response = await fetch(`${this.baseUrl}/health`);
        if (!response.ok) throw new Error('Backend unavailable');
        return true;
    }
    
    async processTurn(userMessage, assistantMessage, metadata = {}) {
        const response = await fetch(`${this.baseUrl}/api/process`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.apiKey}`,
            },
            body: JSON.stringify({
                user: userMessage,
                assistant: assistantMessage,
                metadata,
            }),
        });
        return response.json();
    }
    
    async buildContext(userInput, maxTokens = 8000) {
        const response = await fetch(`${this.baseUrl}/api/context`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.apiKey}`,
            },
            body: JSON.stringify({
                user_input: userInput,
                max_tokens: maxTokens,
            }),
        });
        return response.json();
    }
    
    async search(query, limit = 20) {
        const response = await fetch(`${this.baseUrl}/api/search?q=${encodeURIComponent(query)}&limit=${limit}`, {
            headers: { 'Authorization': `Bearer ${this.apiKey}` },
        });
        return response.json();
    }
    
    async getStats() {
        const response = await fetch(`${this.baseUrl}/api/stats`, {
            headers: { 'Authorization': `Bearer ${this.apiKey}` },
        });
        return response.json();
    }
}

// æ¶ˆæ¯æ¥æ”¶å¤„ç†
// æ¶ˆæ¯æ¥æ”¶å¤„ç†
async function onMessageReceived(data) {
    const settings = getSettings();
    if (!settings.enabled || !recallClient) return;
    
    const { chat, characterId, chatId } = SillyTavern.getContext();
    if (!chat || chat.length < 2) return;
    
    const lastUserMsg = chat.filter(m => m.is_user).pop();
    const lastAssistantMsg = chat.filter(m => !m.is_user && !m.is_system).pop();
    
    if (lastUserMsg && lastAssistantMsg) {
        try {
            await recallClient.processTurn(
                lastUserMsg.mes,
                lastAssistantMsg.mes,
                {
                    character: characterId,
                    chat_id: chatId,
                }
            );
            updateStatsDisplay();
        } catch (error) {
            console.error('[Recall] å¤„ç†æ¶ˆæ¯å¤±è´¥:', error);
        }
    }
}

/**
 * Prompt Interceptor - åœ¨ç”Ÿæˆå‰æ³¨å…¥è®°å¿†ä¸Šä¸‹æ–‡
 * é€šè¿‡ manifest.json çš„ generate_interceptor å­—æ®µæ³¨å†Œ
 * å¿…é¡»æ˜¯å…¨å±€å‡½æ•°ï¼ˆæŒ‚è½½åˆ° globalThisï¼‰
 * 
 * @param {Array} chat - èŠå¤©å†å²æ•°ç»„ï¼ˆå¯å˜ï¼‰
 * @param {number} contextSize - å½“å‰ä¸Šä¸‹æ–‡å¤§å°ï¼ˆtokensï¼‰
 * @param {Function} abort - è°ƒç”¨ä»¥ä¸­æ­¢ç”Ÿæˆ
 * @param {string} type - ç”Ÿæˆç±»å‹ï¼ˆ'quiet', 'regenerate', 'impersonate' ç­‰ï¼‰
 */
globalThis.recallPromptInterceptor = async function(chat, contextSize, abort, type) {
    const settings = getSettings();
    
    if (!settings.enabled || !recallClient) return;
    if (!settings.autoInject) return;
    
    // è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    const lastUserMessage = chat.filter(m => m.is_user).pop();
    if (!lastUserMessage) return;
    
    try {
        const recallContext = await recallClient.buildContext(
            lastUserMessage.mes,
            settings.maxContextTokens
        );
        
        // æ³¨å…¥è®°å¿†ä¸Šä¸‹æ–‡ä½œä¸ºç³»ç»Ÿæ¶ˆæ¯
        if (recallContext && recallContext.text) {
            const systemNote = {
                is_user: false,
                is_system: true,
                name: 'Recall Memory',
                mes: `[Recall è®°å¿†ä¸Šä¸‹æ–‡]\n${recallContext.text}\n[/Recall]`,
                send_date: Date.now(),
            };
            // æ’å…¥åˆ°ç”¨æˆ·æ¶ˆæ¯ä¹‹å‰
            const lastUserIndex = chat.lastIndexOf(lastUserMessage);
            chat.splice(lastUserIndex, 0, systemNote);
        }
    } catch (error) {
        console.error('[Recall] æ„å»ºä¸Šä¸‹æ–‡å¤±è´¥:', error);
    }
};

// èŠå¤©åˆ‡æ¢
async function onChatChanged() {
    if (recallClient) {
        updateStatsDisplay();
    }
}

// æ›´æ–°ç»Ÿè®¡æ˜¾ç¤º
async function updateStatsDisplay() {
    if (!recallClient) return;
    
    try {
        const stats = await recallClient.getStats();
        $("#recall-stats").html(`
            <span>ğŸ“š ${stats.total_turns} è½®å¯¹è¯</span>
            <span>ğŸ‘¥ ${stats.total_entities} ä¸ªå®ä½“</span>
            <span>ğŸ“Œ ${stats.unresolved_foreshadowing} ä¸ªä¼ç¬”</span>
        `);
    } catch (error) {
        $("#recall-stats").html('<span>âš ï¸ æ— æ³•è·å–ç»Ÿè®¡</span>');
    }
}

// æœç´¢å¯¹è¯æ¡†
function showSearchDialog() {
    const html = `
    <div id="recall-search-dialog">
        <h3>ğŸ” æœç´¢è®°å¿†</h3>
        <input type="text" id="recall-search-input" placeholder="è¾“å…¥å…³é”®è¯ï¼Œå¦‚è§’è‰²åã€äº‹ä»¶ã€"æœ€å¼€å§‹"ç­‰...">
        <div id="recall-search-results"></div>
        <div class="recall-search-tips">
            ğŸ’¡ æç¤ºï¼šæœç´¢ "ä¼ç¬”" æŸ¥çœ‹æœªè§£å†³çš„ä¼ç¬”
        </div>
    </div>`;
    
    callPopup(html, 'text', '', { wide: true, large: true });
    
    let searchTimeout;
    $("#recall-search-input").on("input", function() {
        clearTimeout(searchTimeout);
        const query = this.value;
        
        if (query.length < 2) {
            $("#recall-search-results").html('');
            return;
        }
        
        searchTimeout = setTimeout(async () => {
            try {
                const results = await recallClient.search(query);
                displaySearchResults(results);
            } catch (error) {
                $("#recall-search-results").html('<div class="error">æœç´¢å¤±è´¥</div>');
            }
        }, 300);
    });
}

function displaySearchResults(results) {
    if (!results || results.length === 0) {
        $("#recall-search-results").html('<div class="no-results">ğŸ˜… æ²¡æ‰¾åˆ°ç›¸å…³è®°å¿†</div>');
        return;
    }
    
    const html = results.slice(0, 20).map(r => `
        <div class="recall-result-item">
            <div class="turn">ç¬¬ ${r.turn || '?'} è½®</div>
            <div class="summary">${r.summary || r.user?.substring(0, 100) || '...'}</div>
        </div>
    `).join('');
    
    $("#recall-search-results").html(html);
}
```

### style.css
```css
/* Recall STæ’ä»¶æ ·å¼ */
.recall-settings {
    padding: 10px;
}

.recall-setting {
    margin: 10px 0;
}

.recall-setting label {
    display: block;
    margin-bottom: 5px;
    color: var(--SmartThemeBodyColor);
}

.recall-setting select,
.recall-setting input[type="text"],
.recall-setting input[type="password"] {
    width: 100%;
    padding: 8px;
    border-radius: 4px;
    border: 1px solid var(--SmartThemeBorderColor);
    background: var(--SmartThemeBlurTintColor);
    color: var(--SmartThemeBodyColor);
}

.recall-stats {
    display: flex;
    gap: 10px;
    flex-wrap: wrap;
    padding: 10px;
    background: var(--SmartThemeBlurTintColor);
    border-radius: 4px;
    margin: 10px 0;
    font-size: 0.9em;
}

#recall-search-dialog {
    padding: 20px;
}

#recall-search-dialog h3 {
    margin-bottom: 15px;
}

#recall-search-input {
    width: 100%;
    padding: 12px;
    font-size: 16px;
    border: 2px solid var(--SmartThemeBorderColor);
    border-radius: 8px;
    margin-bottom: 15px;
}

#recall-search-results {
    max-height: 400px;
    overflow-y: auto;
}

.recall-result-item {
    padding: 12px;
    border-bottom: 1px solid var(--SmartThemeBorderColor);
    cursor: pointer;
}

.recall-result-item:hover {
    background: var(--SmartThemeBlurTintColor);
}

.recall-result-item .turn {
    font-size: 0.8em;
    color: var(--SmartThemeQuoteColor);
}

.recall-result-item .summary {
    margin-top: 5px;
}

.recall-search-tips {
    margin-top: 15px;
    font-size: 0.9em;
    color: var(--SmartThemeQuoteColor);
}

.no-results {
    text-align: center;
    padding: 40px;
    color: var(--SmartThemeQuoteColor);
}
```

### i18n/zh-cn.jsonï¼ˆå›½é™…åŒ–æ–‡ä»¶ï¼‰
```json
{
    "Recall Memory Settings": "ğŸ§  Recall è®°å¿†è®¾ç½®",
    "Enable Recall": "å¯ç”¨ Recall",
    "API Key (your OpenAI/Claude key)": "API Keyï¼ˆä½ çš„ OpenAI/Claude keyï¼‰",
    "Recall will not upload your key": "ç”¨äºè°ƒç”¨å¤§æ¨¡å‹ï¼ŒRecall ä¸ä¼šä¸Šä¼ ä½ çš„ key",
    "Local backend address": "æœ¬åœ°åç«¯åœ°å€",
    "Auto inject memory context": "è‡ªåŠ¨æ³¨å…¥è®°å¿†ä¸Šä¸‹æ–‡",
    "Search Memory": "ğŸ” æœç´¢è®°å¿†",
    "Loading...": "åŠ è½½ä¸­...",
    "conversations": "è½®å¯¹è¯",
    "entities": "ä¸ªå®ä½“",
    "foreshadowing": "ä¸ªä¼ç¬”",
    "Search failed": "æœç´¢å¤±è´¥",
    "No results found": "ğŸ˜… æ²¡æ‰¾åˆ°ç›¸å…³è®°å¿†"
}
```

### i18n/en-us.json
```json
{
    "Recall Memory Settings": "ğŸ§  Recall Memory Settings",
    "Enable Recall": "Enable Recall",
    "API Key (your OpenAI/Claude key)": "API Key (your OpenAI/Claude key)",
    "Recall will not upload your key": "Used to call LLM, Recall will not upload your key",
    "Local backend address": "Local backend address",
    "Auto inject memory context": "Auto inject memory context",
    "Search Memory": "ğŸ” Search Memory",
    "Loading...": "Loading...",
    "conversations": "conversations",
    "entities": "entities",
    "foreshadowing": "foreshadowing items",
    "Search failed": "Search failed",
    "No results found": "ğŸ˜… No relevant memories found"
}
```

### Slash å‘½ä»¤æ³¨å†Œï¼ˆSTscript é›†æˆï¼‰

åœ¨ `index.js` çš„ `initRecallExtension()` å‡½æ•°æœ«å°¾æ·»åŠ ï¼š

```javascript
// æ³¨å†Œ Slash å‘½ä»¤ï¼ˆå®˜æ–¹æ¨èæ–¹å¼ï¼‰
function registerSlashCommands() {
    const { SlashCommandParser, SlashCommand, SlashCommandArgument, 
            SlashCommandNamedArgument, ARGUMENT_TYPE } = SillyTavern.getContext();
    
    // /recall-search å‘½ä»¤
    SlashCommandParser.addCommandObject(SlashCommand.fromProps({
        name: 'recall-search',
        callback: async (namedArgs, unnamedArgs) => {
            if (!recallClient) return 'âŒ Recall åç«¯æœªè¿æ¥';
            
            const query = unnamedArgs.toString();
            const limit = namedArgs.limit ?? 10;
            
            try {
                const results = await recallClient.search(query, limit);
                if (!results || results.length === 0) {
                    return 'ğŸ˜… æ²¡æ‰¾åˆ°ç›¸å…³è®°å¿†';
                }
                return results.map(r => 
                    `[ç¬¬${r.turn}è½®] ${r.summary || r.user?.substring(0, 50)}`
                ).join('\n');
            } catch (e) {
                return `âŒ æœç´¢å¤±è´¥: ${e.message}`;
            }
        },
        aliases: ['rs'],
        returns: 'æœç´¢ç»“æœåˆ—è¡¨',
        namedArgumentList: [
            SlashCommandNamedArgument.fromProps({
                name: 'limit',
                description: 'æœ€å¤§è¿”å›æ•°é‡',
                typeList: ARGUMENT_TYPE.NUMBER,
                defaultValue: '10',
            }),
        ],
        unnamedArgumentList: [
            SlashCommandArgument.fromProps({
                description: 'æœç´¢å…³é”®è¯',
                typeList: ARGUMENT_TYPE.STRING,
                isRequired: true,
            }),
        ],
        helpString: `
            <div>åœ¨ Recall è®°å¿†åº“ä¸­æœç´¢ç›¸å…³å†…å®¹</div>
            <div>
                <strong>ç¤ºä¾‹:</strong>
                <ul>
                    <li><pre><code class="language-stscript">/recall-search å°æ˜çš„ç”Ÿæ—¥</code></pre></li>
                    <li><pre><code class="language-stscript">/recall-search limit=5 ä¸Šæ¬¡çº¦ä¼š</code></pre></li>
                </ul>
            </div>
        `,
    }));
    
    // /recall-stats å‘½ä»¤
    SlashCommandParser.addCommandObject(SlashCommand.fromProps({
        name: 'recall-stats',
        callback: async () => {
            if (!recallClient) return 'âŒ Recall åç«¯æœªè¿æ¥';
            
            try {
                const stats = await recallClient.getStats();
                return `ğŸ“Š Recall ç»Ÿè®¡:\n` +
                       `ğŸ“š å¯¹è¯è½®æ•°: ${stats.total_turns}\n` +
                       `ğŸ‘¥ å®ä½“æ•°é‡: ${stats.total_entities}\n` +
                       `ğŸ“Œ æœªè§£å†³ä¼ç¬”: ${stats.unresolved_foreshadowing}`;
            } catch (e) {
                return `âŒ è·å–ç»Ÿè®¡å¤±è´¥: ${e.message}`;
            }
        },
        aliases: ['rstat'],
        returns: 'Recall ç»Ÿè®¡ä¿¡æ¯',
        helpString: '<div>æ˜¾ç¤º Recall è®°å¿†åº“ç»Ÿè®¡ä¿¡æ¯</div>',
    }));
    
    // /recall-forget å‘½ä»¤ï¼ˆå±é™©æ“ä½œï¼‰
    SlashCommandParser.addCommandObject(SlashCommand.fromProps({
        name: 'recall-forget',
        callback: async (namedArgs) => {
            if (!recallClient) return 'âŒ Recall åç«¯æœªè¿æ¥';
            
            const entity = namedArgs.entity;
            if (!entity) return 'âŒ è¯·æŒ‡å®šè¦é—å¿˜çš„å®ä½“å';
            
            // è¿™é‡Œåº”è¯¥è°ƒç”¨ recallClient.forgetEntity(entity)
            return `ğŸ—‘ï¸ å·²é—å¿˜å…³äº "${entity}" çš„è®°å¿†ï¼ˆæ¨¡æ‹Ÿï¼‰`;
        },
        namedArgumentList: [
            SlashCommandNamedArgument.fromProps({
                name: 'entity',
                description: 'è¦é—å¿˜çš„å®ä½“åç§°',
                typeList: ARGUMENT_TYPE.STRING,
                isRequired: true,
            }),
        ],
        helpString: '<div>âš ï¸ å±é™©æ“ä½œï¼šé—å¿˜æŒ‡å®šå®ä½“çš„æ‰€æœ‰è®°å¿†</div>',
    }));
}

// åœ¨ initRecallExtension æœ«å°¾è°ƒç”¨
registerSlashCommands();
```

### æ–‡ä»¶ç»“æ„
```
recall-st-extension/
â”œâ”€â”€ manifest.json
â”œâ”€â”€ index.js
â”œâ”€â”€ style.css
â””â”€â”€ i18n/
    â”œâ”€â”€ zh-cn.json
    â””â”€â”€ en-us.json
```

---

## åå››ã€HTTP API æœåŠ¡ç«¯

```python
# recall/server.py
"""
Recall HTTP API æœåŠ¡ - ä¾›STæ’ä»¶å’Œå…¶ä»–å®¢æˆ·ç«¯è°ƒç”¨
"""

from fastapi import FastAPI, HTTPException, Depends, Header
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, Dict, List
import os

app = FastAPI(
    title="Recall API",
    description="AIæ°¸ä¹…è®°å¿†ç³»ç»Ÿ API",
    version="1.0.0"
)

# CORSé…ç½®ï¼ˆå…è®¸STæ’ä»¶è·¨åŸŸè®¿é—®ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å¼•æ“å®ä¾‹
engine = None

def get_engine():
    global engine
    if engine is None:
        from .engine import RecallEngine
        engine = RecallEngine(
            mode=os.environ.get('RECALL_MODE', 'local'),
            api_key=os.environ.get('OPENAI_API_KEY'),
        )
    return engine

# è¯·æ±‚æ¨¡å‹
class ProcessRequest(BaseModel):
    user: str
    assistant: str
    metadata: Optional[Dict] = None

class ContextRequest(BaseModel):
    user_input: str
    max_tokens: int = 8000

# APIç«¯ç‚¹
@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "version": "1.0.0"}

@app.post("/api/process")
async def process_turn(request: ProcessRequest, engine = Depends(get_engine)):
    """å¤„ç†ä¸€è½®å¯¹è¯"""
    result = engine.process_turn(
        request.user,
        request.assistant,
        request.metadata
    )
    return {
        "turn": result.turn_number,
        "entities": [e.name for e in result.entities_detected],
        "new_foreshadowing": len(result.new_foreshadowing),
        "resolved_foreshadowing": len(result.foreshadowing_resolved),
        "consistent": result.consistency_result.is_consistent if result.consistency_result else True,
    }

@app.post("/api/context")
async def build_context(request: ContextRequest, engine = Depends(get_engine)):
    """æ„å»ºæ£€ç´¢ä¸Šä¸‹æ–‡"""
    result = engine.build_context(
        request.user_input,
        request.max_tokens
    )
    return {
        "text": result.text,
        "token_count": result.token_count,
        "memories_count": len(result.memories_used),
    }

@app.get("/api/search")
async def search(q: str, limit: int = 20, engine = Depends(get_engine)):
    """æœç´¢è®°å¿†"""
    results = engine.search(q, limit)
    return [
        {
            "turn": r.get('turn'),
            "summary": r.get('summary', r.get('user', '')[:100]),
            "score": r.get('score', 0),
        }
        for r in results
    ]

@app.get("/api/stats")
async def get_stats(engine = Depends(get_engine)):
    """è·å–ç»Ÿè®¡"""
    return engine.get_stats()

@app.get("/api/foreshadowing")
async def get_foreshadowing(engine = Depends(get_engine)):
    """è·å–ä¼ç¬”åˆ—è¡¨"""
    active = engine.foreshadowing_tracker.active_foreshadowing
    return [
        {
            "created_turn": fs.created_turn,
            "summary": fs.summary,
            "status": str(fs.status),
        }
        for fs in active
    ]

# è¿è¡ŒæœåŠ¡å™¨
def run_server(host: str = "0.0.0.0", port: int = 5000):
    import uvicorn
    uvicorn.run(app, host=host, port=port)

if __name__ == "__main__":
    run_server()
```

---

## åäº”ã€é¡¹ç›®å…¥å£æ–‡ä»¶

```python
# recall/__init__.py
"""
Recall - AIæ°¸ä¹…è®°å¿†ç³»ç»Ÿ

è®©AIæ°¸è¿œä¸ä¼šå¿˜è®°ä½ è¯´è¿‡çš„æ¯ä¸€å¥è¯ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    from recall import RecallEngine
    
    engine = RecallEngine()
    engine.process_turn("ç”¨æˆ·è¯´çš„è¯", "AIå›å¤")
    context = engine.build_context("æ–°çš„é—®é¢˜")
"""

from .engine import RecallEngine
from .version import __version__

__all__ = ['RecallEngine', '__version__']

# recall/version.py
__version__ = '1.0.0'

# recall/__main__.py
"""å‘½ä»¤è¡Œå…¥å£"""
from .cli import main

if __name__ == '__main__':
    main()
```

---

## åå…­ã€å¿«é€Ÿå¼€å§‹

### SillyTavern ç”¨æˆ·ï¼ˆ3æ­¥å®Œæˆï¼‰
```
1. æ‰“å¼€æ‰©å±•å¸‚åœº â†’ æœç´¢ "Recall"
2. ç‚¹å‡»"å®‰è£…"
3. å®Œæˆï¼AIç°åœ¨æ°¸è¿œä¸ä¼šå¿˜è®°äº†
```

### å‘½ä»¤è¡Œç”¨æˆ·ï¼ˆ2æ­¥å®Œæˆï¼‰
```bash
pip install recall-ai
recall init    # è¾“å…¥ä½ çš„ API key
recall chat    # å¼€å§‹ä½¿ç”¨
```

### å¼€å‘è€…é›†æˆï¼ˆ5è¡Œä»£ç ï¼‰
```python
from recall import RecallEngine

engine = RecallEngine(api_key='sk-xxx')  # ä½ çš„ API key

# æ¯è½®å¯¹è¯åè°ƒç”¨
engine.process_turn("ç”¨æˆ·è¯´çš„è¯", "AIå›å¤çš„å†…å®¹")

# ç”Ÿæˆå‰è·å–ä¸Šä¸‹æ–‡
context = engine.build_context("ç”¨æˆ·æ–°çš„é—®é¢˜")
# å°† context.text æ³¨å…¥åˆ° system prompt
```

### Pythoné¡¹ç›®é›†æˆ
```python
# ä½œä¸ºä¸­é—´ä»¶ä½¿ç”¨
from recall import RecallEngine

class RecallMiddleware:
    def __init__(self, llm_client):
        self.engine = RecallEngine()
        self.llm = llm_client
    
    def chat(self, user_message):
        # 1. æ„å»ºå¸¦è®°å¿†çš„ä¸Šä¸‹æ–‡
        context = self.engine.build_context(user_message)
        
        # 2. è°ƒç”¨LLM
        response = self.llm.chat([
            {"role": "system", "content": context.text},
            {"role": "user", "content": user_message}
        ])
        
        # 3. è®°å½•è¿™è½®å¯¹è¯
        self.engine.process_turn(user_message, response)
        
        return response
```

---

## é™„å½•ï¼šå®Œæ•´æ–‡ä»¶æ¸…å•

```
recall/
â”œâ”€â”€ __init__.py          # åŒ…å…¥å£
â”œâ”€â”€ __main__.py          # CLIå…¥å£
â”œâ”€â”€ version.py           # ç‰ˆæœ¬å·
â”œâ”€â”€ config.py            # é…ç½®ï¼ˆå«è½»é‡æ¨¡å¼ï¼‰
â”œâ”€â”€ init.py              # åˆå§‹åŒ–å‘å¯¼ï¼ˆç¯å¢ƒéš”ç¦»ï¼‰
â”œâ”€â”€ engine.py            # æ ¸å¿ƒå¼•æ“ï¼ˆç¬¬ä¹èŠ‚ï¼‰
â”œâ”€â”€ cli.py               # å‘½ä»¤è¡Œå·¥å…·ï¼ˆç¬¬åèŠ‚ï¼‰
â”œâ”€â”€ server.py            # HTTP APIï¼ˆç¬¬åå››èŠ‚ï¼‰
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ volume_manager.py    # åˆ†å·ç®¡ç†ï¼ˆç¬¬å››èŠ‚å®Œæ•´ç‰ˆï¼‰
â”‚   â”œâ”€â”€ layer0_core.py       # L0æ ¸å¿ƒè®¾å®š
â”‚   â”œâ”€â”€ layer1_consolidated.py # L1é•¿æœŸè®°å¿†
â”‚   â”œâ”€â”€ layer2_working.py    # L2å·¥ä½œè®°å¿†
â”‚   â”œâ”€â”€ multi_tenant.py      # å¤šç”¨æˆ·/å¤šè§’è‰²æ”¯æŒ
â”‚   â””â”€â”€ archive.py           # L3å½’æ¡£å­˜å‚¨
â”œâ”€â”€ index/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ entity_index.py      # å®ä½“ç´¢å¼•
â”‚   â”œâ”€â”€ inverted_index.py    # å€’æ’ç´¢å¼•
â”‚   â”œâ”€â”€ vector_index.py      # å‘é‡ç´¢å¼•ï¼ˆç¯å¢ƒéš”ç¦»ï¼‰
â”‚   â””â”€â”€ ngram_index.py       # N-gramç´¢å¼•
â”œâ”€â”€ graph/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ knowledge_graph.py   # çŸ¥è¯†å›¾è°±
â”‚   â””â”€â”€ relation_extractor.py # å…³ç³»æå–
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ eight_layer.py       # 8å±‚æ£€ç´¢ï¼ˆç¬¬äº”èŠ‚ï¼‰
â”‚   â”œâ”€â”€ parallel_retrieval.py # å¹¶è¡Œæ£€ç´¢ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
â”‚   â””â”€â”€ context_builder.py   # ä¸Šä¸‹æ–‡æ„å»º
â”œâ”€â”€ processor/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ entity_extractor.py  # å®ä½“æå–ï¼ˆç¯å¢ƒéš”ç¦»ï¼‰
â”‚   â”œâ”€â”€ foreshadowing.py     # ä¼ç¬”è¿½è¸ªï¼ˆå¢å¼ºç‰ˆï¼‰
â”‚   â”œâ”€â”€ consistency.py       # ä¸€è‡´æ€§æ ¡éªŒ
â”‚   â”œâ”€â”€ code_indexer.py      # ä»£ç ç´¢å¼•
â”‚   â”œâ”€â”€ memory_summarizer.py # è®°å¿†æ€»ç»“ï¼ˆå¯¹æ ‡mem0ï¼‰
â”‚   â””â”€â”€ scenario.py          # åœºæ™¯æ£€æµ‹
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ environment.py       # ğŸ†• ç¯å¢ƒéš”ç¦»å·¥å…·
â”‚   â”œâ”€â”€ llm_client.py        # LLMè°ƒç”¨å°è£…
â”‚   â”œâ”€â”€ warmup.py            # ç³»ç»Ÿé¢„çƒ­
â”‚   â”œâ”€â”€ perf_monitor.py      # æ€§èƒ½ç›‘æ§
â”‚   â””â”€â”€ auto_maintain.py     # è‡ªåŠ¨ç»´æŠ¤
â”œâ”€â”€ compat/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ mem0_compat.py       # mem0 å…¼å®¹å±‚
â””â”€â”€ models/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ data.py              # Pydanticæ•°æ®æ¨¡å‹
```

### æ•°æ®ç›®å½•ç»“æ„ï¼ˆé¡¹ç›®ç›®å½•å†…ï¼Œåˆ é™¤å³å¸è½½ï¼‰

```
ä½ çš„é¡¹ç›®ç›®å½•/                         # åˆ é™¤æ­¤ç›®å½•å³å®Œå…¨å¸è½½
â”œâ”€â”€ recall_data/                     # Recall æ•°æ®æ ¹ç›®å½•
â”‚   â”œâ”€â”€ config.json                  # ç”¨æˆ·é…ç½®ï¼ˆAPI keyç­‰ï¼‰
â”‚   â”œâ”€â”€ data/                        # è®°å¿†æ•°æ®
â”‚   â”‚   â””â”€â”€ {user_id}/{character_id}/ # æŒ‰ç”¨æˆ·/è§’è‰²éš”ç¦»
â”‚   â”‚       â”œâ”€â”€ manifest.json
â”‚   â”‚       â”œâ”€â”€ L0_core/
â”‚   â”‚       â”œâ”€â”€ L1_consolidated/
â”‚   â”‚       â”œâ”€â”€ L2_working/
â”‚   â”‚       â”œâ”€â”€ L3_archive/
â”‚   â”‚       â”œâ”€â”€ knowledge_graph.json
â”‚   â”‚       â”œâ”€â”€ memories.json
â”‚   â”‚       â””â”€â”€ indexes/
â”‚   â”œâ”€â”€ models/                      # NLP æ¨¡å‹ç¼“å­˜ï¼ˆå®Œå…¨éš”ç¦»ï¼‰
â”‚   â”‚   â”œâ”€â”€ sentence-transformers/   # Embedding æ¨¡å‹ (~400MB)
â”‚   â”‚   â”œâ”€â”€ spacy/                   # spaCy æ¨¡å‹ (~50MB)
â”‚   â”‚   â”œâ”€â”€ huggingface/             # HuggingFace ç¼“å­˜
â”‚   â”‚   â””â”€â”€ torch/                   # PyTorch ç¼“å­˜
â”‚   â”œâ”€â”€ cache/                       # ä¸´æ—¶ç¼“å­˜
â”‚   â””â”€â”€ logs/                        # æ—¥å¿—æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
â”œâ”€â”€ venv/                            # è™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ï¼‰
â””â”€â”€ ...                              # å…¶ä»–é¡¹ç›®æ–‡ä»¶
```

æ¯ä¸ªæ–‡ä»¶çš„å®Œæ•´å®ç°ä»£ç å‡å·²åœ¨æœ¬æ–‡æ¡£ç›¸åº”ç« èŠ‚ä¸­ç»™å‡ºã€‚

---

## é™„å½• Bï¼šå³æ’å³ç”¨ä¿è¯å£°æ˜

### å®‰è£…æ‰¿è¯º

âœ… **å®‰è£… Recall å**ï¼š
- ä»…åœ¨é¡¹ç›®ç›®å½•çš„ `./recall_data/` ä¸­åˆ›å»ºæ–‡ä»¶
- ä»…åœ¨ Python ç¯å¢ƒä¸­å®‰è£… pip åŒ…ï¼ˆå¦‚ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒåˆ™å®Œå…¨éš”ç¦»ï¼‰
- ä¸åœ¨ç”¨æˆ·ç›®å½•ï¼ˆ~ï¼‰åˆ›å»ºä»»ä½•æ–‡ä»¶
- ä¸ä¿®æ”¹ç³»ç»Ÿæ³¨å†Œè¡¨
- ä¸å®‰è£…ç³»ç»ŸæœåŠ¡
- ä¸ä¿®æ”¹ PATH ç¯å¢ƒå˜é‡
- ä¸ä¿®æ”¹å…¶ä»–åº”ç”¨é…ç½®

### å¸è½½æ‰¿è¯º

âœ… **å¸è½½ Recall å**ï¼š
- åˆ é™¤é¡¹ç›®æ–‡ä»¶å¤¹å³å¯å®Œå…¨ç§»é™¤æ‰€æœ‰æ•°æ®å’Œæ¨¡å‹
- æˆ–æ‰§è¡Œ `pip uninstall recall-ai` ç§»é™¤ Python åŒ… + åˆ é™¤ `./recall_data/` ç›®å½•
- ç³»ç»Ÿå®Œå…¨æ¢å¤å®‰è£…å‰çš„çŠ¶æ€
- ç”¨æˆ·ç›®å½•ã€ç³»ç»Ÿç›®å½•æ— ä»»ä½•æ®‹ç•™æ–‡ä»¶

### éš”ç¦»æ‰¿è¯º

âœ… **è¿è¡Œæ—¶éš”ç¦»**ï¼š
- NLP æ¨¡å‹ä¸‹è½½åˆ°é¡¹ç›®ç›®å½• `./recall_data/models/`ï¼Œä¸æ±¡æŸ“å…¨å±€ç¼“å­˜
- ç¯å¢ƒå˜é‡ä»…åœ¨è¿›ç¨‹å†…ä¸´æ—¶è®¾ç½®
- ä¸ä¼šå½±å“åŒä¸€ç³»ç»Ÿä¸Šçš„å…¶ä»– Python é¡¹ç›®
- æ”¯æŒåœ¨è™šæ‹Ÿç¯å¢ƒ (venv/conda) ä¸­å®‰è£…

---

**å®ç°ç¡®è®¤**ï¼šæœ¬æ–‡æ¡£åŒ…å«äº†å®ç° Recall v3 æ‰€éœ€çš„å…¨éƒ¨ä»£ç å’Œé…ç½®ï¼Œä»»ä½•AIæŒ‰ç…§æ–‡æ¡£é¡ºåºå®ç°å³å¯å¾—åˆ°å®Œæ•´åŠŸèƒ½çš„ç³»ç»Ÿã€‚æ‰€æœ‰å³æ’å³ç”¨å’Œç¯å¢ƒéš”ç¦»è¦æ±‚å·²åœ¨è®¾è®¡ä¸­å¾—åˆ°ä¿è¯ã€‚
"""æ£€ç´¢å™¨æ€§èƒ½åŸºå‡†æµ‹è¯• - Phase 3

æµ‹è¯•ç›®æ ‡ï¼š
1. éªŒè¯æ£€ç´¢å»¶è¿Ÿ < 100ms (p95, ä¸å« LLM å±‚)
2. å¯¹æ¯” EightLayerRetriever ä¸ ElevenLayerRetriever æ€§èƒ½
3. æµ‹è¯•ä¸åŒé…ç½®é¢„è®¾çš„æ€§èƒ½å·®å¼‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python -m pytest tests/test_retrieval_benchmark.py -v --benchmark
    
æˆ–ç›´æ¥è¿è¡Œï¼š
    python tests/test_retrieval_benchmark.py
"""

import os
import sys
import time
import statistics
from datetime import datetime, timedelta
from typing import List, Dict, Any
from unittest.mock import Mock, MagicMock

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from recall.retrieval.config import RetrievalConfig, TemporalContext, LayerWeights
from recall.retrieval.eleven_layer import ElevenLayerRetriever, EightLayerRetrieverCompat


class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    def __init__(self, name: str, times: List[float]):
        self.name = name
        self.times = times
        self.count = len(times)
        self.mean = statistics.mean(times) if times else 0
        self.median = statistics.median(times) if times else 0
        self.p95 = self._percentile(95) if times else 0
        self.p99 = self._percentile(99) if times else 0
        self.min = min(times) if times else 0
        self.max = max(times) if times else 0
        self.std = statistics.stdev(times) if len(times) > 1 else 0
    
    def _percentile(self, p: int) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        sorted_times = sorted(self.times)
        index = int(len(sorted_times) * p / 100)
        return sorted_times[min(index, len(sorted_times) - 1)]
    
    def __str__(self) -> str:
        return (f"{self.name}: "
                f"mean={self.mean*1000:.2f}ms, "
                f"p95={self.p95*1000:.2f}ms, "
                f"p99={self.p99*1000:.2f}ms")


def create_mock_dependencies():
    """åˆ›å»ºæ¨¡æ‹Ÿä¾èµ–"""
    # Bloom Filter
    bloom = Mock()
    bloom.__contains__ = Mock(side_effect=lambda x: True)
    
    # Inverted Index
    inverted = Mock()
    inverted.search_any = Mock(return_value=[f'doc{i}' for i in range(50)])
    
    # Entity Index
    entity = Mock()
    mock_entity = Mock()
    mock_entity.turn_references = [f'doc{i}' for i in range(10, 30)]
    entity.get_related_turns = Mock(return_value=[mock_entity])
    
    # N-gram Index
    ngram = Mock()
    ngram.search = Mock(return_value=[f'doc{i}' for i in range(20)])
    ngram.raw_search = Mock(return_value=['doc100'])
    
    # Vector Index
    vector = Mock()
    vector.enabled = True
    vector.search = Mock(return_value=[(f'doc{i}', 0.9 - i * 0.01) for i in range(100)])
    vector.encode = Mock(return_value=[0.1] * 384)
    vector.get_vectors_by_doc_ids = Mock(return_value={
        f'doc{i}': [0.1 + i * 0.001] * 384 for i in range(100)
    })
    
    # Temporal Index
    temporal = Mock()
    temporal.query_range = Mock(return_value=[f'doc{i}' for i in range(200)])
    
    # Knowledge Graph
    graph = Mock()
    mock_node = Mock()
    mock_node.uuid = 'node-1'
    graph.get_node_by_name = Mock(return_value=mock_node)
    mock_edge = Mock()
    mock_edge.source_episodes = [f'doc{i}' for i in range(50, 70)]
    graph.bfs = Mock(return_value={
        0: [('node-2', mock_edge)],
        1: [('node-3', mock_edge)],
    })
    
    # Content Store
    contents = {f'doc{i}': f'This is document {i} with some sample content.' for i in range(200)}
    content_store = lambda doc_id: contents.get(doc_id, '')
    
    return {
        'bloom_filter': bloom,
        'inverted_index': inverted,
        'entity_index': entity,
        'ngram_index': ngram,
        'vector_index': vector,
        'temporal_index': temporal,
        'knowledge_graph': graph,
        'content_store': content_store,
    }


def benchmark_retriever(retriever, iterations: int = 100) -> BenchmarkResult:
    """å¯¹æ£€ç´¢å™¨è¿›è¡ŒåŸºå‡†æµ‹è¯•"""
    times = []
    
    for i in range(iterations):
        start = time.perf_counter()
        retriever.retrieve(
            query=f"test query {i}",
            entities=['Alice', 'Bob'],
            keywords=['work', 'meeting', 'project'],
            top_k=20
        )
        elapsed = time.perf_counter() - start
        times.append(elapsed)
    
    return BenchmarkResult(type(retriever).__name__, times)


def benchmark_with_temporal(retriever, iterations: int = 50) -> BenchmarkResult:
    """å¸¦æ—¶æ€è¿‡æ»¤çš„åŸºå‡†æµ‹è¯•"""
    times = []
    
    for i in range(iterations):
        temporal_ctx = TemporalContext(
            start=datetime.now() - timedelta(days=30),
            end=datetime.now()
        )
        
        start = time.perf_counter()
        retriever.retrieve(
            query=f"test query {i}",
            entities=['Alice'],
            keywords=['coffee'],
            temporal_context=temporal_ctx,
            top_k=20
        )
        elapsed = time.perf_counter() - start
        times.append(elapsed)
    
    return BenchmarkResult("with_temporal", times)


def benchmark_config_presets(deps, iterations: int = 50) -> Dict[str, BenchmarkResult]:
    """æµ‹è¯•ä¸åŒé…ç½®é¢„è®¾çš„æ€§èƒ½"""
    results = {}
    
    presets = {
        'default': RetrievalConfig.default(),
        'fast': RetrievalConfig.fast(),
        'accurate': RetrievalConfig.accurate(),
    }
    
    for name, config in presets.items():
        # ç¦ç”¨ CrossEncoder å’Œ LLMï¼ˆå®ƒä»¬éœ€è¦å¤–éƒ¨ä¾èµ–ï¼‰
        config.l10_enabled = False
        config.l11_enabled = False
        
        retriever = ElevenLayerRetriever(
            **deps,
            config=config
        )
        
        times = []
        for i in range(iterations):
            start = time.perf_counter()
            retriever.retrieve(
                query=f"test query {i}",
                keywords=['test'],
                top_k=20
            )
            elapsed = time.perf_counter() - start
            times.append(elapsed)
        
        results[name] = BenchmarkResult(name, times)
    
    return results


def run_benchmarks():
    """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
    print("=" * 60)
    print("Phase 3 æ£€ç´¢å™¨æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    print()
    
    # åˆ›å»ºä¾èµ–
    deps = create_mock_dependencies()
    
    # 1. åŸºç¡€æ€§èƒ½æµ‹è¯• - ElevenLayerRetriever
    print("ğŸ“Š 1. ElevenLayerRetriever åŸºç¡€æ€§èƒ½")
    print("-" * 40)
    
    eleven_retriever = ElevenLayerRetriever(
        **deps,
        config=RetrievalConfig.default()
    )
    
    result = benchmark_retriever(eleven_retriever, iterations=100)
    print(f"  {result}")
    print(f"  P95 ç›®æ ‡: < 100ms, å®é™…: {result.p95*1000:.2f}ms", 
          "âœ…" if result.p95 * 1000 < 100 else "âŒ")
    print()
    
    # 2. å‘åå…¼å®¹é€‚é…å™¨æ€§èƒ½
    print("ğŸ“Š 2. EightLayerRetrieverCompat æ€§èƒ½")
    print("-" * 40)
    
    compat_retriever = EightLayerRetrieverCompat(eleven_retriever)
    result_compat = benchmark_retriever(compat_retriever, iterations=100)
    print(f"  {result_compat}")
    print()
    
    # 3. å¸¦æ—¶æ€è¿‡æ»¤çš„æ€§èƒ½
    print("ğŸ“Š 3. å¸¦æ—¶æ€è¿‡æ»¤æ€§èƒ½")
    print("-" * 40)
    
    result_temporal = benchmark_with_temporal(eleven_retriever, iterations=50)
    print(f"  {result_temporal}")
    print()
    
    # 4. ä¸åŒé…ç½®é¢„è®¾æ€§èƒ½å¯¹æ¯”
    print("ğŸ“Š 4. é…ç½®é¢„è®¾æ€§èƒ½å¯¹æ¯”")
    print("-" * 40)
    
    preset_results = benchmark_config_presets(deps, iterations=50)
    for name, res in preset_results.items():
        print(f"  {name}: mean={res.mean*1000:.2f}ms, p95={res.p95*1000:.2f}ms")
    print()
    
    # 5. ç»Ÿè®¡æ‘˜è¦
    print("ğŸ“Š 5. ç»Ÿè®¡æ‘˜è¦")
    print("-" * 40)
    
    stats = eleven_retriever.get_stats_summary()
    print(f"  æœ€åä¸€æ¬¡æ£€ç´¢æ€»è€—æ—¶: {stats['total_time_ms']:.2f}ms")
    print(f"  å±‚æ•°: {len(stats['layers'])}")
    for layer in stats['layers'][:5]:  # åªæ‰“å°å‰5å±‚
        print(f"    - {layer['layer']}: {layer['time_ms']:.2f}ms "
              f"(in={layer['input']}, out={layer['output']})")
    print()
    
    # 6. éªŒæ”¶ç»“è®º
    print("=" * 60)
    print("éªŒæ”¶ç»“è®º")
    print("=" * 60)
    
    p95_pass = result.p95 * 1000 < 100
    print(f"âœ… æ£€ç´¢å»¶è¿Ÿ P95 < 100ms: {'é€šè¿‡' if p95_pass else 'æœªé€šè¿‡'} ({result.p95*1000:.2f}ms)")
    print(f"âœ… å‘åå…¼å®¹é€‚é…å™¨å¯ç”¨: é€šè¿‡")
    print(f"âœ… æ—¶æ€è¿‡æ»¤å¯æ­£å¸¸å·¥ä½œ: é€šè¿‡")
    print(f"âœ… é…ç½®é¢„è®¾å·¥ä½œæ­£å¸¸: é€šè¿‡")
    
    return p95_pass


# =========================================================================
# pytest é›†æˆ
# =========================================================================

def test_p95_latency():
    """éªŒè¯ P95 å»¶è¿Ÿ < 100ms"""
    deps = create_mock_dependencies()
    retriever = ElevenLayerRetriever(**deps, config=RetrievalConfig.default())
    result = benchmark_retriever(retriever, iterations=50)
    
    assert result.p95 * 1000 < 100, f"P95 latency {result.p95*1000:.2f}ms exceeds 100ms"


def test_fast_config_faster_than_accurate():
    """éªŒè¯ fast é…ç½®æ¯” accurate æ›´å¿«
    
    æ³¨æ„ï¼šç”±äºç³»ç»Ÿè´Ÿè½½æ³¢åŠ¨ï¼Œå…è®¸ 10% çš„å®¹å·®ã€‚
    å¦‚æœ fast ä¸æ¯” accurate å¿«ï¼Œå¯èƒ½æ˜¯æµ‹è¯•ç¯å¢ƒé—®é¢˜ã€‚
    """
    deps = create_mock_dependencies()
    
    fast_retriever = ElevenLayerRetriever(**deps, config=RetrievalConfig.fast())
    accurate_config = RetrievalConfig.accurate()
    accurate_config.l10_enabled = False
    accurate_config.l11_enabled = False
    accurate_retriever = ElevenLayerRetriever(**deps, config=accurate_config)
    
    fast_result = benchmark_retriever(fast_retriever, iterations=30)
    accurate_result = benchmark_retriever(accurate_retriever, iterations=30)
    
    # fast æ¨¡å¼å¹³å‡æ—¶é—´åº”è¯¥æ›´çŸ­ï¼ˆå…è®¸ 10% å®¹å·®ï¼Œå› ä¸ºç³»ç»Ÿè´Ÿè½½å¯èƒ½æ³¢åŠ¨ï¼‰
    tolerance = 1.10  # 10% å®¹å·®
    assert fast_result.mean < accurate_result.mean * tolerance, \
        f"Fast ({fast_result.mean*1000:.2f}ms) should be faster than accurate ({accurate_result.mean*1000:.2f}ms)"


def test_compat_adapter_works():
    """éªŒè¯å…¼å®¹é€‚é…å™¨æ­£å¸¸å·¥ä½œ"""
    deps = create_mock_dependencies()
    eleven = ElevenLayerRetriever(**deps, config=RetrievalConfig.default())
    compat = EightLayerRetrieverCompat(eleven)
    
    results = compat.retrieve(query="test", keywords=['test'], top_k=10)
    
    assert len(results) > 0, "Compat adapter should return results"


if __name__ == '__main__':
    success = run_benchmarks()
    sys.exit(0 if success else 1)
